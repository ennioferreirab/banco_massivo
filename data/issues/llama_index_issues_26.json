[
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6787",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6787/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6787/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6787/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6787",
        "id": 1794628753,
        "node_id": "I_kwDOIWuq585q99yR",
        "number": 6787,
        "title": "[Documentation]: ValueError in sql_query_tool.query_engine",
        "user": {
            "login": "viai957",
            "id": 29157342,
            "node_id": "MDQ6VXNlcjI5MTU3MzQy",
            "avatar_url": "https://avatars.githubusercontent.com/u/29157342?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/viai957",
            "html_url": "https://github.com/viai957",
            "followers_url": "https://api.github.com/users/viai957/followers",
            "following_url": "https://api.github.com/users/viai957/following{/other_user}",
            "gists_url": "https://api.github.com/users/viai957/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/viai957/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/viai957/subscriptions",
            "organizations_url": "https://api.github.com/users/viai957/orgs",
            "repos_url": "https://api.github.com/users/viai957/repos",
            "events_url": "https://api.github.com/users/viai957/events{/privacy}",
            "received_events_url": "https://api.github.com/users/viai957/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318866,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/documentation",
                "name": "documentation",
                "color": "0075ca",
                "default": true,
                "description": "Improvements or additions to documentation"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-08T02:39:38Z",
        "updated_at": "2023-10-14T20:08:09Z",
        "closed_at": "2023-10-14T20:08:08Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Documentation Issue Description\n\nValueError in sql_query_tool.query_engine\r\n\r\nsql_query_engine is not defined but it exists in the BaseQueryEngine class\n\n### Documenation Link\n\nhttps://github.com/viai957/llama_index/blob/main/docs/examples/query_engine/SQLJoinQueryEngine.ipynb",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6787/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6787/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6786",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6786/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6786/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6786/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6786",
        "id": 1794611089,
        "node_id": "PR_kwDOIWuq585U-cAX",
        "number": 6786,
        "title": "Update dataset_generation.py",
        "user": {
            "login": "mzamini92",
            "id": 32536264,
            "node_id": "MDQ6VXNlcjMyNTM2MjY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/32536264?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mzamini92",
            "html_url": "https://github.com/mzamini92",
            "followers_url": "https://api.github.com/users/mzamini92/followers",
            "following_url": "https://api.github.com/users/mzamini92/following{/other_user}",
            "gists_url": "https://api.github.com/users/mzamini92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mzamini92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mzamini92/subscriptions",
            "organizations_url": "https://api.github.com/users/mzamini92/orgs",
            "repos_url": "https://api.github.com/users/mzamini92/repos",
            "events_url": "https://api.github.com/users/mzamini92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mzamini92/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-08T02:11:08Z",
        "updated_at": "2023-09-21T17:04:50Z",
        "closed_at": "2023-09-21T17:04:50Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6786",
            "html_url": "https://github.com/run-llama/llama_index/pull/6786",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6786.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6786.patch",
            "merged_at": null
        },
        "body": "The _generate_questions_for_node method now uses async and await to make the question generation process asynchronous. This allows multiple API requests to be made concurrently, improving the overall efficiency of the code. The generate_questions_from_nodes method uses asyncio.gather to concurrently execute the _generate_questions_for_node method for each node. This parallel processing further enhances the performance by utilizing multiple CPU cores.\r\n\r\n# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6786/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6786/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6785",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6785/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6785/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6785/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6785",
        "id": 1794594978,
        "node_id": "I_kwDOIWuq585q91ii",
        "number": 6785,
        "title": "[Feature Request]: use `set_global_service_context` in majority of examples and notebooks",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-08T01:32:32Z",
        "updated_at": "2023-10-14T20:08:14Z",
        "closed_at": "2023-10-14T20:08:13Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nWhere appropriate (when the user does need to manage multiple service contexts), `set_global_service_context` should make the code slightly less verbose\r\n\r\nSee: https://github.com/jerryjliu/llama_index/pull/6726 for some additional context\r\n\r\n### Reason\r\n\r\n_No response_\r\n\r\n### Value of Feature\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6785/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6785/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6784",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6784/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6784/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6784/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6784",
        "id": 1794577939,
        "node_id": "PR_kwDOIWuq585U-U4d",
        "number": 6784,
        "title": "[Feature] Add support for Cassandra Vector Store",
        "user": {
            "login": "hemidactylus",
            "id": 14221764,
            "node_id": "MDQ6VXNlcjE0MjIxNzY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/14221764?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hemidactylus",
            "html_url": "https://github.com/hemidactylus",
            "followers_url": "https://api.github.com/users/hemidactylus/followers",
            "following_url": "https://api.github.com/users/hemidactylus/following{/other_user}",
            "gists_url": "https://api.github.com/users/hemidactylus/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hemidactylus/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hemidactylus/subscriptions",
            "organizations_url": "https://api.github.com/users/hemidactylus/orgs",
            "repos_url": "https://api.github.com/users/hemidactylus/repos",
            "events_url": "https://api.github.com/users/hemidactylus/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hemidactylus/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-07-08T00:49:12Z",
        "updated_at": "2023-08-23T08:04:10Z",
        "closed_at": "2023-08-22T22:21:15Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6784",
            "html_url": "https://github.com/run-llama/llama_index/pull/6784",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6784.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6784.patch",
            "merged_at": "2023-08-22T22:21:15Z"
        },
        "body": "# Description\r\n\r\nThis adds a `VectorStore` implementation for working with Apache Cassandra\u00ae databases, including the cloud database DataStax Astra DB.\r\n\r\nThis vector store supports both \"default\" and MMR query methods.\r\n\r\nThis is a high-performance, highly-scalable, zero-downtime NoSQL database and as such is a good match for robust applications using vector-search techniques at scale.\r\n\r\n## Type of Change\r\n\r\n- [X] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [X] Added new unit/integration tests: **unit tests** (there do not seem to be integration tests throughout the codebase)\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [X] I have successfully run a few end-to-end Python programs that make use of the functionalities (\"manual integration tests\", if you will)\r\n\r\n# Other observations\r\n\r\nThe implementation is based on the `cassIO` library, which abstracts the database access in a ML- and genAI-oriented way.\r\n\r\nAround line 103 of `cassandra.py` I use the construct `getattr(result.node, DEFAULT_TEXT_KEY, \"\")` to extract the \"text\" contained in a node. I resort to this since, looking at `DEFAULT_TEXT_KEY`, it seems unsafe to just assume it will always be called `\"text\"`. Can that be considered idiomatic? Should that be changed?\r\n\r\nAlso, incidentally: when implementing MMR, I noticed that in the `SimpleVectorStoreData` implementation the possibility to find member `mmr_threshold` directly in the passed `VectorStoreQuery` object (as suggested by the definition of the latter structure) is overlooked, and the parameter is searched directly in the method kwargs. To err on the safe side, in my implementation of the MMR search I check for `mmr_threshold` in both places, with the one in `query` taking precedence.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6784/reactions",
            "total_count": 6,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 1,
            "confused": 0,
            "heart": 4,
            "rocket": 1,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6784/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6783",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6783/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6783/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6783/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6783",
        "id": 1794557608,
        "node_id": "PR_kwDOIWuq585U-QZF",
        "number": 6783,
        "title": "[version] bump version to 0.7.3",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-08T00:14:00Z",
        "updated_at": "2023-07-08T00:29:34Z",
        "closed_at": "2023-07-08T00:29:33Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6783",
            "html_url": "https://github.com/run-llama/llama_index/pull/6783",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6783.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6783.patch",
            "merged_at": "2023-07-08T00:29:33Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6783/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6783/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6782",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6782/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6782/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6782/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6782",
        "id": 1794519594,
        "node_id": "PR_kwDOIWuq585U-IHq",
        "number": 6782,
        "title": "Remove usage of stop token in Prompt, SQL gen",
        "user": {
            "login": "hongyishi",
            "id": 7098202,
            "node_id": "MDQ6VXNlcjcwOTgyMDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7098202?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hongyishi",
            "html_url": "https://github.com/hongyishi",
            "followers_url": "https://api.github.com/users/hongyishi/followers",
            "following_url": "https://api.github.com/users/hongyishi/following{/other_user}",
            "gists_url": "https://api.github.com/users/hongyishi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hongyishi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hongyishi/subscriptions",
            "organizations_url": "https://api.github.com/users/hongyishi/orgs",
            "repos_url": "https://api.github.com/users/hongyishi/repos",
            "events_url": "https://api.github.com/users/hongyishi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hongyishi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-07T23:09:00Z",
        "updated_at": "2023-07-08T00:11:38Z",
        "closed_at": "2023-07-08T00:11:37Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6782",
            "html_url": "https://github.com/run-llama/llama_index/pull/6782",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6782.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6782.patch",
            "merged_at": "2023-07-08T00:11:37Z"
        },
        "body": "# Description\r\n\r\nWe used to use stop_token in SQL generation, which was removed during the llm predictor migration. This removes usage of  stop tokens in Prompt, and instead uses string comprehension to remove the unused \"SQLResult: ...\"\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [ ] Added new notebook (that tests end-to-end)\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6782/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6782/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6781",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6781/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6781/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6781/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6781",
        "id": 1794492581,
        "node_id": "I_kwDOIWuq585q9cil",
        "number": 6781,
        "title": "[Bug]: sqlite3.Warning: You can only execute one statement at a time in NLSQLTableQueryEngine",
        "user": {
            "login": "talvola",
            "id": 6888436,
            "node_id": "MDQ6VXNlcjY4ODg0MzY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6888436?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/talvola",
            "html_url": "https://github.com/talvola",
            "followers_url": "https://api.github.com/users/talvola/followers",
            "following_url": "https://api.github.com/users/talvola/following{/other_user}",
            "gists_url": "https://api.github.com/users/talvola/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/talvola/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/talvola/subscriptions",
            "organizations_url": "https://api.github.com/users/talvola/orgs",
            "repos_url": "https://api.github.com/users/talvola/repos",
            "events_url": "https://api.github.com/users/talvola/events{/privacy}",
            "received_events_url": "https://api.github.com/users/talvola/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-07T22:39:36Z",
        "updated_at": "2023-07-10T23:57:34Z",
        "closed_at": "2023-07-10T23:57:34Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nUsing the example from https://gpt-index.readthedocs.io/en/latest/guides/tutorials/sql_guide.html - with an in-memory SQLite database, one ends up with:\r\n\r\n`>>> response = query_engine.query(query_str)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/llama_index/indices/query/base.py\", line 23, in query\r\n    response = self._query(str_or_query_bundle)\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/llama_index/indices/struct_store/sql_query.py\", line 267, in _query\r\n    raw_response_str, metadata = self._sql_database.run_sql(sql_query_str)\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/llama_index/langchain_helpers/sql_wrapper.py\", line 82, in run_sql\r\n    cursor = connection.execute(text(command))\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1412, in execute\r\n    return meth(\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/sqlalchemy/sql/elements.py\", line 483, in _execute_on_connection\r\n    return connection._execute_clauseelement(\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1635, in _execute_clauseelement\r\n    ret = self._execute_context(\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1844, in _execute_context\r\n    return self._exec_single_context(\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1984, in _exec_single_context\r\n    self._handle_dbapi_exception(\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 2342, in _handle_dbapi_exception\r\n    raise exc_info[1].with_traceback(exc_info[2])\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/sqlalchemy/engine/base.py\", line 1965, in _exec_single_context\r\n    self.dialect.do_execute(\r\n  File \"/home/talvola/llama_testing/lib/python3.10/site-packages/sqlalchemy/engine/default.py\", line 921, in do_execute\r\n    cursor.execute(statement, parameters)\r\nsqlite3.Warning: You can only execute one statement at a time.`\r\n\r\nI'm not sure if the prompt needs changing, but turning on logging - the SQL query generated is:\r\n\r\n`INFO:llama_index.indices.struct_store.sql_query:> Table desc str: Table 'city_stats' has columns: city_name (VARCHAR(16)), population (INTEGER), country (VARCHAR(16)) and foreign keys: .\r\nDEBUG:llama_index.indices.struct_store.sql_query:> Predicted SQL query: SELECT city_name, population\r\nFROM city_stats\r\nORDER BY population DESC\r\nLIMIT 1;\r\nSQLResult:\r\ncity_name | population\r\nTokyo     | 37000000\r\nAnswer: Tokyo has the highest population.`\r\n\r\nwith OpenAI (in my case) already filling in part of the answer.\r\n\r\nI fixed this by stripping out the rest of the SQL statement:\r\n\r\n`$ git diff llama_index/indices/struct_store/sql_query.py\r\ndiff --git a/llama_index/indices/struct_store/sql_query.py b/llama_index/indices/struct_store/sql_query.py\r\nindex 5dd5b6cf..27254eed 100644\r\n--- a/llama_index/indices/struct_store/sql_query.py\r\n+++ b/llama_index/indices/struct_store/sql_query.py\r\n@@ -237,7 +237,7 @@ class BaseSQLTableQueryEngine(BaseQueryEngine):\r\n\r\n     def _parse_response_to_sql(self, response: str) -> str:\r\n         \"\"\"Parse response to SQL.\"\"\"\r\n-        result_response = response.strip()\r\n+        result_response = response.strip().split(\";\")[0] + \";\"\r\n\r\n\r\nbut could be better to change the prompt?  \r\n\r\nseparately - the sample code on the Guide didn't work for me without doing an explicit commit like connection.commit() after the execute for the insert statements.\r\n\r\n\r\n### Version\r\n\r\n0.7.2\r\n\r\n### Steps to Reproduce\r\n\r\nsimply run the steps from https://gpt-index.readthedocs.io/en/latest/guides/tutorials/sql_guide.html as in the description\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6781/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6781/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6780",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6780/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6780/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6780/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6780",
        "id": 1794364120,
        "node_id": "PR_kwDOIWuq585U9k0i",
        "number": 6780,
        "title": "add query bundle to node postprocessor call",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-07T21:14:06Z",
        "updated_at": "2023-07-08T01:18:24Z",
        "closed_at": "2023-07-08T01:18:23Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6780",
            "html_url": "https://github.com/run-llama/llama_index/pull/6780",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6780.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6780.patch",
            "merged_at": "2023-07-08T01:18:23Z"
        },
        "body": "# Description\r\n\r\nSome node-postprocessors require the query bundle, we should be passing that in for users.\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6780/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6780/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6779",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6779/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6779/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6779/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6779",
        "id": 1793668386,
        "node_id": "PR_kwDOIWuq585U7In8",
        "number": 6779,
        "title": "Embedding timeouts",
        "user": {
            "login": "ahwitz",
            "id": 3514397,
            "node_id": "MDQ6VXNlcjM1MTQzOTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3514397?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ahwitz",
            "html_url": "https://github.com/ahwitz",
            "followers_url": "https://api.github.com/users/ahwitz/followers",
            "following_url": "https://api.github.com/users/ahwitz/following{/other_user}",
            "gists_url": "https://api.github.com/users/ahwitz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ahwitz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ahwitz/subscriptions",
            "organizations_url": "https://api.github.com/users/ahwitz/orgs",
            "repos_url": "https://api.github.com/users/ahwitz/repos",
            "events_url": "https://api.github.com/users/ahwitz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ahwitz/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-07T14:41:27Z",
        "updated_at": "2023-07-18T04:27:22Z",
        "closed_at": "2023-07-18T04:27:21Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6779",
            "html_url": "https://github.com/run-llama/llama_index/pull/6779",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6779.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6779.patch",
            "merged_at": "2023-07-18T04:27:21Z"
        },
        "body": "# Description\r\n\r\nWe're trying to index large code repositories using VectorStoreIndexes, and have been tweaking the `embed_batch_size` parameter on the `OpenAIEmbedding` class. We're at the point where the repositories are large enough that the default value of `10` takes a significant amount of time to run, but we've found that using higher values for that triggers a rate limiting error from OpenAI:\r\n\r\n> openai.error.RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization on tokens per min. Limit: 1000000 / min. Current: 832988 / min. Contact us through our help center at help.openai.com if you continue to have issues.\r\n\r\nBecause their rates are per minute, bumping the `@retry` decorator to ensure requests can made until 60s after the original failure felt correct, and does resolve our issues.\r\n\r\nI can also work towards having a proper mechanism to pass in a `max_tokens_per_minute` and have it cap based off the tokenizer, or even some sort of callback after each batch is complete, but those are probably way more effort than would be worth it.\r\n        \r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\nIt felt like a bad idea to add mocks for a timeout-based test.\r\n\r\nTo test, I did this in two segments with the `RateLimitError` in the OpenAI code, but you can probably drop the following three lines in the `get_embeddings` function:\r\n\r\n```\r\n    import time\r\n    print('attempt', len(list_of_text), int(time.time()))\r\n    raise openai.error.RateLimitError('fake')\r\n```\r\n\r\n...and the third param to `print` will print out once at [first printout + 60] before the RateLimitError escapes and stops execution.\r\n\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [can go back and do it if desired] I have commented my code, particularly in hard-to-understand areas\r\n- [don't think docs exist] I have made corresponding changes to the documentation\r\n- [x] My changes generate no new warnings\r\n- [written] I have added tests that prove my fix is effective or that my feature works\r\n- [will find out shortly] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6779/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6779/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6778",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6778/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6778/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6778/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6778",
        "id": 1793558829,
        "node_id": "PR_kwDOIWuq585U6w2H",
        "number": 6778,
        "title": "Fixed variable sql_query_engine in the notebook",
        "user": {
            "login": "ravi03071991",
            "id": 12198101,
            "node_id": "MDQ6VXNlcjEyMTk4MTAx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12198101?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ravi03071991",
            "html_url": "https://github.com/ravi03071991",
            "followers_url": "https://api.github.com/users/ravi03071991/followers",
            "following_url": "https://api.github.com/users/ravi03071991/following{/other_user}",
            "gists_url": "https://api.github.com/users/ravi03071991/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ravi03071991/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ravi03071991/subscriptions",
            "organizations_url": "https://api.github.com/users/ravi03071991/orgs",
            "repos_url": "https://api.github.com/users/ravi03071991/repos",
            "events_url": "https://api.github.com/users/ravi03071991/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ravi03071991/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-07T13:31:52Z",
        "updated_at": "2023-07-07T15:24:16Z",
        "closed_at": "2023-07-07T15:24:16Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6778",
            "html_url": "https://github.com/run-llama/llama_index/pull/6778",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6778.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6778.patch",
            "merged_at": "2023-07-07T15:24:16Z"
        },
        "body": "# Description\r\n\r\nCorrected the variable name in the notebook from 'query_engine' to 'sql_query_engine' for consistency and accuracy.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] Ran the notebook end to end to make sure it runs without error.\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have made corresponding changes to the documentation\r\n- [x] My changes generate no new warnings\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6778/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6778/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6777",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6777/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6777/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6777/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6777",
        "id": 1793486985,
        "node_id": "PR_kwDOIWuq585U6hOk",
        "number": 6777,
        "title": "Add configurable DynamoDB endpoint",
        "user": {
            "login": "rjarun8",
            "id": 50106442,
            "node_id": "MDEyOk9yZ2FuaXphdGlvbjUwMTA2NDQy",
            "avatar_url": "https://avatars.githubusercontent.com/u/50106442?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjarun8",
            "html_url": "https://github.com/rjarun8",
            "followers_url": "https://api.github.com/users/rjarun8/followers",
            "following_url": "https://api.github.com/users/rjarun8/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjarun8/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjarun8/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjarun8/subscriptions",
            "organizations_url": "https://api.github.com/users/rjarun8/orgs",
            "repos_url": "https://api.github.com/users/rjarun8/repos",
            "events_url": "https://api.github.com/users/rjarun8/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjarun8/received_events",
            "type": "Organization",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-07T12:42:56Z",
        "updated_at": "2023-08-25T17:33:15Z",
        "closed_at": "2023-08-25T17:33:15Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6777",
            "html_url": "https://github.com/run-llama/llama_index/pull/6777",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6777.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6777.patch",
            "merged_at": "2023-08-25T17:33:15Z"
        },
        "body": "# Description\r\n\r\nThis change allows users to specify a custom URL for DynamoDB by setting the `DYNAMODB_URL` environment variable. This is particularly useful for development environments where DynamoDB is hosted locally. If `DYNAMODB_URL` is not set, the application will use the default AWS DynamoDB service.\r\n\r\nFixes #6760\r\n\r\n## Type of Change\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nThe changes have been tested by running the application with and without the `DYNAMODB_URL` environment variable set. The application correctly uses the specified URL when the variable is set, and the default AWS DynamoDB service when it's not.\r\n\r\n- [x] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [x] My changes generate no new warnings\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6777/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6777/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6776",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6776/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6776/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6776/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6776",
        "id": 1793384653,
        "node_id": "PR_kwDOIWuq585U6Kw1",
        "number": 6776,
        "title": "added file and dir validation",
        "user": {
            "login": "rjarun8",
            "id": 50106442,
            "node_id": "MDEyOk9yZ2FuaXphdGlvbjUwMTA2NDQy",
            "avatar_url": "https://avatars.githubusercontent.com/u/50106442?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rjarun8",
            "html_url": "https://github.com/rjarun8",
            "followers_url": "https://api.github.com/users/rjarun8/followers",
            "following_url": "https://api.github.com/users/rjarun8/following{/other_user}",
            "gists_url": "https://api.github.com/users/rjarun8/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rjarun8/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rjarun8/subscriptions",
            "organizations_url": "https://api.github.com/users/rjarun8/orgs",
            "repos_url": "https://api.github.com/users/rjarun8/repos",
            "events_url": "https://api.github.com/users/rjarun8/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rjarun8/received_events",
            "type": "Organization",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-07T11:33:45Z",
        "updated_at": "2023-07-19T22:24:17Z",
        "closed_at": "2023-07-19T22:24:17Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6776",
            "html_url": "https://github.com/run-llama/llama_index/pull/6776",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6776.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6776.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nThis change includes the implementation of a new feature in the `llama_index` project that allows for the validation of directories and files. The motivation for this change is to ensure that the directories and files being processed by the `llama_index` project are valid and accessible, which is crucial for the correct functioning of the project.\r\n\r\nThis change does not fix any existing issues and does not require any additional dependencies.\r\n\r\nFixes:  Add Path validation to SimpleDirectoryReader #6770\r\n\r\n## Type of Change\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\nThe changes have been tested by adding new unit tests that verify the correct functioning of the directory and file validation feature. These tests create temporary directories and files and then verify that the validation feature correctly identifies valid and invalid directories and files.\r\n\r\n- [x] Added new unit/integration tests\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [x] My changes generate no new warnings\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6776/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6776/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6775",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6775/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6775/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6775/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6775",
        "id": 1793332321,
        "node_id": "I_kwDOIWuq585q5BRh",
        "number": 6775,
        "title": "Is llamaindex supports conversationBufferMemory?",
        "user": {
            "login": "pradeepdev-1995",
            "id": 41164884,
            "node_id": "MDQ6VXNlcjQxMTY0ODg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/41164884?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pradeepdev-1995",
            "html_url": "https://github.com/pradeepdev-1995",
            "followers_url": "https://api.github.com/users/pradeepdev-1995/followers",
            "following_url": "https://api.github.com/users/pradeepdev-1995/following{/other_user}",
            "gists_url": "https://api.github.com/users/pradeepdev-1995/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pradeepdev-1995/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pradeepdev-1995/subscriptions",
            "organizations_url": "https://api.github.com/users/pradeepdev-1995/orgs",
            "repos_url": "https://api.github.com/users/pradeepdev-1995/repos",
            "events_url": "https://api.github.com/users/pradeepdev-1995/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pradeepdev-1995/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-07-07T11:05:02Z",
        "updated_at": "2023-08-14T14:10:51Z",
        "closed_at": "2023-08-14T14:10:51Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nIs there any chat memory keeping functionality in **llamaindex** like that in langchain?\r\nEg: \r\n```\r\nfrom langchain.memory import ConversationBufferMemory\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6775/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6775/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6774",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6774/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6774/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6774/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6774",
        "id": 1793232672,
        "node_id": "I_kwDOIWuq585q4o8g",
        "number": 6774,
        "title": "[Question]: RetrieverQueryEngine with CustomRetriever and LLM other than OpenAI",
        "user": {
            "login": "stephaneleroi",
            "id": 43995198,
            "node_id": "MDQ6VXNlcjQzOTk1MTk4",
            "avatar_url": "https://avatars.githubusercontent.com/u/43995198?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stephaneleroi",
            "html_url": "https://github.com/stephaneleroi",
            "followers_url": "https://api.github.com/users/stephaneleroi/followers",
            "following_url": "https://api.github.com/users/stephaneleroi/following{/other_user}",
            "gists_url": "https://api.github.com/users/stephaneleroi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stephaneleroi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stephaneleroi/subscriptions",
            "organizations_url": "https://api.github.com/users/stephaneleroi/orgs",
            "repos_url": "https://api.github.com/users/stephaneleroi/repos",
            "events_url": "https://api.github.com/users/stephaneleroi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stephaneleroi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-07T10:09:05Z",
        "updated_at": "2023-10-14T20:08:20Z",
        "closed_at": "2023-10-14T20:08:19Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI try to set up a simple hybrid search \r\nLike this one \r\nhttps://gpt-index.readthedocs.io/en/latest/examples/query_engine/CustomRetrievers.html\r\n\r\nI use a custom LLM\r\n\r\nllm_predictor = LLMPredictor(llm=CustomLLM())\r\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"T-Systems-onsite/cross-en-fr-roberta-sentence-transformer\"))\r\nservice_context = ServiceContext.from_defaults(  llm_predictor=llm_predictor,  embed_model=embed_model)\r\n\r\nAnd Indexes for this LLM\r\nvector_index = VectorStoreIndex.from_documents(docs, service_context=service_context)\r\nkeyword_index = SimpleKeywordTableIndex.from_documents(docs, service_context=service_context)\r\n\r\n\r\nI set up the query engine like this : \r\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=4)\r\n\r\nvector_query_engine = RetrieverQueryEngine(\r\n    retriever=vector_retriever,\r\n    response_synthesizer=refined_response_synth,\r\n)\r\n\r\n# the reponse synthetiser use refined text_qa_template \r\nrefined_response_synth = get_response_synthesizer(text_qa_template = qa_template2, response_mode = ResponseMode.COMPACT)\r\n\r\nvector_query_engine = RetrieverQueryEngine(\r\n    retriever=vector_retriever,\r\n    response_synthesizer=refined_response_synth,\r\n)\r\n\r\nWhen I query I have this error \r\nresponse = vector_query_engine.query(\"Quel est le contenu de l'article 222-29?\")\r\nprint (response)\r\n\r\n\r\nAuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\r\n\r\nIf I initialize the VectorIndexRetriever with the service_context I have the same error....\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6774/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6774/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6773",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6773/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6773/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6773/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6773",
        "id": 1793180646,
        "node_id": "I_kwDOIWuq585q4cPm",
        "number": 6773,
        "title": "[Question]:  Running old code always fail. ",
        "user": {
            "login": "deter3",
            "id": 11637246,
            "node_id": "MDQ6VXNlcjExNjM3MjQ2",
            "avatar_url": "https://avatars.githubusercontent.com/u/11637246?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/deter3",
            "html_url": "https://github.com/deter3",
            "followers_url": "https://api.github.com/users/deter3/followers",
            "following_url": "https://api.github.com/users/deter3/following{/other_user}",
            "gists_url": "https://api.github.com/users/deter3/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/deter3/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/deter3/subscriptions",
            "organizations_url": "https://api.github.com/users/deter3/orgs",
            "repos_url": "https://api.github.com/users/deter3/repos",
            "events_url": "https://api.github.com/users/deter3/events{/privacy}",
            "received_events_url": "https://api.github.com/users/deter3/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-07T09:41:29Z",
        "updated_at": "2023-07-07T16:26:18Z",
        "closed_at": "2023-07-07T16:26:05Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIt's a big frustration by using llama_index . When you try to run 1 month old code , always fail . When you try to search old version documentation , no result .  Man,  you 're keeping adding lots of fancy features , personally , I guess you're losing focus and trying to be a master key for all problems when the whole industry is moving really fast .  Focus on couple scenarios you really have deep understanding . \r\n\r\nWish you guys good luck .  ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6773/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6773/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6772",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6772/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6772/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6772/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6772",
        "id": 1793071547,
        "node_id": "I_kwDOIWuq585q4Bm7",
        "number": 6772,
        "title": "[Bug]: cannot import name 'HuggingFaceLLMPredictor' from 'llama_index.llm_predictor' ",
        "user": {
            "login": "vladimircape",
            "id": 1789618,
            "node_id": "MDQ6VXNlcjE3ODk2MTg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1789618?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vladimircape",
            "html_url": "https://github.com/vladimircape",
            "followers_url": "https://api.github.com/users/vladimircape/followers",
            "following_url": "https://api.github.com/users/vladimircape/following{/other_user}",
            "gists_url": "https://api.github.com/users/vladimircape/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vladimircape/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vladimircape/subscriptions",
            "organizations_url": "https://api.github.com/users/vladimircape/orgs",
            "repos_url": "https://api.github.com/users/vladimircape/repos",
            "events_url": "https://api.github.com/users/vladimircape/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vladimircape/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-07T08:35:46Z",
        "updated_at": "2023-07-17T18:12:11Z",
        "closed_at": "2023-07-07T16:23:51Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nin 0.7.2 version it's not working\r\n` from llama_index.llm_predictor import HuggingFaceLLMPredictor`\n\n### Version\n\n0.7.2\n\n### Steps to Reproduce\n\nfrom llama_index.llm_predictor import HuggingFaceLLMPredictor\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6772/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6772/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6771",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6771/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6771/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6771/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6771",
        "id": 1793061241,
        "node_id": "I_kwDOIWuq585q3_F5",
        "number": 6771,
        "title": "DECOMPOSE_QUERY_TRANSFORM with LLMs other than OpenAI",
        "user": {
            "login": "mattkallo",
            "id": 7277091,
            "node_id": "MDQ6VXNlcjcyNzcwOTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7277091?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mattkallo",
            "html_url": "https://github.com/mattkallo",
            "followers_url": "https://api.github.com/users/mattkallo/followers",
            "following_url": "https://api.github.com/users/mattkallo/following{/other_user}",
            "gists_url": "https://api.github.com/users/mattkallo/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mattkallo/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mattkallo/subscriptions",
            "organizations_url": "https://api.github.com/users/mattkallo/orgs",
            "repos_url": "https://api.github.com/users/mattkallo/repos",
            "events_url": "https://api.github.com/users/mattkallo/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mattkallo/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-07T08:31:51Z",
        "updated_at": "2023-10-14T20:08:24Z",
        "closed_at": "2023-10-14T20:08:23Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nHi - Has anyone tried query transformation/decomposition with any models other than OpenAI ones? Which all LLMs support the decomposition prompt? I am unable to get the response right from other LLMs. Tried with Huggingface LLMs, but did not get the response correctly from decomposition. \r\n\r\nTried the same query as what was given in the documentation: \"Compare and contrast the airports in Seattle, Houston, and Toronto\"",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6771/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6771/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6770",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6770/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6770/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6770/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6770",
        "id": 1793026938,
        "node_id": "I_kwDOIWuq585q32t6",
        "number": 6770,
        "title": "Add `Path` validation to `SimpleDirectoryReader`",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-07T08:10:31Z",
        "updated_at": "2023-07-11T03:00:12Z",
        "closed_at": "2023-07-11T03:00:12Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nAdd `Path` validation to `SimpleDirectoryReader` e.g.\r\n\r\n```python\r\nif os.path.isfile(\"filename.txt\"):\r\n\r\nif os.path.isdir(\"data\"):\r\n```\r\n\r\n_Originally posted by @jon-chuang in https://github.com/jerryjliu/llama_index/issues/6765#issuecomment-1624973663_\r\n            \r\nPlease also add unit tests with `with pytest.raises`\n\n### Reason\n\n_No response_\n\n### Value of Feature\n\nPrevent silent errors e.g. https://github.com/jerryjliu/llama_index/issues/6765",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6770/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6770/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6769",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6769/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6769/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6769/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6769",
        "id": 1792934488,
        "node_id": "PR_kwDOIWuq585U4oWT",
        "number": 6769,
        "title": "Patch 2",
        "user": {
            "login": "lilulilulilu",
            "id": 83462065,
            "node_id": "MDQ6VXNlcjgzNDYyMDY1",
            "avatar_url": "https://avatars.githubusercontent.com/u/83462065?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lilulilulilu",
            "html_url": "https://github.com/lilulilulilu",
            "followers_url": "https://api.github.com/users/lilulilulilu/followers",
            "following_url": "https://api.github.com/users/lilulilulilu/following{/other_user}",
            "gists_url": "https://api.github.com/users/lilulilulilu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lilulilulilu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lilulilulilu/subscriptions",
            "organizations_url": "https://api.github.com/users/lilulilulilu/orgs",
            "repos_url": "https://api.github.com/users/lilulilulilu/repos",
            "events_url": "https://api.github.com/users/lilulilulilu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lilulilulilu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-07-07T07:04:52Z",
        "updated_at": "2023-10-15T21:39:27Z",
        "closed_at": "2023-10-15T21:39:27Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6769",
            "html_url": "https://github.com/run-llama/llama_index/pull/6769",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6769.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6769.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6769/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6769/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6768",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6768/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6768/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6768/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6768",
        "id": 1792861822,
        "node_id": "I_kwDOIWuq585q3OZ-",
        "number": 6768,
        "title": "[Question]: How to set HuggingFaceLLM for \"tiiuae/falcon-7b-instruct\"?",
        "user": {
            "login": "wodecki",
            "id": 14348685,
            "node_id": "MDQ6VXNlcjE0MzQ4Njg1",
            "avatar_url": "https://avatars.githubusercontent.com/u/14348685?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wodecki",
            "html_url": "https://github.com/wodecki",
            "followers_url": "https://api.github.com/users/wodecki/followers",
            "following_url": "https://api.github.com/users/wodecki/following{/other_user}",
            "gists_url": "https://api.github.com/users/wodecki/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wodecki/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wodecki/subscriptions",
            "organizations_url": "https://api.github.com/users/wodecki/orgs",
            "repos_url": "https://api.github.com/users/wodecki/repos",
            "events_url": "https://api.github.com/users/wodecki/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wodecki/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-07T06:12:51Z",
        "updated_at": "2023-07-07T16:27:46Z",
        "closed_at": "2023-07-07T16:27:45Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI want to use \"tiiuae/falcon-7b-instruct\" as my CustomLLM.\r\n\r\nThe model card is here: https://huggingface.co/tiiuae/falcon-7b-instruct\r\n\r\nThe Huggingface implementation looks like this:\r\n```\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\nimport torch\r\n\r\nmodel = \"tiiuae/falcon-7b-instruct\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\npipeline = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    torch_dtype=torch.bfloat16,\r\n    trust_remote_code=True,\r\n    device_map=\"auto\",\r\n)\r\nsequences = pipeline(\r\n   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\r\n    max_length=200,\r\n    do_sample=True,\r\n    top_k=10,\r\n    num_return_sequences=1,\r\n    eos_token_id=tokenizer.eos_token_id,\r\n)\r\nfor seq in sequences:\r\n    print(f\"Result: {seq['generated_text']}\")\r\n```\r\n\r\nHow to adopt the example below to work with falcon-7b-instruct? I struggle with pipeline parameters, model kwargs, etc. \r\n\r\n```\r\n# setup prompts - specific to StableLM\r\nfrom llama_index.prompts.prompts import SimpleInputPrompt\r\n\r\nsystem_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\r\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\r\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\r\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\r\n- StableLM will refuse to participate in anything that could harm a human.\r\n\"\"\"\r\n\r\n# This will wrap the default prompts that are internal to llama-index\r\nquery_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\r\n\r\nimport torch\r\n\r\nllm = HuggingFaceLLM(\r\n    context_window=4096,\r\n    max_new_tokens=256,\r\n    temperature=0.7,\r\n    do_sample=False,\r\n    system_prompt=system_prompt,\r\n    query_wrapper_prompt=query_wrapper_prompt,\r\n    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\r\n    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\r\n    device_map=\"auto\",\r\n    stopping_ids=[50278, 50279, 50277, 1, 0],\r\n    tokenizer_kwargs={\"max_length\": 4096},\r\n    # uncomment this if using CUDA to reduce memory usage\r\n    model_kwargs={\"torch_dtype\": torch.float16}\r\n)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6768/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6768/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6767",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6767/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6767/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6767/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6767",
        "id": 1792571666,
        "node_id": "PR_kwDOIWuq585U3X1a",
        "number": 6767,
        "title": "Fix imports",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-07T01:58:21Z",
        "updated_at": "2023-07-07T03:44:12Z",
        "closed_at": "2023-07-07T03:44:12Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6767",
            "html_url": "https://github.com/run-llama/llama_index/pull/6767",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6767.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6767.patch",
            "merged_at": "2023-07-07T03:44:12Z"
        },
        "body": "Add missing imports to `llama_index.llms` module level ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6767/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6767/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6766",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6766/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6766/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6766/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6766",
        "id": 1792550147,
        "node_id": "PR_kwDOIWuq585U3TM0",
        "number": 6766,
        "title": "fix llm metadata",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-07T01:33:38Z",
        "updated_at": "2023-07-07T01:52:53Z",
        "closed_at": "2023-07-07T01:52:52Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6766",
            "html_url": "https://github.com/run-llama/llama_index/pull/6766",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6766.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6766.patch",
            "merged_at": "2023-07-07T01:52:52Z"
        },
        "body": "# Description\r\n\r\nIf you set num_output or context_window in the service context, it will error out because `llm_metadata` is a pydantic class now, not a dataclass\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] Tested in terminal\r\n- [x] I stared at the code and made sure it makes sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6766/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6766/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6765",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6765/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6765/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6765/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6765",
        "id": 1792546456,
        "node_id": "I_kwDOIWuq585q2BaY",
        "number": 6765,
        "title": "[Question]: When I used colab, the query didn't answer",
        "user": {
            "login": "HopeZhi",
            "id": 137783782,
            "node_id": "U_kgDOCDZp5g",
            "avatar_url": "https://avatars.githubusercontent.com/u/137783782?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/HopeZhi",
            "html_url": "https://github.com/HopeZhi",
            "followers_url": "https://api.github.com/users/HopeZhi/followers",
            "following_url": "https://api.github.com/users/HopeZhi/following{/other_user}",
            "gists_url": "https://api.github.com/users/HopeZhi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/HopeZhi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/HopeZhi/subscriptions",
            "organizations_url": "https://api.github.com/users/HopeZhi/orgs",
            "repos_url": "https://api.github.com/users/HopeZhi/repos",
            "events_url": "https://api.github.com/users/HopeZhi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/HopeZhi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-07-07T01:28:46Z",
        "updated_at": "2023-07-07T08:21:27Z",
        "closed_at": "2023-07-07T08:21:27Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nhalo\uff0ceditors.\r\nWhen I used colab, the query didn't answer,just like this  \r\n![image](https://github.com/jerryjliu/llama_index/assets/137783782/ba8f9f72-790c-49a1-8bc8-c131067dcfc9)\r\n![image](https://github.com/jerryjliu/llama_index/assets/137783782/f5be762e-eef5-4940-975b-1d923ad59e1d)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6765/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6765/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6764",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6764/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6764/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6764/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6764",
        "id": 1792546299,
        "node_id": "PR_kwDOIWuq585U3SXa",
        "number": 6764,
        "title": "feat(node_parser): `MetadataExtractor` - Feature Augmentation via node parser post-processing",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 8,
        "created_at": "2023-07-07T01:28:31Z",
        "updated_at": "2023-07-08T15:36:08Z",
        "closed_at": "2023-07-08T15:36:08Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6764",
            "html_url": "https://github.com/run-llama/llama_index/pull/6764",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6764.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6764.patch",
            "merged_at": "2023-07-08T15:36:08Z"
        },
        "body": "# Description\r\n\r\nRendered link to notebook: \r\nhttps://github.com/jerryjliu/llama_index/blob/56060f7c3ee1643d6cdca0fd24c9d4b96acecf60/docs/examples/metadata_extraction/MetadataExtractionSEC.ipynb\r\n\r\nFixes https://github.com/jerryjliu/llama_index/issues/6627\r\nPartial implementation: https://github.com/jerryjliu/llama_index/issues/6625 (needs further evaluation)\r\n\r\nConcerns:\r\n(outdated)\r\n<details>\r\n1. Document title works well, but other features not so well and may be considered \"experimental features\". How could we indicate this?\r\n2. In order to deal with the scenario where single document may be split apart into multiple (as with PDF pages), we apply the document title to the entire set of documents. However, this may not be the best default. Ideally, the user can structure their documents that preserves the hierarchical parent child relationship. Another way is to apply document title to the nodes with the same file name. However, file name may not be available. This is quite a concerning problem. \r\n</details>\r\n\r\n## Type of Change\r\n\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\n\r\n- [x] Added new notebook (that tests end-to-end)\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [x] My changes generate no new warnings\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6764/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6764/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6763",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6763/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6763/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6763/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6763",
        "id": 1792492766,
        "node_id": "I_kwDOIWuq585q10Te",
        "number": 6763,
        "title": "[Bug]: TypeError: replace() should be called on dataclass instances",
        "user": {
            "login": "kiasar",
            "id": 23178294,
            "node_id": "MDQ6VXNlcjIzMTc4Mjk0",
            "avatar_url": "https://avatars.githubusercontent.com/u/23178294?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kiasar",
            "html_url": "https://github.com/kiasar",
            "followers_url": "https://api.github.com/users/kiasar/followers",
            "following_url": "https://api.github.com/users/kiasar/following{/other_user}",
            "gists_url": "https://api.github.com/users/kiasar/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kiasar/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kiasar/subscriptions",
            "organizations_url": "https://api.github.com/users/kiasar/orgs",
            "repos_url": "https://api.github.com/users/kiasar/repos",
            "events_url": "https://api.github.com/users/kiasar/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kiasar/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-07T00:11:26Z",
        "updated_at": "2023-07-07T02:17:09Z",
        "closed_at": "2023-07-07T02:16:43Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI'm trying my best to replicate the[ official custom LLM example](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced) but I find it hard to do so!\r\n\r\nWith Llama_index version 0.7.2, first I changed the import to this to make it work:\r\n```\r\nfrom llama_index.llms import CustomLLM\r\nfrom llama_index.llms.base import CompletionResponse, LLMMetadata, CompletionResponseGen\r\n```\r\nP.S. Note that CompletionResponseGen is not even imported in the example.\r\n\r\nBut now I get this error: \r\n`TypeError: replace() should be called on dataclass instances`\r\nWhich is from these lines:\r\n```\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=llm,\r\n    context_window=context_window,\r\n    num_output=num_output\r\n)\r\n```\r\n\r\nThis is the full error if that helps:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/afs/csail.mit.edu/u/z/zahra/PyCharm/Document_filler/main.py\", line 135, in <module>\r\n    service_context = ServiceContext.from_defaults(\r\n  File \"/afs/csail.mit.edu/u/z/zahra/.virtualenvs/MIT_prune/lib/python3.10/site-packages/llama_index/indices/service_context.py\", line 140, in from_defaults\r\n    prompt_helper = prompt_helper or _get_default_prompt_helper(\r\n  File \"/afs/csail.mit.edu/u/z/zahra/.virtualenvs/MIT_prune/lib/python3.10/site-packages/llama_index/indices/service_context.py\", line 42, in _get_default_prompt_helper\r\n    llm_metadata = dataclasses.replace(llm_metadata, context_window=context_window)\r\n  File \"/usr/lib/python3.10/dataclasses.py\", line 1425, in replace\r\n    raise TypeError(\"replace() should be called on dataclass instances\")\r\nTypeError: replace() should be called on dataclass instances\r\n```\n\n### Version\n\n0.7.2\n\n### Steps to Reproduce\n\nRun the official example (but first correct the imports)\r\n\r\nhttps://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6763/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6763/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6762",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6762/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6762/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6762/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6762",
        "id": 1792463771,
        "node_id": "PR_kwDOIWuq585U3BII",
        "number": 6762,
        "title": "Update __init__.py",
        "user": {
            "login": "kiasar",
            "id": 23178294,
            "node_id": "MDQ6VXNlcjIzMTc4Mjk0",
            "avatar_url": "https://avatars.githubusercontent.com/u/23178294?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kiasar",
            "html_url": "https://github.com/kiasar",
            "followers_url": "https://api.github.com/users/kiasar/followers",
            "following_url": "https://api.github.com/users/kiasar/following{/other_user}",
            "gists_url": "https://api.github.com/users/kiasar/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kiasar/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kiasar/subscriptions",
            "organizations_url": "https://api.github.com/users/kiasar/orgs",
            "repos_url": "https://api.github.com/users/kiasar/repos",
            "events_url": "https://api.github.com/users/kiasar/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kiasar/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-06T23:35:28Z",
        "updated_at": "2023-07-07T03:44:05Z",
        "closed_at": "2023-07-07T03:44:05Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6762",
            "html_url": "https://github.com/run-llama/llama_index/pull/6762",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6762.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6762.patch",
            "merged_at": null
        },
        "body": "Updating imports.\r\nAs an official example of the \"Custom LLM Model\" indicates, the two classes `CompletionResponse` and `LLMMetadata` should be accessible via `llama_index.llms`.\r\n\r\n# Description\r\n\r\nThis PR is addressing an inconsistency found within the llama_index.llms module. Currently, two classes namely CompletionResponse and LLMMetadata aren't imported via this module while the official \"Custom LLM Model\" example indicates they should be.\r\nThis issue might cause unexpected errors for users who are trying to use these classes as per the documentation, hence the necessity of this change.\r\n\r\nFixes # 6753\r\n\r\n## Type of Change\r\n\r\n- [ *] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nIt is trivial. It is just an import change.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6762/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6762/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6761",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6761/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6761/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6761/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6761",
        "id": 1792370832,
        "node_id": "PR_kwDOIWuq585U2sDl",
        "number": 6761,
        "title": "fix required function fields",
        "user": {
            "login": "yisding",
            "id": 1209314,
            "node_id": "MDQ6VXNlcjEyMDkzMTQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1209314?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yisding",
            "html_url": "https://github.com/yisding",
            "followers_url": "https://api.github.com/users/yisding/followers",
            "following_url": "https://api.github.com/users/yisding/following{/other_user}",
            "gists_url": "https://api.github.com/users/yisding/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yisding/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yisding/subscriptions",
            "organizations_url": "https://api.github.com/users/yisding/orgs",
            "repos_url": "https://api.github.com/users/yisding/repos",
            "events_url": "https://api.github.com/users/yisding/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yisding/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-06T22:22:44Z",
        "updated_at": "2023-07-07T18:10:46Z",
        "closed_at": "2023-07-07T18:10:45Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6761",
            "html_url": "https://github.com/run-llama/llama_index/pull/6761",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6761.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6761.patch",
            "merged_at": "2023-07-07T18:10:45Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [X ] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [X ] Added new unit/integration tests\r\n\r\n# Suggested Checklist:\r\n\r\n- [X ] I have performed a self-review of my own code\r\n- [X ] My changes generate no new warnings\r\n- [X ] I have added tests that prove my fix is effective or that my feature works\r\n- [X ] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6761/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6761/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6760",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6760/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6760/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6760/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6760",
        "id": 1792319274,
        "node_id": "I_kwDOIWuq585q1J8q",
        "number": 6760,
        "title": "[Feature Request]: Configurable DynamoDB url",
        "user": {
            "login": "louislivingston",
            "id": 89083670,
            "node_id": "MDQ6VXNlcjg5MDgzNjcw",
            "avatar_url": "https://avatars.githubusercontent.com/u/89083670?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/louislivingston",
            "html_url": "https://github.com/louislivingston",
            "followers_url": "https://api.github.com/users/louislivingston/followers",
            "following_url": "https://api.github.com/users/louislivingston/following{/other_user}",
            "gists_url": "https://api.github.com/users/louislivingston/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/louislivingston/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/louislivingston/subscriptions",
            "organizations_url": "https://api.github.com/users/louislivingston/orgs",
            "repos_url": "https://api.github.com/users/louislivingston/repos",
            "events_url": "https://api.github.com/users/louislivingston/events{/privacy}",
            "received_events_url": "https://api.github.com/users/louislivingston/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-06T21:53:52Z",
        "updated_at": "2023-08-25T17:33:16Z",
        "closed_at": "2023-08-25T17:33:16Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nPlease add the ability to set the url for DynamoDB. A suggestion would be to get it from environment variables\n\n### Reason\n\nI am interested in using the DynamoDB implementation for document storage, however, I noticed that the boto3 initialization does not give me the ability to specify the url to use for DynamoDB.\n\n### Value of Feature\n\nThis would help out greatly in development environments where DynamoDB is hosted locally.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6760/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6760/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6759",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6759/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6759/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6759/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6759",
        "id": 1792177127,
        "node_id": "I_kwDOIWuq585q0nPn",
        "number": 6759,
        "title": "How to Integrate Cache into LLM Predictor",
        "user": {
            "login": "BharahthyKannan",
            "id": 17752392,
            "node_id": "MDQ6VXNlcjE3NzUyMzky",
            "avatar_url": "https://avatars.githubusercontent.com/u/17752392?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BharahthyKannan",
            "html_url": "https://github.com/BharahthyKannan",
            "followers_url": "https://api.github.com/users/BharahthyKannan/followers",
            "following_url": "https://api.github.com/users/BharahthyKannan/following{/other_user}",
            "gists_url": "https://api.github.com/users/BharahthyKannan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BharahthyKannan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BharahthyKannan/subscriptions",
            "organizations_url": "https://api.github.com/users/BharahthyKannan/orgs",
            "repos_url": "https://api.github.com/users/BharahthyKannan/repos",
            "events_url": "https://api.github.com/users/BharahthyKannan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BharahthyKannan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-06T20:19:07Z",
        "updated_at": "2023-07-12T22:58:46Z",
        "closed_at": "2023-07-12T22:58:46Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIn a recent version, ChatGPTLLMPredictor is removed. We wired the GPT cache with the llmpredictor. Now it is breaking. What is the alternate way to integrate GPT cache with the LLM predictor ? ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6759/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6759/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6758",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6758/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6758/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6758/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6758",
        "id": 1791299132,
        "node_id": "I_kwDOIWuq585qxQ48",
        "number": 6758,
        "title": "[Bug]: validation error for CompletionResponse in CustomLLM implementation",
        "user": {
            "login": "wodecki",
            "id": 14348685,
            "node_id": "MDQ6VXNlcjE0MzQ4Njg1",
            "avatar_url": "https://avatars.githubusercontent.com/u/14348685?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wodecki",
            "html_url": "https://github.com/wodecki",
            "followers_url": "https://api.github.com/users/wodecki/followers",
            "following_url": "https://api.github.com/users/wodecki/following{/other_user}",
            "gists_url": "https://api.github.com/users/wodecki/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wodecki/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wodecki/subscriptions",
            "organizations_url": "https://api.github.com/users/wodecki/orgs",
            "repos_url": "https://api.github.com/users/wodecki/repos",
            "events_url": "https://api.github.com/users/wodecki/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wodecki/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-06T10:46:50Z",
        "updated_at": "2023-07-06T17:03:33Z",
        "closed_at": "2023-07-06T17:03:33Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI want to reproduce [HuggingFace LLM - StableLM](https://gpt-index.readthedocs.io/en/latest/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.html)\r\n\r\n`response = query_engine.query(\"What did the author do growing up?\")`\r\n\r\ngenerates:\r\nValidationError: 1 validation error for CompletionResponse raw value is not a valid dict (type=type_error.dict)\r\n\r\nThe same problem persists for Camel-5B\r\n\r\nRemark: for StabilityAI/stablelm-tuned-alpha-3b I had to comment-out temperature and do_sample\n\n### Version\n\n0.7.1\n\n### Steps to Reproduce\n\nimport logging\r\nimport sys\r\n\r\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\r\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\r\n\r\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\r\nfrom llama_index.llms import HuggingFaceLLM\r\n\r\ndocuments = SimpleDirectoryReader('data').load_data()\r\n\r\n# setup prompts - specific to StableLM\r\nfrom llama_index.prompts.prompts import SimpleInputPrompt\r\n\r\nsystem_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\r\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\r\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\r\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\r\n- StableLM will refuse to participate in anything that could harm a human.\r\n\"\"\"\r\n\r\n# This will wrap the default prompts that are internal to llama-index\r\nquery_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\r\n\r\nimport torch\r\n\r\nllm = HuggingFaceLLM(\r\n    context_window=4096,\r\n    max_new_tokens=256,\r\n    #temperature=0.7,\r\n    #do_sample=False,\r\n    system_prompt=system_prompt,\r\n    query_wrapper_prompt=query_wrapper_prompt,\r\n    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\r\n    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\r\n    device_map=\"auto\",\r\n    stopping_ids=[50278, 50279, 50277, 1, 0],\r\n    tokenizer_kwargs={\"max_length\": 4096},\r\n    # uncomment this if using CUDA to reduce memory usage\r\n    model_kwargs={\"torch_dtype\": torch.float16}\r\n)\r\nservice_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)\r\n\r\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\r\n\r\nquery_engine = index.as_query_engine()\r\nresponse = query_engine.query(\"What did the author do growing up?\")\n\n### Relevant Logs/Tracbacks\n\n```shell\n---------------------------------------------------------------------------\r\nValidationError                           Traceback (most recent call last)\r\n/mnt/llm/llamaindex/2.2 - error.py in line 2\r\n      49 query_engine = index.as_query_engine()\r\n----> 50 response = query_engine.query(\"What did the author do growing up?\")\r\n\r\nFile ~/.conda/envs/llamaindex/lib/python3.10/site-packages/llama_index/indices/query/base.py:23, in BaseQueryEngine.query(self, str_or_query_bundle)\r\n     21 if isinstance(str_or_query_bundle, str):\r\n     22     str_or_query_bundle = QueryBundle(str_or_query_bundle)\r\n---> 23 response = self._query(str_or_query_bundle)\r\n     24 return response\r\n\r\nFile ~/.conda/envs/llamaindex/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:152, in RetrieverQueryEngine._query(self, query_bundle)\r\n    145 nodes = self._retriever.retrieve(query_bundle)\r\n    146 self.callback_manager.on_event_end(\r\n    147     CBEventType.RETRIEVE,\r\n    148     payload={EventPayload.NODES: nodes},\r\n    149     event_id=retrieve_id,\r\n    150 )\r\n--> 152 response = self._response_synthesizer.synthesize(\r\n    153     query=query_bundle,\r\n    154     nodes=nodes,\r\n    155 )\r\n    157 self.callback_manager.on_event_end(\r\n    158     CBEventType.QUERY,\r\n    159     payload={EventPayload.RESPONSE: response},\r\n    160     event_id=query_id,\r\n    161 )\r\n    162 return response\r\n\r\nFile ~/.conda/envs/llamaindex/lib/python3.10/site-packages/llama_index/response_synthesizers/base.py:124, in BaseSynthesizer.synthesize(self, query, nodes, additional_source_nodes)\r\n    121 if isinstance(query, str):\r\n    122     query = QueryBundle(query_str=query)\r\n--> 124 response_str = self.get_response(\r\n    125     query_str=query.query_str,\r\n    126     text_chunks=[\r\n    127         n.node.get_content(metadata_mode=MetadataMode.LLM) for n in nodes\r\n    128     ],\r\n    129 )\r\n    131 additional_source_nodes = additional_source_nodes or []\r\n    132 source_nodes = list(nodes) + list(additional_source_nodes)\r\n\r\nFile ~/.conda/envs/llamaindex/lib/python3.10/site-packages/llama_index/response_synthesizers/compact_and_refine.py:34, in CompactAndRefine.get_response(self, query_str, text_chunks, **response_kwargs)\r\n     30 # use prompt helper to fix compact text_chunks under the prompt limitation\r\n     31 # TODO: This is a temporary fix - reason it's temporary is that\r\n     32 # the refine template does not account for size of previous answer.\r\n     33 new_texts = self._make_compact_text_chunks(query_str, text_chunks)\r\n---> 34 response = super().get_response(\r\n     35     query_str=query_str, text_chunks=new_texts, **response_kwargs\r\n     36 )\r\n     37 return response\r\n\r\nFile ~/.conda/envs/llamaindex/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:49, in Refine.get_response(self, query_str, text_chunks, **response_kwargs)\r\n     45 for text_chunk in text_chunks:\r\n     46     if prev_response_obj is None:\r\n     47         # if this is the first chunk, and text chunk already\r\n     48         # is an answer, then return it\r\n---> 49         response = self._give_response_single(\r\n     50             query_str,\r\n     51             text_chunk,\r\n     52         )\r\n     53     else:\r\n     54         response = self._refine_response_single(\r\n     55             prev_response_obj, query_str, text_chunk\r\n     56         )\r\n\r\nFile ~/.conda/envs/llamaindex/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py:80, in Refine._give_response_single(self, query_str, text_chunk, **response_kwargs)\r\n     78 for cur_text_chunk in text_chunks:\r\n     79     if response is None and not self._streaming:\r\n---> 80         response = self._service_context.llm_predictor.predict(\r\n     81             text_qa_template,\r\n     82             context_str=cur_text_chunk,\r\n     83         )\r\n     84     elif response is None and self._streaming:\r\n     85         response = self._service_context.llm_predictor.stream(\r\n     86             text_qa_template,\r\n     87             context_str=cur_text_chunk,\r\n     88         )\r\n\r\nFile ~/.conda/envs/llamaindex/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:123, in LLMPredictor.predict(self, prompt, **prompt_args)\r\n    121 else:\r\n    122     formatted_prompt = prompt.format(llm=self._llm, **prompt_args)\r\n--> 123     response = self._llm.complete(formatted_prompt)\r\n    124     output = response.text\r\n    126 logger.debug(output)\r\n\r\nFile ~/.conda/envs/llamaindex/lib/python3.10/site-packages/llama_index/llms/huggingface.py:128, in HuggingFaceLLM.complete(self, prompt, **kwargs)\r\n    125 self._total_tokens_used += len(completion_tokens) + inputs[\"input_ids\"].size(1)\r\n    126 completion = self.tokenizer.decode(completion_tokens, skip_special_tokens=True)\r\n--> 128 return CompletionResponse(text=completion, raw=tokens)\r\n\r\nFile ~/.conda/envs/llamaindex/lib/python3.10/site-packages/pydantic/main.py:341, in pydantic.main.BaseModel.__init__()\r\n\r\nValidationError: 1 validation error for CompletionResponse\r\nraw\r\n  value is not a valid dict (type=type_error.dict)\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6758/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6758/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6757",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6757/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6757/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6757/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6757",
        "id": 1790985044,
        "node_id": "PR_kwDOIWuq585Ux-LN",
        "number": 6757,
        "title": "[version] bump version to 0.7.2",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-06T07:32:06Z",
        "updated_at": "2023-07-06T15:40:05Z",
        "closed_at": "2023-07-06T15:40:04Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6757",
            "html_url": "https://github.com/run-llama/llama_index/pull/6757",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6757.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6757.patch",
            "merged_at": "2023-07-06T15:40:04Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6757/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6757/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6756",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6756/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6756/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6756/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6756",
        "id": 1790921962,
        "node_id": "I_kwDOIWuq585qv0zq",
        "number": 6756,
        "title": "openai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. ",
        "user": {
            "login": "pradeepdev-1995",
            "id": 41164884,
            "node_id": "MDQ6VXNlcjQxMTY0ODg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/41164884?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pradeepdev-1995",
            "html_url": "https://github.com/pradeepdev-1995",
            "followers_url": "https://api.github.com/users/pradeepdev-1995/followers",
            "following_url": "https://api.github.com/users/pradeepdev-1995/following{/other_user}",
            "gists_url": "https://api.github.com/users/pradeepdev-1995/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pradeepdev-1995/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pradeepdev-1995/subscriptions",
            "organizations_url": "https://api.github.com/users/pradeepdev-1995/orgs",
            "repos_url": "https://api.github.com/users/pradeepdev-1995/repos",
            "events_url": "https://api.github.com/users/pradeepdev-1995/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pradeepdev-1995/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-06T06:45:24Z",
        "updated_at": "2023-07-06T17:27:37Z",
        "closed_at": "2023-07-06T17:27:37Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nI am using the AzureOpenAI llm. Not openai LLM directly.\r\nI given the all azure open ai credentails in the code including azure open ai key . But it still shows the error \r\n\r\n```\r\nopenai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\r\n```\r\n\r\nand \r\n```\r\ntenacity.RetryError: RetryError[<Future at 0x7f99f9ea6ca0 state=finished raised AuthenticationError>] \r\n```\r\n\r\n### Version\r\n\r\nllama-index-0.7.1\r\n\r\n### Steps to Reproduce\r\n\r\n```\r\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader,LLMPredictor, ServiceContext\r\nfrom langchain.llms import AzureOpenAI\r\nimport os\r\nimport openai\r\n\r\nos.environ[\"OPENAI_API_TYPE\"] = \"type\"\r\nos.environ[\"OPENAI_API_VERSION\"] = \"version\"\r\nos.environ[\"OPENAI_API_BASE\"] = \"api_base\"\r\nos.environ[\"OPENAI_API_KEY\"] = \"azure_open_ai_key\"\r\n\r\n\r\nllm_predictor = LLMPredictor(llm=AzureOpenAI(aptemperature=0, model_name=\"model_name\"))\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\n\r\n\r\ndocuments = SimpleDirectoryReader('Data/').load_data()\r\ncustom_llm_index = VectorStoreIndex.from_documents(documents,service_context=service_context)\r\ncustom_llm_query_engine = custom_llm_index.as_query_engine()\r\nresponse = custom_llm_query_engine.query(\"who is this text about?\")\r\nprint(response)\r\n```\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6756/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6756/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6755",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6755/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6755/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6755/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6755",
        "id": 1790915667,
        "node_id": "PR_kwDOIWuq585UxvZe",
        "number": 6755,
        "title": "Fix: restore ResponseMode.NO_TEXT when using as_query_engine()",
        "user": {
            "login": "tilleul",
            "id": 3061106,
            "node_id": "MDQ6VXNlcjMwNjExMDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3061106?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tilleul",
            "html_url": "https://github.com/tilleul",
            "followers_url": "https://api.github.com/users/tilleul/followers",
            "following_url": "https://api.github.com/users/tilleul/following{/other_user}",
            "gists_url": "https://api.github.com/users/tilleul/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tilleul/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tilleul/subscriptions",
            "organizations_url": "https://api.github.com/users/tilleul/orgs",
            "repos_url": "https://api.github.com/users/tilleul/repos",
            "events_url": "https://api.github.com/users/tilleul/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tilleul/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-06T06:40:05Z",
        "updated_at": "2023-07-06T17:28:27Z",
        "closed_at": "2023-07-06T17:28:27Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6755",
            "html_url": "https://github.com/run-llama/llama_index/pull/6755",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6755.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6755.patch",
            "merged_at": "2023-07-06T17:28:26Z"
        },
        "body": "# Description\r\n\r\nSince v0.7.0, `ResponseMode.NO_TEXT` is broken when using `index.as_query_engine()`. This is typically used to retrieve top_k nodes without sending a request to openAI (or whatever AI service).\r\n\r\nThis is because there is a better way to get the top_k nodes: use `index.as_retriever()`. Unfortunately, this is not mentioned clearly in the docs and in fact there are several examples in the docs and on github (some in jupyter notebooks) that still use the \"old\" way.\r\n\r\nThis fix restores the old way for the time being. The NoText class is mostly empty and is more a workaround than a real fix.\r\n\r\nThe real fix would be to update all the examples in the docs/github/jupyter notebooks and get rid of ResponseMode.NO_TEXT altogether.\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\nTested locally with my own local copy of llama_index on an app I'm working on that uses `ResponseMode.NO_TEXT`\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] My changes generate no new warnings\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6755/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6755/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6754",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6754/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6754/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6754/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6754",
        "id": 1790882713,
        "node_id": "I_kwDOIWuq585qvrOZ",
        "number": 6754,
        "title": "[Question]: ",
        "user": {
            "login": "sumanthp",
            "id": 11244510,
            "node_id": "MDQ6VXNlcjExMjQ0NTEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/11244510?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sumanthp",
            "html_url": "https://github.com/sumanthp",
            "followers_url": "https://api.github.com/users/sumanthp/followers",
            "following_url": "https://api.github.com/users/sumanthp/following{/other_user}",
            "gists_url": "https://api.github.com/users/sumanthp/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sumanthp/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sumanthp/subscriptions",
            "organizations_url": "https://api.github.com/users/sumanthp/orgs",
            "repos_url": "https://api.github.com/users/sumanthp/repos",
            "events_url": "https://api.github.com/users/sumanthp/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sumanthp/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-06T06:10:39Z",
        "updated_at": "2023-10-12T16:03:31Z",
        "closed_at": "2023-10-12T16:03:30Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nWhat is the reason for milvus vector store to persist the data to local machine when the collections should be persisted in milvus?\r\nCurrently I am persisting the index/collection created in milvus to local storage. When I try to use milvus sdk methods to list collections, those are not available. Are there any alternatives for this",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6754/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6754/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6753",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6753/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6753/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6753/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6753",
        "id": 1790851941,
        "node_id": "I_kwDOIWuq585qvjtl",
        "number": 6753,
        "title": "[Documentation]: Example: Using a Custom LLM Model - Advanced",
        "user": {
            "login": "kiasar",
            "id": 23178294,
            "node_id": "MDQ6VXNlcjIzMTc4Mjk0",
            "avatar_url": "https://avatars.githubusercontent.com/u/23178294?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kiasar",
            "html_url": "https://github.com/kiasar",
            "followers_url": "https://api.github.com/users/kiasar/followers",
            "following_url": "https://api.github.com/users/kiasar/following{/other_user}",
            "gists_url": "https://api.github.com/users/kiasar/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kiasar/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kiasar/subscriptions",
            "organizations_url": "https://api.github.com/users/kiasar/orgs",
            "repos_url": "https://api.github.com/users/kiasar/repos",
            "events_url": "https://api.github.com/users/kiasar/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kiasar/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318866,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/documentation",
                "name": "documentation",
                "color": "0075ca",
                "default": true,
                "description": "Improvements or additions to documentation"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-06T05:42:54Z",
        "updated_at": "2023-07-06T23:39:05Z",
        "closed_at": "2023-07-06T17:21:07Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Documentation Issue Description\n\nMy package is updated (0.7.1). But I still get an error running your example code.\r\n`ImportError: cannot import name 'CustomLLM' from 'llama_index.llms'`\n\n### Documenation Link\n\nhttps://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6753/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6753/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6752",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6752/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6752/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6752/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6752",
        "id": 1790837927,
        "node_id": "PR_kwDOIWuq585UxetS",
        "number": 6752,
        "title": "Add missing `CustomLLM` import under `llama_index.llms`",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-06T05:28:39Z",
        "updated_at": "2023-07-06T05:29:39Z",
        "closed_at": "2023-07-06T05:29:38Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6752",
            "html_url": "https://github.com/run-llama/llama_index/pull/6752",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6752.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6752.patch",
            "merged_at": "2023-07-06T05:29:38Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6752/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6752/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6751",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6751/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6751/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6751/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6751",
        "id": 1790817191,
        "node_id": "PR_kwDOIWuq585UxaMN",
        "number": 6751,
        "title": "Update __init__.py",
        "user": {
            "login": "lilulilulilu",
            "id": 83462065,
            "node_id": "MDQ6VXNlcjgzNDYyMDY1",
            "avatar_url": "https://avatars.githubusercontent.com/u/83462065?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lilulilulilu",
            "html_url": "https://github.com/lilulilulilu",
            "followers_url": "https://api.github.com/users/lilulilulilu/followers",
            "following_url": "https://api.github.com/users/lilulilulilu/following{/other_user}",
            "gists_url": "https://api.github.com/users/lilulilulilu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lilulilulilu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lilulilulilu/subscriptions",
            "organizations_url": "https://api.github.com/users/lilulilulilu/orgs",
            "repos_url": "https://api.github.com/users/lilulilulilu/repos",
            "events_url": "https://api.github.com/users/lilulilulilu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lilulilulilu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-07-06T05:07:10Z",
        "updated_at": "2023-07-06T05:30:08Z",
        "closed_at": "2023-07-06T05:30:07Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6751",
            "html_url": "https://github.com/run-llama/llama_index/pull/6751",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6751.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6751.patch",
            "merged_at": null
        },
        "body": "Traceback (most recent call last):\r\n  File \"/cognitive_comp/lilu/chatpdf/chatPDF/main.py\", line 9, in <module>\r\n    from llama_index.llms import CustomLLM, CompletionResponse, LLMMetadata\r\nImportError: cannot import name 'CustomLLM' from 'llama_index.llms'\r\n\r\n# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6751/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6751/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6750",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6750/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6750/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6750/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6750",
        "id": 1790706461,
        "node_id": "I_kwDOIWuq585qvAMd",
        "number": 6750,
        "title": "[Bug]: ValueError: \"HuggingFaceEmbeddings\" object has no field \"callback_manager\"",
        "user": {
            "login": "wyzhhhh",
            "id": 105030082,
            "node_id": "U_kgDOBkKhwg",
            "avatar_url": "https://avatars.githubusercontent.com/u/105030082?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wyzhhhh",
            "html_url": "https://github.com/wyzhhhh",
            "followers_url": "https://api.github.com/users/wyzhhhh/followers",
            "following_url": "https://api.github.com/users/wyzhhhh/following{/other_user}",
            "gists_url": "https://api.github.com/users/wyzhhhh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wyzhhhh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wyzhhhh/subscriptions",
            "organizations_url": "https://api.github.com/users/wyzhhhh/orgs",
            "repos_url": "https://api.github.com/users/wyzhhhh/repos",
            "events_url": "https://api.github.com/users/wyzhhhh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wyzhhhh/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-06T03:00:32Z",
        "updated_at": "2023-07-06T17:25:50Z",
        "closed_at": "2023-07-06T17:25:49Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI replaced the original model with the native language model and the embedding model, but I got an error when running code of\r\n**service_context = ServiceContext.from_defaults(\r\n    llm_predictor=llm_predictor,\r\n    embed_model=embed_model,\r\n    context_window=context_window,\r\n    num_output=num_output\r\n)**\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/data/llama-index/Try_llamaindex.py\", line 41, in <module>\r\n    service_context = ServiceContext.from_defaults(\r\n  File \"/root/anaconda3/lib/python3.9/site-packages/llama_index/indices/service_context.py\", line 138, in from_defaults\r\n    embed_model.callback_manager = callback_manager\r\n  File \"pydantic/main.py\", line 357, in pydantic.main.BaseModel.__setattr__\r\nValueError: \"HuggingFaceEmbeddings\" object has no field \"callback_manager\"\n\n### Version\n\nv0.6.38\n\n### Steps to Reproduce\n\nimport torch\r\nfrom langchain.llms.base import LLM\r\nfrom llama_index import LLMPredictor\r\nfrom transformers import pipeline\r\nfrom typing import Optional, List, Mapping, Any\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\n# set context window size\r\ncontext_window = 2048\r\n# set number of output tokens\r\nnum_output = 256\r\n\r\n# store the pipeline/model outisde of the LLM class to avoid memory issues\r\nmodel_name = \"vicuna-chinese\"\r\npipeline = pipeline(\"text-generation\", model=model_name, device=\"cuda:0\", model_kwargs={\"torch_dtype\": torch.bfloat16})\r\n\r\nclass CustomLLM(LLM):\r\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\r\n        prompt_length = len(prompt)\r\n        response = pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\r\n        # only return newly generated tokens\r\n        return response[prompt_length:]\r\n    @property\r\n    def _identifying_params(self) -> Mapping[str, Any]:\r\n        return {\"name_of_model\": model_name}\r\n    @property\r\n    def _llm_type(self) -> str:\r\n        return \"custom\"\r\n\r\n# define our LLM\r\nllm_predictor = LLMPredictor(llm=CustomLLM())\r\n\r\n#from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\r\nfrom llama_index import LangchainEmbedding, ServiceContext\r\n#embed_model = LangchainEmbedding(HuggingFaceInstructEmbeddings())\r\nfrom text2vec import SentenceModel\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\n#embed_model= SentenceModel('./text2vec-base-chinese')\r\nembed_model = HuggingFaceEmbeddings(model_name='./text2vec-base-chinese')\r\n#embed_model = SentenceTransformer(\"./text2vec-base-chinese\")\r\nservice_context = ServiceContext.from_defaults(\r\n    llm_predictor=llm_predictor,\r\n    embed_model=embed_model,\r\n    context_window=context_window,\r\n    num_output=num_output\r\n)\n\n### Relevant Logs/Tracbacks\n\n```shell\nTraceback (most recent call last):\r\n  File \"/home/data/llama-index/Try_llamaindex.py\", line 41, in <module>\r\n    service_context = ServiceContext.from_defaults(\r\n  File \"/root/anaconda3/lib/python3.9/site-packages/llama_index/indices/service_context.py\", line 138, in from_defaults\r\n    embed_model.callback_manager = callback_manager\r\n  File \"pydantic/main.py\", line 357, in pydantic.main.BaseModel.__setattr__\r\nValueError: \"HuggingFaceEmbeddings\" object has no field \"callback_manager\"\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6750/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6750/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6749",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6749/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6749/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6749/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6749",
        "id": 1790677972,
        "node_id": "PR_kwDOIWuq585Uw8DA",
        "number": 6749,
        "title": "patch node postprocessors",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-06T02:27:55Z",
        "updated_at": "2023-07-06T02:33:16Z",
        "closed_at": "2023-07-06T02:33:15Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6749",
            "html_url": "https://github.com/run-llama/llama_index/pull/6749",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6749.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6749.patch",
            "merged_at": "2023-07-06T02:33:15Z"
        },
        "body": "# Description\r\n\r\n`_query()` was skipping the node post processors in q few query engines. \r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6749/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6749/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6748",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6748/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6748/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6748/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6748",
        "id": 1790673114,
        "node_id": "I_kwDOIWuq585qu4Da",
        "number": 6748,
        "title": "[Feature Request]: Dynamically selecting from multiple prompts",
        "user": {
            "login": "123zzw",
            "id": 65516153,
            "node_id": "MDQ6VXNlcjY1NTE2MTUz",
            "avatar_url": "https://avatars.githubusercontent.com/u/65516153?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/123zzw",
            "html_url": "https://github.com/123zzw",
            "followers_url": "https://api.github.com/users/123zzw/followers",
            "following_url": "https://api.github.com/users/123zzw/following{/other_user}",
            "gists_url": "https://api.github.com/users/123zzw/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/123zzw/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/123zzw/subscriptions",
            "organizations_url": "https://api.github.com/users/123zzw/orgs",
            "repos_url": "https://api.github.com/users/123zzw/repos",
            "events_url": "https://api.github.com/users/123zzw/events{/privacy}",
            "received_events_url": "https://api.github.com/users/123zzw/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-06T02:21:49Z",
        "updated_at": "2023-10-14T20:08:29Z",
        "closed_at": "2023-10-14T20:08:28Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nHello, I have an idea. Can we create a Dynamically selecting from multiple prompts based on the Retriever Router Query Engine for different problem types, such as\r\n\r\n\r\nphysics_template = \"\"\"You are a very smart physics professor. \\\r\nYou are great at answering questions about physics in a concise and easy to understand manner. \\\r\nWhen you don't know the answer to a question you admit that you don't know.\r\nHere is a question:\r\n{input}\"\"\"\r\n\r\nmath_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\r\nYou are so good because you are able to break down hard problems into their component parts, \\\r\nanswer the component parts, and then put them together to answer the broader question.\r\nHere is a question:\r\n{input}\"\"\"\r\n\n\n### Reason\n\n_No response_\n\n### Value of Feature\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6748/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6748/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6747",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6747/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6747/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6747/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6747",
        "id": 1790609007,
        "node_id": "PR_kwDOIWuq585UwsvL",
        "number": 6747,
        "title": "Fix missing as_query_engine() in tutorial",
        "user": {
            "login": "nichochar",
            "id": 1666947,
            "node_id": "MDQ6VXNlcjE2NjY5NDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1666947?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nichochar",
            "html_url": "https://github.com/nichochar",
            "followers_url": "https://api.github.com/users/nichochar/followers",
            "following_url": "https://api.github.com/users/nichochar/following{/other_user}",
            "gists_url": "https://api.github.com/users/nichochar/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nichochar/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nichochar/subscriptions",
            "organizations_url": "https://api.github.com/users/nichochar/orgs",
            "repos_url": "https://api.github.com/users/nichochar/repos",
            "events_url": "https://api.github.com/users/nichochar/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nichochar/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-06T01:22:30Z",
        "updated_at": "2023-07-06T16:11:19Z",
        "closed_at": "2023-07-06T16:11:19Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6747",
            "html_url": "https://github.com/run-llama/llama_index/pull/6747",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6747.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6747.patch",
            "merged_at": "2023-07-06T16:11:19Z"
        },
        "body": "# Description\r\n\r\nI was following a guide and had to bugfix the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/nicholascharriere/axilla/docllama/cities/load.py\", line 77, in <module>\r\n    response = vector_indices[\"Paris\"].query(\"What are the sports teams in Paris?\")\r\nAttributeError: 'VectorStoreIndex' object has no attribute 'query'\r\n```\r\n\r\nThe fix is quite simple, we need to convert from a `VectorStoreIndex` object to a query engine. This also ran the black linter. It changes nothing to the output.\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n# How Has This Been Tested?\r\nI am running the code.\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] This is simply a docs change\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6747/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6747/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6746",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6746/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6746/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6746/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6746",
        "id": 1790585831,
        "node_id": "I_kwDOIWuq585quivn",
        "number": 6746,
        "title": "[Question]: how to set the temperature for local LLM",
        "user": {
            "login": "stl2015",
            "id": 11513171,
            "node_id": "MDQ6VXNlcjExNTEzMTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/11513171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stl2015",
            "html_url": "https://github.com/stl2015",
            "followers_url": "https://api.github.com/users/stl2015/followers",
            "following_url": "https://api.github.com/users/stl2015/following{/other_user}",
            "gists_url": "https://api.github.com/users/stl2015/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stl2015/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stl2015/subscriptions",
            "organizations_url": "https://api.github.com/users/stl2015/orgs",
            "repos_url": "https://api.github.com/users/stl2015/repos",
            "events_url": "https://api.github.com/users/stl2015/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stl2015/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-07-06T00:50:51Z",
        "updated_at": "2023-10-14T20:08:34Z",
        "closed_at": "2023-10-14T20:08:33Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nFor custom LLM using LLMPredictor to define index, is it possible to define the temperature in predicting? In definition of LLMPredictor, I only found following where it is possible in OpenAI interface? Thanks.\r\n\r\n```\r\n        self._llm = llm or OpenAI(\r\n            temperature=0, model_name=\"text-davinci-003\", max_tokens=-1\r\n        )\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6746/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6746/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6745",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6745/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6745/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6745/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6745",
        "id": 1790303199,
        "node_id": "PR_kwDOIWuq585Uvoiu",
        "number": 6745,
        "title": "Add sources to subquestion engine",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-07-05T20:57:10Z",
        "updated_at": "2023-07-07T20:50:02Z",
        "closed_at": "2023-07-07T20:50:01Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6745",
            "html_url": "https://github.com/run-llama/llama_index/pull/6745",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6745.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6745.patch",
            "merged_at": "2023-07-07T20:50:01Z"
        },
        "body": "# Description\r\n\r\nThis PR does two things Adds sources to sub-questions for the sub question query engine. This is accomplished by using the new sub-question callback manager events.\r\n\r\nIn the future, would like to move this to a more user-facing UX. But, much more planning is needed for this.\r\n\r\n## Type of Change\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# TODO\r\n\r\n- [x] Confirm this actually works in using the full pipeline with an agent\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6745/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6745/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6744",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6744/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6744/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6744/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6744",
        "id": 1790196707,
        "node_id": "PR_kwDOIWuq585UvQ2O",
        "number": 6744,
        "title": "Only consider LLM content for text splitting",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-05T19:49:47Z",
        "updated_at": "2023-08-28T17:11:39Z",
        "closed_at": "2023-08-08T21:15:22Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6744",
            "html_url": "https://github.com/run-llama/llama_index/pull/6744",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6744.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6744.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nOur text splitters should only consider LLM content/metadata when splitting text. This is because people may put a lot of extra text in the metadata that is only intended to be used for embeddings.\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6744/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6744/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6743",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6743/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6743/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6743/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6743",
        "id": 1790179003,
        "node_id": "PR_kwDOIWuq585UvM2G",
        "number": 6743,
        "title": "support langchain system messages",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-05T19:40:56Z",
        "updated_at": "2023-07-05T19:58:18Z",
        "closed_at": "2023-07-05T19:58:17Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6743",
            "html_url": "https://github.com/run-llama/llama_index/pull/6743",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6743.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6743.patch",
            "merged_at": "2023-07-05T19:58:17Z"
        },
        "body": "# Description\r\n\r\nOur `from_lc_messages()` util function was not using system messages. This is used by users who customized prompt templates for query engines to include system messages.\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] Added new unit/integration tests\r\n- [x] I stared at the code and made sure it makes sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6743/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6743/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6742",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6742/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6742/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6742/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6742",
        "id": 1790156505,
        "node_id": "PR_kwDOIWuq585UvHqP",
        "number": 6742,
        "title": "Set default chat engine mode to use openai agent",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-05T19:32:06Z",
        "updated_at": "2023-08-28T17:11:41Z",
        "closed_at": "2023-07-12T18:45:37Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": true,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6742",
            "html_url": "https://github.com/run-llama/llama_index/pull/6742",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6742.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6742.patch",
            "merged_at": null
        },
        "body": "# Description\r\nSet default chat engine mode to use openai agent\r\n\r\n### Caveat\r\nRight now we use `text-davinci-003` as the default LLM, but we need the newer `gpt-3.5-turbo-0613` to use the openai agent. This creates an extra step required (setting LLM) before the user can run `index.as_chat_engine(...)`\r\n\r\nIMO ideally, we can move to using `gpt-3.5-turbo-0613` as the default LLM everywhere in llama index. Right now this is undesirable since our prompts don't work well with the new turbo model.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6742/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6742/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6741",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6741/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6741/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6741/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6741",
        "id": 1790111054,
        "node_id": "PR_kwDOIWuq585Uu9q6",
        "number": 6741,
        "title": "trulens integration",
        "user": {
            "login": "joshreini1",
            "id": 60949774,
            "node_id": "MDQ6VXNlcjYwOTQ5Nzc0",
            "avatar_url": "https://avatars.githubusercontent.com/u/60949774?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/joshreini1",
            "html_url": "https://github.com/joshreini1",
            "followers_url": "https://api.github.com/users/joshreini1/followers",
            "following_url": "https://api.github.com/users/joshreini1/following{/other_user}",
            "gists_url": "https://api.github.com/users/joshreini1/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/joshreini1/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/joshreini1/subscriptions",
            "organizations_url": "https://api.github.com/users/joshreini1/orgs",
            "repos_url": "https://api.github.com/users/joshreini1/repos",
            "events_url": "https://api.github.com/users/joshreini1/events{/privacy}",
            "received_events_url": "https://api.github.com/users/joshreini1/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-05T18:59:40Z",
        "updated_at": "2023-07-07T06:02:59Z",
        "closed_at": "2023-07-07T06:02:59Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6741",
            "html_url": "https://github.com/run-llama/llama_index/pull/6741",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6741.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6741.patch",
            "merged_at": "2023-07-07T06:02:59Z"
        },
        "body": "# Description\r\n\r\nAdd TruLens integration to the docs!\r\n\r\n- [X] This change requires a documentation update",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6741/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6741/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6740",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6740/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6740/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6740/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6740",
        "id": 1789977986,
        "node_id": "I_kwDOIWuq585qsOWC",
        "number": 6740,
        "title": "[Bug]: VectorIndexRetriever filtering not working as intended for 'node_ids' and 'doc_ids' parameters",
        "user": {
            "login": "xmalina",
            "id": 5224095,
            "node_id": "MDQ6VXNlcjUyMjQwOTU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5224095?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xmalina",
            "html_url": "https://github.com/xmalina",
            "followers_url": "https://api.github.com/users/xmalina/followers",
            "following_url": "https://api.github.com/users/xmalina/following{/other_user}",
            "gists_url": "https://api.github.com/users/xmalina/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xmalina/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xmalina/subscriptions",
            "organizations_url": "https://api.github.com/users/xmalina/orgs",
            "repos_url": "https://api.github.com/users/xmalina/repos",
            "events_url": "https://api.github.com/users/xmalina/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xmalina/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-05T17:19:51Z",
        "updated_at": "2023-09-29T08:22:16Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nPossible bug in VectorIndexRetriever? \r\n\r\nI've tried both llamaindex 0.6.7 and 0.7.1 -- but in both the node_ids and doc_ids filters are not working at all as a filter. \r\n\r\nNo matter what, it always returns all nodes in the index, it doesn't filter them only to the node_ids or doc_ids provided in the parameter, which is how the documentation suggests to use it. \r\n\r\nVectorIndexRetriever(index, similarity_top_k=10,doc_ids=list_of_doc_ids).retrieve(\"hello\")\r\nVectorIndexRetriever(index, similarity_top_k=10,node_ids=list_of_node_ids).retrieve(\"hello\")\r\n\r\nBoth return results independent of the provided doc_ids or node_ids, using TextNodes in a SimpleVectoreStore.\n\n### Version\n\n0.7.1\n\n### Steps to Reproduce\n\n```\r\n#note: Index is a SimpleVectorIndex but this may also affect other indexes.\r\n\r\nfiltered_list_of_node_ids = [n.node_id for n in nodes_][:3]\r\nVectorIndexRetriever(index, similarity_top_k=1,node_ids=filtered_list_of_node_ids).retrieve(\"hello\")\r\n\r\n#expected result:\r\n#1 node_id from within the filtered_list_of_node_ids\r\n\r\n#Current result:\r\n#node_id not included in the filtered_list_of_node_ids\r\n```\r\n\r\n\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6740/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6740/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6739",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6739/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6739/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6739/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6739",
        "id": 1789966017,
        "node_id": "PR_kwDOIWuq585UueUe",
        "number": 6739,
        "title": "feat(test): add `Document.example()`",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-05T17:10:21Z",
        "updated_at": "2023-07-13T04:52:29Z",
        "closed_at": "2023-07-13T04:52:29Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6739",
            "html_url": "https://github.com/run-llama/llama_index/pull/6739",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6739.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6739.patch",
            "merged_at": "2023-07-13T04:52:28Z"
        },
        "body": "# Description\r\n\r\nspeed up dev loop with a semantically meaningful example\r\n\r\nTLDR:\r\n\r\n```python\r\nfrom llama_index import Document, VectorStoreIndex\r\nindex = VectorStoreIndex.from_documents([Document.example()])\r\n\r\n# Try interacting with the document somehow\r\n```\r\n\r\n## Type of Change\r\n\r\n- [X] New feature (non-breaking change which adds functionality)\r\n\r\nFixes: partial https://github.com/jerryjliu/llama_index/issues/6738\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6739/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6739/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6738",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6738/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6738/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6738/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6738",
        "id": 1789958034,
        "node_id": "I_kwDOIWuq585qsJeS",
        "number": 6738,
        "title": "feat(testing): implement `example` method for all data types",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-05T17:04:06Z",
        "updated_at": "2023-10-19T16:04:30Z",
        "closed_at": "2023-10-19T16:04:30Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nFor data types: semantically meaningful text, images, embeddings etc\r\n\r\nExample:\r\n```python\r\ndocs = [Document.example()]\r\n```\r\n\r\nCaveats: not sure how pythonic this is\r\n\r\nData types:\r\n- [x] Document https://github.com/jerryjliu/llama_index/pull/6739\r\n- [ ] ImageDocument\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6738/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6738/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6737",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6737/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6737/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6737/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6737",
        "id": 1789874552,
        "node_id": "PR_kwDOIWuq585UuKTd",
        "number": 6737,
        "title": "fix raw output for hf llm",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-05T16:09:54Z",
        "updated_at": "2023-07-05T17:17:24Z",
        "closed_at": "2023-07-05T17:17:23Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6737",
            "html_url": "https://github.com/run-llama/llama_index/pull/6737",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6737.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6737.patch",
            "merged_at": "2023-07-05T17:17:23Z"
        },
        "body": "# Description\r\n\r\nThe `CompletionResponse` raw attribute needs to be a dict, but hugging face will output a list.\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6737/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6737/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6736",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6736/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6736/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6736/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6736",
        "id": 1789430325,
        "node_id": "I_kwDOIWuq585qqIo1",
        "number": 6736,
        "title": "LangChain with Conversational memory in ChatBot",
        "user": {
            "login": "tulsipatro",
            "id": 63704720,
            "node_id": "MDQ6VXNlcjYzNzA0NzIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/63704720?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tulsipatro",
            "html_url": "https://github.com/tulsipatro",
            "followers_url": "https://api.github.com/users/tulsipatro/followers",
            "following_url": "https://api.github.com/users/tulsipatro/following{/other_user}",
            "gists_url": "https://api.github.com/users/tulsipatro/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tulsipatro/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tulsipatro/subscriptions",
            "organizations_url": "https://api.github.com/users/tulsipatro/orgs",
            "repos_url": "https://api.github.com/users/tulsipatro/repos",
            "events_url": "https://api.github.com/users/tulsipatro/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tulsipatro/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-05T12:16:09Z",
        "updated_at": "2023-10-12T16:03:45Z",
        "closed_at": "2023-10-12T16:03:45Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nfrom langchain.chains.conversation.memory import ConversationBufferWindowMemory\r\nopenai_llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\r\nconversational_memory = ConversationBufferWindowMemory( memory_key='chat_history', k=5, return_messages=True )\r\nllm_predictor = LLMPredictor(openai_llm)\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=4096)\r\nindex = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\r\n\r\nHow to use my conversational_memory for this piece of code so that I can maintain history of the conversation from the bot?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6736/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6736/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6735",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6735/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6735/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6735/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6735",
        "id": 1789404363,
        "node_id": "I_kwDOIWuq585qqCTL",
        "number": 6735,
        "title": "[Bug]: Updating/Refreshing documents not working correctly with supabase Vector Store",
        "user": {
            "login": "Daniel199438",
            "id": 16019073,
            "node_id": "MDQ6VXNlcjE2MDE5MDcz",
            "avatar_url": "https://avatars.githubusercontent.com/u/16019073?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Daniel199438",
            "html_url": "https://github.com/Daniel199438",
            "followers_url": "https://api.github.com/users/Daniel199438/followers",
            "following_url": "https://api.github.com/users/Daniel199438/following{/other_user}",
            "gists_url": "https://api.github.com/users/Daniel199438/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Daniel199438/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Daniel199438/subscriptions",
            "organizations_url": "https://api.github.com/users/Daniel199438/orgs",
            "repos_url": "https://api.github.com/users/Daniel199438/repos",
            "events_url": "https://api.github.com/users/Daniel199438/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Daniel199438/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-07-05T12:00:05Z",
        "updated_at": "2023-10-23T16:03:15Z",
        "closed_at": "2023-10-23T16:03:14Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nWhen I retrieve documents from Notion pages via page ids,\r\n\r\n`documents = reader.load_data(page_ids=page_ids)`\r\n\r\n each document gets a new unique id each time, \r\n\r\n\r\n\r\n    # TODO: A lot of backwards compatibility logic here, clean up\r\n    id_: str = Field(\r\n        default_factory=lambda: str(uuid.uuid4()),\r\n        description=\"Unique ID of the node.\",\r\n        alias=\"doc_id\",\r\n    )\r\n\r\n\r\n \r\n    \r\n\r\nbut there is also a page id stored in the meta field. These documents are stored in a vector store (in my case Supabase) including the meta data.\r\n\r\n`docs.append(Document(text=page_text, extra_info={\"page_id\": page_id}))`\r\n\r\nNow when I get the same pages from Notion again on the next call, each document is stored with unique ids again. \r\n\r\nSo I can't really update my Notion pages, but instead they are always reinserted into the vectorstore. \r\n\r\n![tmp_51](https://github.com/jerryjliu/llama_index/assets/16019073/05610f71-b3cf-4bf6-b108-454a33b3563d)\r\n\r\n\r\nIt would be enough for me if I could pass the page id as document id instead when creating the documents (I use a customized NotionPageReader) or if I could update a specific document via the meta field instead of the document id.\r\n\r\n\n\n### Version\n\n0.7.0\n\n### Steps to Reproduce\n\n1. Load a page from NotionPageReader\r\n2. Process the page to a document and store it into vector database\r\n3. Repeat it\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6735/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6735/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6734",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6734/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6734/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6734/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6734",
        "id": 1789403557,
        "node_id": "I_kwDOIWuq585qqCGl",
        "number": 6734,
        "title": "[Question]: how to get hypothethetical answer used in HyDE method in the response object?",
        "user": {
            "login": "sid8491",
            "id": 8565062,
            "node_id": "MDQ6VXNlcjg1NjUwNjI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8565062?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sid8491",
            "html_url": "https://github.com/sid8491",
            "followers_url": "https://api.github.com/users/sid8491/followers",
            "following_url": "https://api.github.com/users/sid8491/following{/other_user}",
            "gists_url": "https://api.github.com/users/sid8491/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sid8491/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sid8491/subscriptions",
            "organizations_url": "https://api.github.com/users/sid8491/orgs",
            "repos_url": "https://api.github.com/users/sid8491/repos",
            "events_url": "https://api.github.com/users/sid8491/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sid8491/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-05T11:59:33Z",
        "updated_at": "2023-10-12T16:03:50Z",
        "closed_at": "2023-10-12T16:03:50Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am trying to use HyDE query transform, and I need the intermediate hypothetical answer generated by the answer for embedding matching. How can I get that?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6734/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6734/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6733",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6733/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6733/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6733/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6733",
        "id": 1789393856,
        "node_id": "I_kwDOIWuq585qp_vA",
        "number": 6733,
        "title": "[Bug]: Knowledge graph index embeddings behave as a vector store - embeddings not generating on the nodes",
        "user": {
            "login": "JViggiani",
            "id": 41336980,
            "node_id": "MDQ6VXNlcjQxMzM2OTgw",
            "avatar_url": "https://avatars.githubusercontent.com/u/41336980?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/JViggiani",
            "html_url": "https://github.com/JViggiani",
            "followers_url": "https://api.github.com/users/JViggiani/followers",
            "following_url": "https://api.github.com/users/JViggiani/following{/other_user}",
            "gists_url": "https://api.github.com/users/JViggiani/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/JViggiani/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/JViggiani/subscriptions",
            "organizations_url": "https://api.github.com/users/JViggiani/orgs",
            "repos_url": "https://api.github.com/users/JViggiani/repos",
            "events_url": "https://api.github.com/users/JViggiani/events{/privacy}",
            "received_events_url": "https://api.github.com/users/JViggiani/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-05T11:52:45Z",
        "updated_at": "2023-10-12T16:03:56Z",
        "closed_at": "2023-10-12T16:03:55Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nConstruction:\r\n\r\n```\r\n\r\n\r\n    max_input_size = 4096\r\n    chunk_size_limit = 512\r\n    chunk_overlap_ratio = 0.1\r\n\r\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs, request_timeout=500))\r\nembed_model = OpenAIEmbedding(mode = OpenAIEmbeddingMode.SIMILARITY_MODE, model = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002)\r\n\r\n    prompt_helper = PromptHelper(context_window=max_input_size, num_output=num_outputs, chunk_overlap_ratio=chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)\r\n    \r\n\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)\r\n    \r\n    documents = SimpleDirectoryReader(input_dir=docstore, recursive=True).load_data()\r\n\r\n    kg_triple_extract_template = ... # custom template\r\n\r\n    # Create a partial function with all the arguments filled\r\n    index = GPTKnowledgeGraphIndex.from_documents(\r\n        documents,\r\n        max_triplets_per_chunk=50, \r\n        service_context=service_context,\r\n        kg_triple_extract_template = kg_triple_extract_template,\r\n        include_embeddings = True\r\n    )\r\n```\r\n\r\nQuerying:\r\n\r\n```\r\n    max_input_size = 4096\r\n    chunk_size_limit = 512\r\n    prompt_helper_outputs = 512\r\n    chunk_overlap_ratio = 0.1\r\n    completion_max_outputs = math.floor((max_input_size - chunk_size_limit - prompt_helper_outputs - 250) / 2) # extra 250 for overhead padding, divided by 2 so that the llm can process two wikis at once.\r\n    prompt_helper = PromptHelper(context_window=max_input_size, num_output=prompt_helper_outputs, chunk_overlap_ratio=chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)\r\n    \r\n    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=completion_max_outputs, request_timeout=500))\r\n    embed_model = OpenAIEmbedding(mode = OpenAIEmbeddingMode.SIMILARITY_MODE, model = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002)\r\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=embed_model)\r\n\r\n    # JOSH graph_store_query_depth on KGTableRetriever retriever here\r\n\r\n    retriever = KGTableRetriever(\r\n        index=index,\r\n        similarity_top_k=5,\r\n        graph_store_query_depth = 4,\r\n        retriever_mode = \"hybrid\", \r\n    )\r\n\r\n    query_engine = index.as_query_engine(\r\n\r\n        service_context=service_context,\r\n        include_text=True, \r\n        response_mode=\"compact\",\r\n        retriever=retriever\r\n    )\r\n\r\n    return query_engine\r\n```\r\n\r\nLogs:\r\n\r\n> [2023-07-05 09:46:11,600] {retriever.py:126} INFO - > Starting query: Generate a detailed wikipedia-like article about The Ashen Ones. Include the sections: Introduction, Biology and Appearance, History, Culture and Society\r\n> [2023-07-05 09:46:11,705] {util.py:60} DEBUG - message='Request to OpenAI API' method=post path=https://api.openai.com/v1/completions\r\n> [2023-07-05 09:46:11,705] {util.py:60} DEBUG - api_version=None data='{\"prompt\": [\"A question is provided below. Given the question, extract up to 10 keywords from the text. Focus on extracting the keywords that we can use to best lookup answers to the question. Avoid stopwords.\\\\n---------------------\\\\nGenerate a detailed wikipedia-like article about The Ashen Ones. Include the sections: Introduction, Biology and Appearance, History, Culture and Society\\\\n---------------------\\\\nProvide keywords in the following comma-separated format: \\'KEYWORDS: <keywords>\\'\\\\n\"], \"model\": \"text-davinci-003\", \"temperature\": 0.0, \"max_tokens\": 3996, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"n\": 1, \"logit_bias\": {}}' message='Post details'\r\n> [2023-07-05 09:46:11,706] {retry.py:351} DEBUG - Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\r\n> [2023-07-05 09:46:11,707] {connectionpool.py:1014} DEBUG - Starting new HTTPS connection (1): api.openai.com:443\r\n> [2023-07-05 09:46:16,276] {connectionpool.py:473} DEBUG - https://api.openai.com:443 \"POST /v1/completions HTTP/1.1\" 200 None\r\n> [2023-07-05 09:46:16,278] {util.py:60} DEBUG - message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=3944 request_id=82adbe2da7666be27f4f42c6552059f3 response_code=200\r\n> [2023-07-05 09:46:16,282] {base.py:240} DEBUG -\r\n> KEYWORDS: Ashen Ones, Introduction, Biology, Appearance, History, Culture, Society\r\n> [2023-07-05 09:46:16,415] {retriever.py:129} INFO - > Query keywords: ['Culture', 'Biology', 'History', 'Ones', 'Introduction', 'Ashen', 'Ashen Ones', 'Appearance', 'Society']\r\n> [2023-07-05 09:46:16,416] {retriever.py:164} DEBUG - rel_map: {'Culture': []}\r\n> [2023-07-05 09:46:16,416] {retriever.py:164} DEBUG - rel_map: {'Biology': []}\r\n> [2023-07-05 09:46:16,416] {retriever.py:164} DEBUG - rel_map: {'History': []}\r\n> [2023-07-05 09:46:16,416] {retriever.py:164} DEBUG - rel_map: {'Ones': []}\r\n> [2023-07-05 09:46:16,416] {retriever.py:164} DEBUG - rel_map: {'Introduction': []}\r\n> [2023-07-05 09:46:16,416] {retriever.py:164} DEBUG - rel_map: {'Ashen': []}\r\n> [2023-07-05 09:46:16,417] {retriever.py:164} DEBUG - rel_map: {'Ashen Ones': []}\r\n> [2023-07-05 09:46:16,417] {retriever.py:164} DEBUG - rel_map: {'Appearance': []}\r\n> [2023-07-05 09:46:16,417] {retriever.py:164} DEBUG - rel_map: {'Society': []}\r\n> [2023-07-05 09:46:16,417] {util.py:60} DEBUG - message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\r\n> [2023-07-05 09:46:16,417] {util.py:60} DEBUG - api_version=None data='{\"input\": [\"Generate a detailed wikipedia-like article about The Ashen Ones. Include the sections: Introduction, Biology and Appearance, History, Culture and Society\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\r\n> [2023-07-05 09:46:16,629] {connectionpool.py:473} DEBUG - https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\r\n> [2023-07-05 09:46:16,630] {util.py:60} DEBUG - message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=31 request_id=14c5dc02a8e7e8fadfe9c342cb9ec0c3 response_code=200\r\n> [2023-07-05 09:46:16,766] {retriever.py:195} DEBUG - Found the following rel_texts+query similarites: [0.8403324398731267, 0.8348493994747843]\r\n> [2023-07-05 09:46:16,767] {retriever.py:198} DEBUG - Found the following top_k rel_texts: []\r\n> [2023-07-05 09:46:16,767] {retriever.py:251} INFO - > Querying with idx: 958ce658-f3a8-464c-9375-dc7e5235953a: <div class=\"api-loaded-article  page-article page-article-main template-speci...\r\n> [2023-07-05 09:46:16,767] {retriever.py:251} INFO - > Querying with idx: 4c1975aa-b901-450f-8536-0c4bcc748055: Ashen Ones\r\n> Rust and Ruin\r\n> The Ashen Ones were first discovered by United Natio...\r\n> [2023-07-05 09:46:16,768] {retriever.py:251} INFO - > Querying with idx: c016b0ce-ffe4-402b-a94a-ae0d3f641d62: Irradiated Republic-</b> Scavengers, machinists, and innovators, this power b...\r\n> [2023-07-05 09:46:16,768] {retriever.py:251} INFO - > Querying with idx: 4f8b3a55-4c73-4949-8ad4-e925373eb246: the Need&#039; skill.\r\n> <span class=\"line-spacer d-block\">&nbsp;</span>\r\n> Ashen O...\r\n> [2023-07-05 09:46:16,768] {retriever.py:280} INFO - > Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\r\n> ('Ashen Ones [MISC]', 'have a unique understanding of', 'technology [MISC]')\r\n> ('Ashen Ones [MISC]', 'were discovered by', 'Robert Tanzer [PERSON]')\r\n> [2023-07-05 09:46:16,795] {util.py:60} DEBUG - message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\r\n> [2023-07-05 09:46:16,795] {util.py:60} DEBUG - api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"Context information is below.\\\\n---------------------\\\\n<div class=\\\\\"api-loaded-article  page-article page-article-main template-species article-9fca329a-009e-4b2f-aa18-b3629da5a171 \\\\\">\\\\n                \\\\n            <div class=\\\\\"row\\\\\">\\\\n        <div class=\\\\\"col-md-8 article-content-left \\\\\">\\\\n            <div class=\\\\\"user-css-vignette\\\\\"><b>Rust and Ruin</b>\\\\nThe Ashen Ones were first discovered by United Nations of Worlds Explorer- Robert Tanzer. Starship crash survivors themselves, their ships pulled down to planet by a mysterious gravitational force. It was quickly determined that their planet, Akhlaqi, is home to a virulent form of plague. Called &#039;The Pale Death&#039; this disease causes a rapid wasting away of living tissue in those contracting it.\\\\n<span class=\\\\\"line-spacer d-block\\\\\">&nbsp;</span>\\\\nIn response to this plague, Ashen One doctors began acting quickly to remove tissue before it could spread, and replace the missing limbs, or body segments with mechanical replacements fashioned from whatever scrap that could be salvaged.\\\\n<span class=\\\\\"line-spacer d-block\\\\\">&nbsp;</span>\\\\nThe Ashen ones were evacuated from their wrecked starships-turned-cities, no trace of the disease coming with them. They now are settled on the lava world of Tsatsos, where their ingenuity with metal and salvage is powering the growing city-forge of Rigers Corps of Engineers.\\\\n<span class=\\\\\"line-spacer d-block\\\\\">&nbsp;</span>\\\\n<b>Characteristics</b>\\\\nAshen Ones are diminuative humanoids, the tallest standing at just 5 feet (1.5m) tall. Their features are sharp and highly angled, remining some of crows or other corvid like Terran creatures. Those who still have their own faces are\\\\n---------------------\\\\nGiven the context information and not prior knowledge, answer the question: Generate a detailed wikipedia-like article about The Ashen Ones. Include the sections: Introduction, Biology and Appearance, History, Culture and Society\\\\n\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": 3072, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\r\n\r\nAs you can see, the keyword lookup didn't return anything, because the nodes are named slightly different \"Ashen Ones [NORP]\" vs \"Ashen Ones\". The embeddings it searches on are based on the originally chunked documents rather than the actual knowledge graph. \r\n\r\nOriginally during construction I tried to set no embed_model on the service_context and instead just set include_embeddings=True on the index, but then during query time it complained that no embeddings were generated. \r\n\r\nAccording to the docs I should just do the latter but this didn't seem to work. \r\n\r\nhttps://gpt-index.readthedocs.io/en/latest/reference/indices/kg.html\n\n### Version\n\n0.6.36\n\n### Steps to Reproduce\n\nSee code snippets above\n\n### Relevant Logs/Tracbacks\n\n```shell\nSee log snippets above\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6733/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6733/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6732",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6732/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6732/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6732/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6732",
        "id": 1789257604,
        "node_id": "PR_kwDOIWuq585UsDgA",
        "number": 6732,
        "title": "feat(formatting): `black[jupyter]`",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-05T10:27:37Z",
        "updated_at": "2023-07-05T17:49:46Z",
        "closed_at": "2023-07-05T17:49:45Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6732",
            "html_url": "https://github.com/run-llama/llama_index/pull/6732",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6732.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6732.patch",
            "merged_at": "2023-07-05T17:49:45Z"
        },
        "body": "# Description\r\nCanonical formatting for jupyter notebooks\r\n\r\nFixes: https://github.com/jerryjliu/llama_index/issues/6731\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6732/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6732/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6731",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6731/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6731/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6731/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6731",
        "id": 1789212961,
        "node_id": "I_kwDOIWuq585qpTkh",
        "number": 6731,
        "title": "[Bug]: Format ipynb with `black[jupyter]`",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-05T10:01:32Z",
        "updated_at": "2023-07-05T17:49:46Z",
        "closed_at": "2023-07-05T17:49:46Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nVarious code tools can change the ipynb and format it.\r\n\r\nLet's have a canonical formating with `black[jupyter]`\n\n### Version\n\nall\n\n### Steps to Reproduce\n\nmake format\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6731/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6731/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6730",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6730/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6730/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6730/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6730",
        "id": 1789169533,
        "node_id": "PR_kwDOIWuq585UrwXB",
        "number": 6730,
        "title": "fix(test): `test_optimizer_chinese`",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-07-05T09:37:29Z",
        "updated_at": "2023-07-05T22:07:48Z",
        "closed_at": "2023-07-05T22:07:47Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6730",
            "html_url": "https://github.com/run-llama/llama_index/pull/6730",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6730.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6730.patch",
            "merged_at": "2023-07-05T22:07:47Z"
        },
        "body": "# Description\r\nfix broken test reported in https://github.com/jerryjliu/llama_index/issues/6728\r\n\r\nCC: @logan-markewich for original author of change\r\n\r\nI have no idea what this particular test is testing with \u2581hello\r\n\r\nAlso, should this test be running on CI?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6730/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6730/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6729",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6729/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6729/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6729/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6729",
        "id": 1789134318,
        "node_id": "PR_kwDOIWuq585UrouK",
        "number": 6729,
        "title": "fix(typo): `get_transformer_tokenizer_fn`",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-05T09:18:51Z",
        "updated_at": "2023-07-05T15:38:21Z",
        "closed_at": "2023-07-05T15:38:20Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6729",
            "html_url": "https://github.com/run-llama/llama_index/pull/6729",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6729.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6729.patch",
            "merged_at": "2023-07-05T15:38:20Z"
        },
        "body": "# Description\r\nsimple typo fix\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6729/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6729/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6728",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6728/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6728/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6728/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6728",
        "id": 1789116920,
        "node_id": "I_kwDOIWuq585qo8H4",
        "number": 6728,
        "title": "[Bug]: postprocess optimizer test failing on main",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-05T09:08:15Z",
        "updated_at": "2023-07-05T22:25:29Z",
        "closed_at": "2023-07-05T22:25:29Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\ntest fails\n\n### Version\n\n5d5013f2\n\n### Steps to Reproduce\n\npython -m pytest tests\n\n### Relevant Logs/Tracbacks\n\n```shell\ntests/indices/postprocessor/test_optimizer.py .F                                                                                                                                                                                               [100%]\r\n\r\n====================================================================================================================== FAILURES ======================================================================================================================\r\n_______________________________________________________________________________________________________________ test_optimizer_chinese _______________________________________________________________________________________________________________\r\n\r\n_mock_embeds = <MagicMock name='_get_text_embeddings' id='140481329494432'>, _mock_embed = <MagicMock name='_get_text_embedding' id='140481264645792'>\r\n\r\n    @patch.object(\r\n        OpenAIEmbedding, \"_get_text_embedding\", side_effect=mock_get_text_embeddings_chinese\r\n    )\r\n    @patch.object(\r\n        OpenAIEmbedding,\r\n        \"_get_text_embeddings\",\r\n        side_effect=mock_get_text_embeddings_chinese,\r\n    )\r\n    @pytest.mark.skipif(AutoTokenizer is None, reason=\"transformers not installed\")\r\n    def test_optimizer_chinese(_mock_embeds: Any, _mock_embed: Any) -> None:\r\n        \"\"\"Test optimizer.\"\"\"\r\n        optimizer = SentenceEmbeddingOptimizer(\r\n            tokenizer_fn=get_large_chinese_tokenizer_fn(), percentile_cutoff=0.5\r\n        )\r\n        query = QueryBundle(query_str=\"\u4f60\u597d \u4e16\u754c\", embedding=[1, 0, 0, 0, 0])\r\n        orig_node = TextNode(text=\"\u4f60\u597d \u4e16\u754c\")\r\n        optimized_node = optimizer.postprocess_nodes(\r\n            [NodeWithScore(node=orig_node)], query\r\n        )[0]\r\n>       assert len(optimized_node.node.get_content()) < len(orig_node.get_content())\r\nE       AssertionError: assert 3 < 3\r\nE        +  where 3 = len('\u4f60 \u754c')\r\nE        +    where '\u4f60 \u754c' = <bound method TextNode.get_content of TextNode(id_='61ecc67e-81bd-4fc6-b984-2b5b8da3235d', embedding=None, metadata={}...ar_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')>()\r\nE        +      where <bound method TextNode.get_content of TextNode(id_='61ecc67e-81bd-4fc6-b984-2b5b8da3235d', embedding=None, metadata={}...ar_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')> = TextNode(id_='61ecc67e-81bd-4fc6-b984-2b5b8da3235d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], exc...har_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n').get_content\r\nE        +        where TextNode(id_='61ecc67e-81bd-4fc6-b984-2b5b8da3235d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], exc...har_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n') = NodeWithScore(node=TextNode(id_='61ecc67e-81bd-4fc6-b984-2b5b8da3235d', embedding=None, metadata={}, excluded_embed_me... text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None).node\r\nE        +  and   3 = len('\u4f60 \u754c')\r\nE        +    where '\u4f60 \u754c' = <bound method TextNode.get_content of TextNode(id_='61ecc67e-81bd-4fc6-b984-2b5b8da3235d', embedding=None, metadata={}...ar_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')>()\r\nE        +      where <bound method TextNode.get_content of TextNode(id_='61ecc67e-81bd-4fc6-b984-2b5b8da3235d', embedding=None, metadata={}...ar_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')> = TextNode(id_='61ecc67e-81bd-4fc6-b984-2b5b8da3235d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], exc...har_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n').get_content\r\n\r\ntests/indices/postprocessor/test_optimizer.py:138: AssertionError\r\n============================================================================================================== short test summary info ===============================================================================================================\r\nFAILED tests/indices/postprocessor/test_optimizer.py::test_optimizer_chinese - AssertionError: assert 3 < 3\r\n============================================================================================================ 1 failed, 1 passed in 1.55s =============================================================================================================\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6728/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6728/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6727",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6727/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6727/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6727/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6727",
        "id": 1789101187,
        "node_id": "PR_kwDOIWuq585UrhpS",
        "number": 6727,
        "title": "[version] bump version to 0.7.1 ",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-05T08:58:48Z",
        "updated_at": "2023-07-05T14:49:24Z",
        "closed_at": "2023-07-05T14:49:23Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6727",
            "html_url": "https://github.com/run-llama/llama_index/pull/6727",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6727.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6727.patch",
            "merged_at": "2023-07-05T14:49:23Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6727/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6727/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6726",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6726/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6726/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6726/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6726",
        "id": 1789048604,
        "node_id": "PR_kwDOIWuq585UrWV8",
        "number": 6726,
        "title": "feat(service/storage context): Globals APIs",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-05T08:27:14Z",
        "updated_at": "2023-07-08T01:23:58Z",
        "closed_at": "2023-07-07T20:15:05Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6726",
            "html_url": "https://github.com/run-llama/llama_index/pull/6726",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6726.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6726.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nFixes https://github.com/jerryjliu/llama_index/issues/6722\r\n\r\nTLDR:\r\n```python\r\n# Initialize the global contexts\r\nllm = OpenAI(temperature=0, model=\"text-davinci-002\")\r\nServiceContext.from_defaults(llm=llm).set_global() \r\nStorageContext.from_defaults(..).set_global() \r\n\r\n# Application code - uses the global choices\r\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\r\nindex = KeywordTableIndex.from_documents(documents=documents)\r\nengine = ReActChatEngine.from_defaults(\r\n  query_engine_tools=[QueryEngineTool.from_defaults(index.as_query_engine())]\r\n)\r\n\r\n# now want to use a local model \r\nServiceContext.get_global().embedding = CustomEmbedding()\r\n```\r\n\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [X] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [X] Added new unit/integration tests\r\n- [X] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [X] I have performed a self-review of my own code\r\n- [X] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [X] My changes generate no new warnings\r\n- [X] I have added tests that prove my fix is effective or that my feature works\r\n- [X] New and existing unit tests pass locally with my changes\r\n\r\nNext steps:\r\n- [ ] Update all notebooks to use `set_global()` where appropriate\r\n- [ ] Remove unnecessary `mock_service_context` to majority of tests, replacing with either default (`set_to_global_default`) or global (`set_global`) mocked service contexts\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6726/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6726/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6725",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6725/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6725/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6725/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6725",
        "id": 1788982173,
        "node_id": "I_kwDOIWuq585qobOd",
        "number": 6725,
        "title": "[Feature Request]: retrieve all nodes who's similarity score is greater than some threshold",
        "user": {
            "login": "LytixDev",
            "id": 29276783,
            "node_id": "MDQ6VXNlcjI5Mjc2Nzgz",
            "avatar_url": "https://avatars.githubusercontent.com/u/29276783?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/LytixDev",
            "html_url": "https://github.com/LytixDev",
            "followers_url": "https://api.github.com/users/LytixDev/followers",
            "following_url": "https://api.github.com/users/LytixDev/following{/other_user}",
            "gists_url": "https://api.github.com/users/LytixDev/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/LytixDev/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/LytixDev/subscriptions",
            "organizations_url": "https://api.github.com/users/LytixDev/orgs",
            "repos_url": "https://api.github.com/users/LytixDev/repos",
            "events_url": "https://api.github.com/users/LytixDev/events{/privacy}",
            "received_events_url": "https://api.github.com/users/LytixDev/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-05T07:46:49Z",
        "updated_at": "2023-10-12T16:04:01Z",
        "closed_at": "2023-10-12T16:04:00Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nFunctionality to retrieve all nodes who's similarity score is greater than some threshold. This could work in tandem with the similarity_top_k argument used in the Retriever classes. \r\n\r\nThis feature would require some changes to the response synthesizer. The response synthesizer assumes there will always be context. This may not be the case if all nodes' scores are lower than the defined threshold. The solution I have come up with is just to have a fallback `text_qa_template` prompt that does not take in any context_str.\r\n\r\n### Reason\r\n\r\nYou can sort of do this. You can set similarity_top_k to a large value and then manually filter out all nodes who's score is lower than some threshold. This is kind of janky though.\r\n\r\n### Value of Feature\r\n\r\nSay you have a chat bot. You retrieve the top k most similar nodes in the database, but at last, non of the nodes happened to be relevant. This will trip up the chat bot as it receives a lot of context that it has to ignore. You can get around this by good \"prompt engineering\", but ultimately you are wasting tokens by sending the context to the LLM.\r\n\r\nExample:\r\nI send \"Hello\" to the chat bot.\r\nThere may not be any relevant context, but since the response synthesizer must take in context, the most similar (yet very dissimilar) context that is not relevant must be submitted to the LLM request. This makes it difficult for the response of the chat bot to simply be \"Hello, how can I help you?\" (or something like that), as the chat bot gets confused by the context provided.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6725/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6725/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6724",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6724/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6724/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6724/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6724",
        "id": 1788964300,
        "node_id": "I_kwDOIWuq585qoW3M",
        "number": 6724,
        "title": "[Bug]: response_gen doesn't generate anything from chat with as_chat_engine in the 0.7.0 version",
        "user": {
            "login": "dinhan92",
            "id": 86275789,
            "node_id": "MDQ6VXNlcjg2Mjc1Nzg5",
            "avatar_url": "https://avatars.githubusercontent.com/u/86275789?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dinhan92",
            "html_url": "https://github.com/dinhan92",
            "followers_url": "https://api.github.com/users/dinhan92/followers",
            "following_url": "https://api.github.com/users/dinhan92/following{/other_user}",
            "gists_url": "https://api.github.com/users/dinhan92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dinhan92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dinhan92/subscriptions",
            "organizations_url": "https://api.github.com/users/dinhan92/orgs",
            "repos_url": "https://api.github.com/users/dinhan92/repos",
            "events_url": "https://api.github.com/users/dinhan92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dinhan92/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-05T07:34:54Z",
        "updated_at": "2023-07-11T02:06:58Z",
        "closed_at": "2023-07-11T02:06:58Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nIn the 0.7.0 version, I try using as_chat_engine with streaming = true. But then when it comes to for text in response.response_gen:, it doesn't generate a text.\r\n\r\n\r\n### Version\r\n\r\n0.7.0\r\n\r\n### Steps to Reproduce\r\n\r\n``` python\r\nllm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-0301\")\r\nservice_context = ServiceContext.from_defaults(\r\n    llm = llm,\r\n    prompt_helper = prompt_helper\r\n)\r\n\r\n       chat_engine = index.as_chat_engine(\r\n            chat_mode='condense_question',\r\n            streaming=True,\r\n         )\r\n        response = chat_engine.chat(input_text)\r\n        @stream_with_context\r\n        def generate():\r\n            for text in response.response_gen:\r\n                yield text;\r\n```\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6724/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6724/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6723",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6723/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6723/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6723/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6723",
        "id": 1788941046,
        "node_id": "PR_kwDOIWuq585Uq_N-",
        "number": 6723,
        "title": "Support prefix messages (e.g. system prompt) in chat engine and OpenAI agent",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-05T07:22:47Z",
        "updated_at": "2023-07-06T01:15:06Z",
        "closed_at": "2023-07-06T01:15:05Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6723",
            "html_url": "https://github.com/run-llama/llama_index/pull/6723",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6723.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6723.patch",
            "merged_at": "2023-07-06T01:15:05Z"
        },
        "body": "# Description\r\nSupport prefix messages (e.g. system prompt) in chat engine and OpenAI agent\r\nAdd a bank of system prompts taken from Azure OpenAI Studio (with attribution) \r\n\r\n### Caveat\r\nNot supported in condense question mode yet. \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6723/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6723/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6722",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6722/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6722/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6722/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6722",
        "id": 1788765305,
        "node_id": "I_kwDOIWuq585qnmR5",
        "number": 6722,
        "title": "[Feature Request]: `ServiceContext` and `StorageContext` - Set `global` - object global and `global_default` - inheritence global ",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-07-05T04:59:22Z",
        "updated_at": "2023-07-08T08:56:35Z",
        "closed_at": "2023-07-08T08:56:34Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nFrom an API viewpoint, is cumbersome to have to pass in the service/storage context to all downstream components.\r\n\r\nThere is an existing API for this, but it is confusing as it does not actually provide a global, but rather sets a default to inherit from.\r\n\r\nInstead, we separate out these two APIs.\r\n\r\nBy default, the global service context will be initialized and set to `ServiceContext.from_defaults()`\r\n\r\nAPIs:\r\nset global - object global\r\n```python\r\n# The default service context for all downstream services except when explicitly passed a service context.\r\n# Changes made to this service context will affect all downstream services\r\nglobal_ctx = ServiceContext.from_defaults(...).set_global()\r\n\r\nServiceContext.from_defaults(...) # does not inherit from global\r\n\r\n# All downstream services that did not define their own service context will now use this llm_predictor\r\n# including all ongoing runs\r\nglobal_ctx.llm_predictor = CustomLLMPredictor() \r\n\r\n# Alternate pattern to modify global context: mutation\r\nglobal_ctx = ServiceContext.get_global()\r\nglobal_ctx.embed_model = CustomEmbedModel() \r\n```\r\n\r\nAdvanced use case: creating a lot of specialized contexts, but want a base default to save hassle.\r\n\r\nset default (API not recommended) - inheritence.\r\n```python\r\n# All calls to from_defaults will inherit from this service context, which is frozen.\r\nsvc_ctx = ServiceContext.from_defaults(...).set_to_global_default()\r\n\r\nServiceContext.from_defaults(...) # inherits from the global default \r\n\r\nsvc_ctx.llm_predictor = CustomLLMPredictor() \r\nServiceContext.from_defaults(...) # still uses previous llm_predictor\r\n```\r\n\r\nTasks:\r\n- [ ] Implement\r\n- [ ] Replace all examples/notebooks with this as best practice",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6722/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6722/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6721",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6721/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6721/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6721/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6721",
        "id": 1788761693,
        "node_id": "PR_kwDOIWuq585UqYQ_",
        "number": 6721,
        "title": "Add exception handling logic at NotionPageReader",
        "user": {
            "login": "livelikeabel",
            "id": 29794325,
            "node_id": "MDQ6VXNlcjI5Nzk0MzI1",
            "avatar_url": "https://avatars.githubusercontent.com/u/29794325?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/livelikeabel",
            "html_url": "https://github.com/livelikeabel",
            "followers_url": "https://api.github.com/users/livelikeabel/followers",
            "following_url": "https://api.github.com/users/livelikeabel/following{/other_user}",
            "gists_url": "https://api.github.com/users/livelikeabel/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/livelikeabel/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/livelikeabel/subscriptions",
            "organizations_url": "https://api.github.com/users/livelikeabel/orgs",
            "repos_url": "https://api.github.com/users/livelikeabel/repos",
            "events_url": "https://api.github.com/users/livelikeabel/events{/privacy}",
            "received_events_url": "https://api.github.com/users/livelikeabel/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-05T04:54:33Z",
        "updated_at": "2023-07-14T13:19:59Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6721",
            "html_url": "https://github.com/run-llama/llama_index/pull/6721",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6721.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6721.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\n**Add exception handling logic at NotionPageReader.** \r\n\r\n**When we use `load_data` method of NotionPageReader, it requests with HTTP inside of `readers.notion` module**\r\n```python\r\npage_ids = [\"6a6e34ef-f855-4b4b-81ed-bea310778be0\"]\r\ndocuments = NotionPageReader(integration_token=integration_token).load_data(page_ids=page_ids)\r\n```\r\n\r\n\r\n**But when request is fail, We can't get exact info from error like this. It's not that useful error message for user.**\r\n<img width=\"667\" alt=\"image\" src=\"https://github.com/jerryjliu/llama_index/assets/29794325/e43fe052-5aa1-46f1-afa4-1ba12e5af6d8\">\r\n\r\n\r\n**So, I added exception handling logic after request code. So, we can get more useful error from notion API server.**\r\n<img width=\"660\" alt=\"image\" src=\"https://github.com/jerryjliu/llama_index/assets/29794325/c2a21c0f-a152-455f-ae14-33df10cb43db\">\r\n\r\n\r\nFixes # (issue)\r\n- related issue https://github.com/jerryjliu/llama_index/issues/3777\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] My changes generate no new warnings\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6721/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6721/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6720",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6720/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6720/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6720/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6720",
        "id": 1788752960,
        "node_id": "I_kwDOIWuq585qnjRA",
        "number": 6720,
        "title": "[Feature Request]: Local-model defaults for non-predictor sensitive experimentation",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-07-05T04:46:55Z",
        "updated_at": "2023-07-17T10:53:51Z",
        "closed_at": "2023-07-14T04:16:19Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nIt can be costly to run experiments using paid APIs, especially, if code is accidentally re-run.\r\n\r\nHence, it should be as easy as `ServiceContext(local=true)` to download e.g. from huggingface, store in local cache, and utilize local model. Local model should be lightweight but effective (100M-500M params, e.g. BERT/sentence transformer).\r\n\r\nTo evaluate: is dev loop slower due to high latency from local model (e.g. on CPU-only devices)\r\n\r\n### Reason\r\nConfiguring a cheap local model each time is cumbersome.\r\n\r\n### Value of Feature\r\nSave cost, save hassle",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6720/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6720/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6719",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6719/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6719/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6719/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6719",
        "id": 1788627818,
        "node_id": "I_kwDOIWuq585qnEtq",
        "number": 6719,
        "title": "[Question]: Why do you remove ChatGPTLLMPredictor and make Stream not work with async?",
        "user": {
            "login": "dinhan92",
            "id": 86275789,
            "node_id": "MDQ6VXNlcjg2Mjc1Nzg5",
            "avatar_url": "https://avatars.githubusercontent.com/u/86275789?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dinhan92",
            "html_url": "https://github.com/dinhan92",
            "followers_url": "https://api.github.com/users/dinhan92/followers",
            "following_url": "https://api.github.com/users/dinhan92/following{/other_user}",
            "gists_url": "https://api.github.com/users/dinhan92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dinhan92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dinhan92/subscriptions",
            "organizations_url": "https://api.github.com/users/dinhan92/orgs",
            "repos_url": "https://api.github.com/users/dinhan92/repos",
            "events_url": "https://api.github.com/users/dinhan92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dinhan92/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-05T01:40:02Z",
        "updated_at": "2023-07-11T02:04:40Z",
        "closed_at": "2023-07-11T02:04:40Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nYesterday I just upgrade my llama index to 0.6.38 and see an error occur when running an api: It say stream not work with async when I run this code : response = await chat_engine.achat(input_text)\r\nSo I wait for the new in this version today: 0.7.0: And see that you remove ChatGPTLLMPredictor.\r\nGuys, I like this ChatGPTLLMPredictor, it has prepend messages feature. If using LLMPredictor, I don't know how to setup this prepend messages :((\r\nHop you guys help.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6719/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6719/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6718",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6718/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6718/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6718/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6718",
        "id": 1788569074,
        "node_id": "PR_kwDOIWuq585Upvf7",
        "number": 6718,
        "title": "AzureOpenAI",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-04T23:50:42Z",
        "updated_at": "2023-07-05T17:16:28Z",
        "closed_at": "2023-07-05T17:16:26Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6718",
            "html_url": "https://github.com/run-llama/llama_index/pull/6718",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6718.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6718.patch",
            "merged_at": "2023-07-05T17:16:26Z"
        },
        "body": "# Description\r\nVery simple support for AzureOpenAI\r\n\r\nUpdate notebooks, and also use `set_global_service_context` for Azure OpenAI examples.\r\n\r\n### Caveat\r\nUnlike the raw openai client, we ask user to specify both `model` and `engine` (i.e. deployment name). \r\nThe reason is that we use `model` to determine the right endpoint to call (completion vs. chat).\r\n\r\n\r\n### Todos\r\n\r\n- [x] waiting on AzureOpenAI account access to properly test this. \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6718/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6718/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6717",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6717/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6717/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6717/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6717",
        "id": 1788555676,
        "node_id": "PR_kwDOIWuq585Upsqi",
        "number": 6717,
        "title": "add streaming to chat engines",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-04T23:22:15Z",
        "updated_at": "2023-07-07T21:17:33Z",
        "closed_at": "2023-07-07T21:17:32Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6717",
            "html_url": "https://github.com/run-llama/llama_index/pull/6717",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6717.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6717.patch",
            "merged_at": "2023-07-07T21:17:32Z"
        },
        "body": "# Description\r\n\r\nThis is an initial stab at adding streaming to chat engines.\r\n\r\nTBH I'm not sure if I like the approach. Since the chat engines query various sources (agent, plain llm, querying an index), it got a little complicated.\r\n\r\n## Type of Change\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# TODO\r\n\r\n- [x] Figure out typing errors (very confused here)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6717/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6717/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6716",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6716/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6716/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6716/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6716",
        "id": 1788547247,
        "node_id": "PR_kwDOIWuq585Upq4Z",
        "number": 6716,
        "title": "Sub question event",
        "user": {
            "login": "sourabhdesai",
            "id": 3005241,
            "node_id": "MDQ6VXNlcjMwMDUyNDE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3005241?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sourabhdesai",
            "html_url": "https://github.com/sourabhdesai",
            "followers_url": "https://api.github.com/users/sourabhdesai/followers",
            "following_url": "https://api.github.com/users/sourabhdesai/following{/other_user}",
            "gists_url": "https://api.github.com/users/sourabhdesai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sourabhdesai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sourabhdesai/subscriptions",
            "organizations_url": "https://api.github.com/users/sourabhdesai/orgs",
            "repos_url": "https://api.github.com/users/sourabhdesai/repos",
            "events_url": "https://api.github.com/users/sourabhdesai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sourabhdesai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-04T23:03:45Z",
        "updated_at": "2023-07-05T23:48:19Z",
        "closed_at": "2023-07-05T23:42:33Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6716",
            "html_url": "https://github.com/run-llama/llama_index/pull/6716",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6716.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6716.patch",
            "merged_at": "2023-07-05T23:42:33Z"
        },
        "body": "# Description\r\n\r\nAdding a new callback event type for recording the sub-questions and sub-answers produced through the `SubQuestionQueryEngine`.\r\n\r\nAlso updating the behavior of `SubQuestionQueryEngine.from_defaults` when determining which `callback_manager` to use. It seems more intuitive that it determines the `callback_manager` for the query engine from the given `service_context` *(if available)* and otherwise tries to determine it from one of its tools.\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] Updated an existing notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6716/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6716/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6715",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6715/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6715/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6715/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6715",
        "id": 1788136857,
        "node_id": "PR_kwDOIWuq585UoRKV",
        "number": 6715,
        "title": "[version] bump version to 0.7.0",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-04T15:27:05Z",
        "updated_at": "2023-07-04T15:32:32Z",
        "closed_at": "2023-07-04T15:32:31Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6715",
            "html_url": "https://github.com/run-llama/llama_index/pull/6715",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6715.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6715.patch",
            "merged_at": "2023-07-04T15:32:31Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6715/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6715/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6714",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6714/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6714/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6714/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6714",
        "id": 1787963944,
        "node_id": "I_kwDOIWuq585qkioo",
        "number": 6714,
        "title": "[Feature Request]: delete and update node ",
        "user": {
            "login": "chuanyue98",
            "id": 92157473,
            "node_id": "U_kgDOBX42IQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/92157473?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/chuanyue98",
            "html_url": "https://github.com/chuanyue98",
            "followers_url": "https://api.github.com/users/chuanyue98/followers",
            "following_url": "https://api.github.com/users/chuanyue98/following{/other_user}",
            "gists_url": "https://api.github.com/users/chuanyue98/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/chuanyue98/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/chuanyue98/subscriptions",
            "organizations_url": "https://api.github.com/users/chuanyue98/orgs",
            "repos_url": "https://api.github.com/users/chuanyue98/repos",
            "events_url": "https://api.github.com/users/chuanyue98/events{/privacy}",
            "received_events_url": "https://api.github.com/users/chuanyue98/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-04T13:44:15Z",
        "updated_at": "2023-11-28T16:03:38Z",
        "closed_at": "2023-11-28T16:03:37Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nModify the content or delete nodes by node_id\n\n### Reason\n\nAfter using the document to create an index, the content of the node is a little dissatisfied, I hope to modify the content, sometimes you also need to delete the node, but I don't know how the nodes are related, I use index.delete_ref_doc() to delete, the node still exists\n\n### Value of Feature\n\nIt can reduce some errors when using document direct import, or some data needs to be updated",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6714/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6714/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6713",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6713/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6713/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6713/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6713",
        "id": 1787673097,
        "node_id": "I_kwDOIWuq585qjboJ",
        "number": 6713,
        "title": "[Experimental/Discussion]: Prompt auto-tuning in eval outer loop",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-04T10:41:13Z",
        "updated_at": "2023-10-17T16:04:14Z",
        "closed_at": "2023-10-17T16:04:13Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nAs part of hyperparameter optimization, rewrite prompts using an LLM predictor against a validation set. \r\n\r\nIt is well known that small tweaks to prompts can lead to better adherence to desired behaviors.\r\n\r\n**Grid Search:** \r\nThe simple form of this is generate and grid search (no feedback).\r\n\r\n**Evolutionary search:** \r\nAlternately, by feeding back the negative, neutral and positive examples of prompt rewriting (neutral - no statistically significant change), we can redirect the prompt semantically, and force the LLM to infer features in the prompts that are leading to more positive results, thus, it is hoped, produce better prompts. In this scenario, we stop the tuning process once the prompts generated show no more improvement over the baseline prompt (we have hit a local optimum). \r\n\r\nNevertheless, one should be cautious not to overfit on the validation set.\r\n\r\nA good pattern might be to optimize over a single prompt or a small handful of interrelated prompts at a time.\r\n\r\nEdit:\r\nSome prior work here https://github.com/mshumer/gpt-prompt-engineer\r\n\r\n### Reason\r\n\r\n_No response_\r\n\r\n### Value of Feature\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6713/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6713/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6712",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6712/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6712/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6712/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6712",
        "id": 1787401669,
        "node_id": "PR_kwDOIWuq585UlxBw",
        "number": 6712,
        "title": "DeepLake New changes with exapmles",
        "user": {
            "login": "adolkhan",
            "id": 54854336,
            "node_id": "MDQ6VXNlcjU0ODU0MzM2",
            "avatar_url": "https://avatars.githubusercontent.com/u/54854336?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/adolkhan",
            "html_url": "https://github.com/adolkhan",
            "followers_url": "https://api.github.com/users/adolkhan/followers",
            "following_url": "https://api.github.com/users/adolkhan/following{/other_user}",
            "gists_url": "https://api.github.com/users/adolkhan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/adolkhan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/adolkhan/subscriptions",
            "organizations_url": "https://api.github.com/users/adolkhan/orgs",
            "repos_url": "https://api.github.com/users/adolkhan/repos",
            "events_url": "https://api.github.com/users/adolkhan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/adolkhan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-04T08:10:37Z",
        "updated_at": "2023-07-14T02:40:05Z",
        "closed_at": "2023-07-14T02:40:05Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6712",
            "html_url": "https://github.com/run-llama/llama_index/pull/6712",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6712.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6712.patch",
            "merged_at": "2023-07-14T02:40:05Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- Done code refactoring to include the Latest DeepLake VectorStore\r\nTo be more Specific:\r\n1. Changed DeepLakeVectorStore init method, now we initialize Our VectorStore inside of DeepLakeVectorStore object\r\n2. Changed DeepLakeVectorStore add method, now we add elements directly to our VectorStore inside of DeepLakeVectorStore object\r\n3. Changed DeepLakeVectorStore delete method, now we delete items from our VectorStore inside of DeepLakeVectorStore object\r\n4. Changed DeepLakeVectorStore query method, now we query DeepLakeVectorStore object under the hood runs VectorStore query call.\r\n5. Added use case where DeepLakeVectorStore is used with complex queries\r\n6. Added new integration tests\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- Added new notebook (that tests end-to-end)\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6712/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6712/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6711",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6711/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6711/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6711/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6711",
        "id": 1787196409,
        "node_id": "I_kwDOIWuq585qhnP5",
        "number": 6711,
        "title": "[Bug]:  markdown reader ",
        "user": {
            "login": "zenwan",
            "id": 13159809,
            "node_id": "MDQ6VXNlcjEzMTU5ODA5",
            "avatar_url": "https://avatars.githubusercontent.com/u/13159809?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zenwan",
            "html_url": "https://github.com/zenwan",
            "followers_url": "https://api.github.com/users/zenwan/followers",
            "following_url": "https://api.github.com/users/zenwan/following{/other_user}",
            "gists_url": "https://api.github.com/users/zenwan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zenwan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zenwan/subscriptions",
            "organizations_url": "https://api.github.com/users/zenwan/orgs",
            "repos_url": "https://api.github.com/users/zenwan/repos",
            "events_url": "https://api.github.com/users/zenwan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zenwan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-04T05:51:14Z",
        "updated_at": "2023-10-19T16:04:41Z",
        "closed_at": "2023-10-19T16:04:40Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nIf the beginning of the document does not match the header, some text information will be lost.\r\n\r\n`\r\n    def markdown_to_tups(self, markdown_text: str) -> List[Tuple[Optional[str], str]]:\r\n        \"\"\"Convert a markdown file to a dictionary.\r\n\r\n        The keys are the headers and the values are the text under each header.\r\n\r\n        \"\"\"\r\n        markdown_tups: List[Tuple[Optional[str], str]] = []\r\n        lines = markdown_text.split(\"\\n\")\r\n\r\n        current_header = None\r\n        current_text = \"\"\r\n\r\n        for line in lines:\r\n            header_match = re.match(r\"^#+\\s\", line)\r\n            if header_match:\r\n                if current_header is not None:\r\n                    if current_text == \"\" or None:\r\n                        continue\r\n                    markdown_tups.append((current_header, current_text))\r\n\r\n                current_header = line\r\n                current_text = \"\"\r\n            else:\r\n                current_text += line + \"\\n\"\r\n        markdown_tups.append((current_header, current_text))\r\n`\r\n\r\n### Version\r\n\r\n0.6.28\r\n\r\n### Steps to Reproduce\r\n\r\ndocs = reader.load_data(file=\"./data/VGC.md\")\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6711/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6711/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6710",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6710/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6710/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6710/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6710",
        "id": 1786981568,
        "node_id": "PR_kwDOIWuq585UkYcZ",
        "number": 6710,
        "title": "Remove deprecated arguments from `PromptHelper` ",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-04T01:04:45Z",
        "updated_at": "2023-07-04T01:22:57Z",
        "closed_at": "2023-07-04T01:22:56Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6710",
            "html_url": "https://github.com/run-llama/llama_index/pull/6710",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6710.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6710.patch",
            "merged_at": "2023-07-04T01:22:56Z"
        },
        "body": "# Description\r\nRemove deprecated arguments from `PromptHelper` ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6710/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6710/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6709",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6709/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6709/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6709/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6709",
        "id": 1786960835,
        "node_id": "I_kwDOIWuq585qgtvD",
        "number": 6709,
        "title": "[Bug]: from llama_index import GPTSimpleVectorIndex",
        "user": {
            "login": "silvacarl2",
            "id": 4220915,
            "node_id": "MDQ6VXNlcjQyMjA5MTU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4220915?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/silvacarl2",
            "html_url": "https://github.com/silvacarl2",
            "followers_url": "https://api.github.com/users/silvacarl2/followers",
            "following_url": "https://api.github.com/users/silvacarl2/following{/other_user}",
            "gists_url": "https://api.github.com/users/silvacarl2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/silvacarl2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/silvacarl2/subscriptions",
            "organizations_url": "https://api.github.com/users/silvacarl2/orgs",
            "repos_url": "https://api.github.com/users/silvacarl2/repos",
            "events_url": "https://api.github.com/users/silvacarl2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/silvacarl2/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-04T00:29:39Z",
        "updated_at": "2023-07-05T15:05:58Z",
        "closed_at": "2023-07-04T18:41:45Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\npip install llama-index\r\n\r\npython\r\nPython 3.8.10 (default, May 26 2023, 14:05:08)\r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> \r\n>>> from llama_index import GPTSimpleVectorIndex\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: cannot import name 'GPTSimpleVectorIndex' from 'llama_index' (/home/silvacarl/.local/lib/python3.8/site-packages/llama_index/__init__.py)\r\n>>>\r\n\r\nit doesnt matter what i do or install, i cannot get around this error.  any ideas?\r\n\r\n\r\n### Version\r\n\r\nlatest/current\r\n\r\n### Steps to Reproduce\r\n\r\nsee above.\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n```shell\r\nsee above.\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6709/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6709/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6708",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6708/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6708/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6708/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6708",
        "id": 1786938959,
        "node_id": "I_kwDOIWuq585qgoZP",
        "number": 6708,
        "title": "[Feature Request]: Parallelize sync APIs with multi-threading",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-03T23:54:00Z",
        "updated_at": "2023-10-12T16:04:12Z",
        "closed_at": "2023-10-12T16:04:11Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nFor I/O bound functions, threading makes sense for GIL, as each thread is blocked on IO.\r\n\r\nOptions: (preference?)\r\n1. [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor)\r\n\r\nQuestions:\r\n1. Should this be a global executor available to the whole process?\r\n\r\n### Reason\r\n\r\nExample use case: https://github.com/jerryjliu/llama_index/issues/6698\r\n\r\n### Value of Feature\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6708/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6708/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6707",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6707/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6707/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6707/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6707",
        "id": 1786523234,
        "node_id": "I_kwDOIWuq585qfC5i",
        "number": 6707,
        "title": "[Bug]: Service context not working correctly on Mac",
        "user": {
            "login": "yousifKashef",
            "id": 39941169,
            "node_id": "MDQ6VXNlcjM5OTQxMTY5",
            "avatar_url": "https://avatars.githubusercontent.com/u/39941169?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yousifKashef",
            "html_url": "https://github.com/yousifKashef",
            "followers_url": "https://api.github.com/users/yousifKashef/followers",
            "following_url": "https://api.github.com/users/yousifKashef/following{/other_user}",
            "gists_url": "https://api.github.com/users/yousifKashef/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yousifKashef/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yousifKashef/subscriptions",
            "organizations_url": "https://api.github.com/users/yousifKashef/orgs",
            "repos_url": "https://api.github.com/users/yousifKashef/repos",
            "events_url": "https://api.github.com/users/yousifKashef/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yousifKashef/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-07-03T16:51:01Z",
        "updated_at": "2023-09-06T19:54:36Z",
        "closed_at": "2023-07-13T22:09:57Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI am trying to run the pdf local example notebook example on my Mac but I am getting this error: ---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[2], line 13\r\n     10 prompt_helper = PromptHelper(max_input_size=512, num_output=256, max_chunk_overlap=-1000)\r\n     11 embed_model = HuggingFaceEmbeddings()\r\n---> 13 service_context = ServiceContext.from_defaults(\r\n     14     llm_predictor=llm_predictor,\r\n     15     embed_model=embed_model,\r\n     16     prompt_helper=prompt_helper,\r\n     17     node_parser=SimpleNodeParser(text_splitter=TokenTextSplitter(chunk_size=300, chunk_overlap=20))\r\n     18 )\r\n     20 # Document setup\r\n     21 PyMuPDFReader = download_loader(\"PyMuPDFReader\")\r\n\r\nFile ~/LLM_venv/lib/python3.10/site-packages/llama_index/indices/service_context.py:138, in ServiceContext.from_defaults(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\r\n    136 # NOTE: the embed_model isn't used in all indices\r\n    137 embed_model = embed_model or OpenAIEmbedding()\r\n--> 138 embed_model.callback_manager = callback_manager\r\n    140 prompt_helper = prompt_helper or _get_default_prompt_helper(\r\n    141     llm_metadata=llm_predictor.get_llm_metadata(),\r\n    142     context_window=context_window,\r\n    143     num_output=num_output,\r\n    144 )\r\n    146 node_parser = node_parser or _get_default_node_parser(\r\n    147     chunk_size=chunk_size,\r\n    148     chunk_overlap=chunk_overlap,\r\n    149     callback_manager=callback_manager,\r\n    150 )\r\n\r\nFile ~/LLM_venv/lib/python3.10/site-packages/pydantic/main.py:357, in pydantic.main.BaseModel.__setattr__()\r\n\r\nValueError: \"HuggingFaceEmbeddings\" object has no field \"callback_manager\"\n\n### Version\n\n0.6.38.post1\n\n### Steps to Reproduce\n\nrun a service context: service_context = ServiceContext.from_defaults(\r\n    llm_predictor=llm_predictor,\r\n    embed_model=embed_model,\r\n    prompt_helper=prompt_helper,\r\n    node_parser=SimpleNodeParser(text_splitter=TokenTextSplitter(chunk_size=300, chunk_overlap=20))\r\n)\r\n\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6707/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6707/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6706",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6706/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6706/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6706/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6706",
        "id": 1786490012,
        "node_id": "PR_kwDOIWuq585UiuWI",
        "number": 6706,
        "title": "Support async in OpenAI LLM",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-03T16:24:47Z",
        "updated_at": "2023-07-03T18:59:42Z",
        "closed_at": "2023-07-03T18:59:36Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6706",
            "html_url": "https://github.com/run-llama/llama_index/pull/6706",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6706.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6706.patch",
            "merged_at": "2023-07-03T18:59:36Z"
        },
        "body": "# Description\r\n\r\nSupport async in OpenAI LLM\r\n\r\n## Todos\r\n- [x] Fix typing\r\n- [x] Add unit tests",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6706/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6706/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6705",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6705/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6705/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6705/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6705",
        "id": 1786315405,
        "node_id": "I_kwDOIWuq585qeQKN",
        "number": 6705,
        "title": "[Bug]: Unable to get proper response using azureopnenai llm and gptvectorstore index to read pdf document",
        "user": {
            "login": "sumanthp",
            "id": 11244510,
            "node_id": "MDQ6VXNlcjExMjQ0NTEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/11244510?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sumanthp",
            "html_url": "https://github.com/sumanthp",
            "followers_url": "https://api.github.com/users/sumanthp/followers",
            "following_url": "https://api.github.com/users/sumanthp/following{/other_user}",
            "gists_url": "https://api.github.com/users/sumanthp/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sumanthp/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sumanthp/subscriptions",
            "organizations_url": "https://api.github.com/users/sumanthp/orgs",
            "repos_url": "https://api.github.com/users/sumanthp/repos",
            "events_url": "https://api.github.com/users/sumanthp/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sumanthp/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-03T14:40:42Z",
        "updated_at": "2023-07-04T05:23:22Z",
        "closed_at": "2023-07-04T05:23:21Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nUnable to get proper response using azureopnenai llm and gptvectorstore index to read pdf documents. Some cases this gives context infomation as below:\r\nNo one wants to write programs in a language that might go away, as so many programming languages do. So most hackers will tend to wait until a language has been around for a couple years before even considering it. And once they've used a language for a project, they're reluctant to use anything else.\r\n\r\nAnswer: Most hackers will tend to wait until a language has been around for a couple years before even considering it. And once they've used a language for a project, they're reluctant to use anything else.\r\n\r\n---------------------\r\nGiven the context information and not prior knowledge, answer the question: What is the importance of early adopters in a popular language?\r\nEarly adopters are sophisticated and demanding, and quickly flush out whatever flaws remain in your technology. When you only have a few users you can be in close contact with all of them. And early adopters are forgiving when you improve your system, even if this causes some breakage.\r\n\r\nAnswer: Early adopters are sophisticated and demanding, and quickly flush out whatever flaws remain in your technology. When you only have a few users you can be in close contact with all of them. And early adopters are forgiving when you improve your system, even if this causes some breakage.\r\n\r\n---------------------\r\nGiven the context information and not prior knowledge, answer the question: What is the importance of having a language that is easy to learn in a popular language?\r\nHaving a language that is easy to learn is important because it makes it easier for people to learn and use the language. This means that more people will be able to use the language, which will increase its popularity.\r\n\r\nAnswer: Having a language that is easy to learn is important because it makes it easier for people to learn and use the language. This means that more people will be able to use the language, which will increase its popularity.\r\n\r\n---------------------\r\nGiven the context information and not prior knowledge, answer the question: What is the importance of having a language with good support for threads in a popular language?\r\nHaving a language with good support for threads is important because it allows for concurrent programming, which is becoming increasingly important as computers become more powerful. This means that more people will be able to use the language for a wider range of applications, which will increase its popularity.\r\n\r\nAnswer: Having a language with good support for threads is important because it allows for concurrent programming, which is becoming increasingly important as computers become more powerful. This means that more people will be able to use the language for a wider range of applications,\r\n\r\nCode:\r\n`llm = AzureOpenAI(\r\n    deployment_name=\"GPT35\", model=\"gpt-3.5-turbo\",\r\n    temperature=0.0,\r\n    max_tokens=512,\r\n    openai_api_base=openai.api_base,\r\n    openai_api_key=openai.api_key,\r\n    openai_api_version=openai.api_version\r\n)\r\nllm_predictor = LLMPredictor(llm=llm)\r\nembedding_llm = LangchainEmbedding(OpenAIEmbeddings(openai_api_base=openai.api_base, openai_api_key=openai.api_key, model=\"text-embedding-ada-002\", deployment=\"embeddingada002\", chunk_size=1))\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embedding_llm)\r\ndocuments = SimpleDirectoryReader('data').load_data()\r\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\r\nquery_engine = index.as_query_engine(response_mode='refine', service_context=service_context)\r\nresponse = query_engine.query(\"What is the mechanics of popularity?\")\r\nprint(str(response))`.\n\n### Version\n\n0.6.38.post1\n\n### Steps to Reproduce\n\nRun the above code \n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6705/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6705/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6704",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6704/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6704/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6704/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6704",
        "id": 1786205787,
        "node_id": "I_kwDOIWuq585qd1Zb",
        "number": 6704,
        "title": "[Question]: Error using Prompts",
        "user": {
            "login": "iooab10",
            "id": 54444854,
            "node_id": "MDQ6VXNlcjU0NDQ0ODU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/54444854?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/iooab10",
            "html_url": "https://github.com/iooab10",
            "followers_url": "https://api.github.com/users/iooab10/followers",
            "following_url": "https://api.github.com/users/iooab10/following{/other_user}",
            "gists_url": "https://api.github.com/users/iooab10/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/iooab10/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/iooab10/subscriptions",
            "organizations_url": "https://api.github.com/users/iooab10/orgs",
            "repos_url": "https://api.github.com/users/iooab10/repos",
            "events_url": "https://api.github.com/users/iooab10/events{/privacy}",
            "received_events_url": "https://api.github.com/users/iooab10/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-03T13:37:55Z",
        "updated_at": "2023-07-03T13:42:08Z",
        "closed_at": "2023-07-03T13:41:57Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6704/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6704/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6703",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6703/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6703/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6703/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6703",
        "id": 1786134808,
        "node_id": "I_kwDOIWuq585qdkEY",
        "number": 6703,
        "title": "[Discussion]: Multi-threaded/multi-process scale-out",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-07-03T12:58:20Z",
        "updated_at": "2023-12-07T14:52:55Z",
        "closed_at": "2023-10-12T16:04:16Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nSo far, we've discussed about async-compatibility. Even under the async scenario, we need to be careful about designing the tracing infra as tracing is known to put a high load on systems.\r\n\r\nWe also need to consider beyond single-threaded async concurrency. In the serving case, one is likely to have to use multiprocess due to python GIL. In this case, there are many in-memory structures that may benefit from being shared. For instance indices. \r\n\r\nIn the serving case, it is likely that the index ought to be some kind of external service hosted on an external process. So this may not be a problem. \r\n\r\nSo the open question is: in the high-load multi process scenario, what are the challenges that need to be anticipated, so that we can keep it in mind while designing some of the systems? \r\n\r\nTo my mind, the best scenario is if llama-index serving is a simple logic layer with minimal statefulness that orchestrates other services (see e.g.  https://github.com/jerryjliu/llama_index/issues/6692). One should horizontally scale each process as though it were an isolated shard, without any cross-process communication. \r\n\r\n### Reason\r\n\r\n_No response_\r\n\r\n### Value of Feature\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6703/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6703/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6702",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6702/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6702/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6702/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6702",
        "id": 1786128530,
        "node_id": "I_kwDOIWuq585qdiiS",
        "number": 6702,
        "title": "[Bug]: Usage of CustomLLM is broken",
        "user": {
            "login": "mirth",
            "id": 1296970,
            "node_id": "MDQ6VXNlcjEyOTY5NzA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1296970?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mirth",
            "html_url": "https://github.com/mirth",
            "followers_url": "https://api.github.com/users/mirth/followers",
            "following_url": "https://api.github.com/users/mirth/following{/other_user}",
            "gists_url": "https://api.github.com/users/mirth/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mirth/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mirth/subscriptions",
            "organizations_url": "https://api.github.com/users/mirth/orgs",
            "repos_url": "https://api.github.com/users/mirth/repos",
            "events_url": "https://api.github.com/users/mirth/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mirth/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-07-03T12:54:55Z",
        "updated_at": "2023-07-05T05:32:52Z",
        "closed_at": "2023-07-03T14:13:59Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nGetting an error when trying to create `ServiceContext` with `CustomLLM`.\r\n\r\n\n\n### Version\n\n0.6.38.post1\n\n### Steps to Reproduce\n\n```\r\nfrom llama_index import ServiceContext\r\nfrom llama_index.llms.custom import CustomLLM\r\nfrom llama_index.llms.base import CompletionResponse, LLMMetadata, CompletionResponseGen\r\nfrom typing import Any\r\n\r\n\r\ncontext_window = 2048\r\nnum_output = 256\r\n\r\nclass OurLLM(CustomLLM):\r\n    @property\r\n    def metadata(self) -> LLMMetadata:\r\n        return LLMMetadata(\r\n            context_window=context_window, num_output=num_output\r\n        )\r\n\r\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\r\n\r\n        return CompletionResponse(text='sample response')\r\n    \r\n    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\r\n        raise NotImplementedError()\r\n    \r\n\r\nllm = OurLLM()\r\n\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=llm, \r\n    context_window=context_window, \r\n    num_output=num_output\r\n)\r\n```\n\n### Relevant Logs/Tracbacks\n\n```shell\nFile \"/repro.py\", line 30, in <module>\r\n    service_context = ServiceContext.from_defaults(\r\n  File \"/Users/user/Library/Python/3.9/lib/python/site-packages/llama_index/indices/service_context.py\", line 141, in from_defaults\r\n    llm_metadata=llm_predictor.get_llm_metadata(),\r\n  File \"/Users/user/Library/Python/3.9/lib/python/site-packages/llama_index/llm_predictor/base.py\", line 189, in get_llm_metadata\r\n    return _get_llm_metadata(self._llm)\r\n  File \"/Users/user/Library/Python/3.9/lib/python/site-packages/llama_index/llm_predictor/base.py\", line 49, in _get_llm_metadata\r\n    raise ValueError(\"llm must be an instance of langchain.llms.base.LLM\")\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6702/reactions",
            "total_count": 2,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 2
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6702/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6701",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6701/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6701/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6701/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6701",
        "id": 1785961721,
        "node_id": "PR_kwDOIWuq585Ug78Q",
        "number": 6701,
        "title": "fixbug:nebugraph NoneType Result",
        "user": {
            "login": "BleakStone",
            "id": 13256512,
            "node_id": "MDQ6VXNlcjEzMjU2NTEy",
            "avatar_url": "https://avatars.githubusercontent.com/u/13256512?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BleakStone",
            "html_url": "https://github.com/BleakStone",
            "followers_url": "https://api.github.com/users/BleakStone/followers",
            "following_url": "https://api.github.com/users/BleakStone/following{/other_user}",
            "gists_url": "https://api.github.com/users/BleakStone/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BleakStone/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BleakStone/subscriptions",
            "organizations_url": "https://api.github.com/users/BleakStone/orgs",
            "repos_url": "https://api.github.com/users/BleakStone/repos",
            "events_url": "https://api.github.com/users/BleakStone/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BleakStone/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-03T11:15:08Z",
        "updated_at": "2023-07-03T17:23:34Z",
        "closed_at": "2023-07-03T17:23:34Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6701",
            "html_url": "https://github.com/run-llama/llama_index/pull/6701",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6701.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6701.patch",
            "merged_at": "2023-07-03T17:23:34Z"
        },
        "body": "# Description\r\n\r\nfix a samll bug which cause NoneType Error in nebulagraph.py \r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6701/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6701/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6700",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6700/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6700/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6700/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6700",
        "id": 1785763618,
        "node_id": "I_kwDOIWuq585qcJci",
        "number": 6700,
        "title": "Difference between vector store and index. ",
        "user": {
            "login": "pradeepdev-1995",
            "id": 41164884,
            "node_id": "MDQ6VXNlcjQxMTY0ODg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/41164884?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pradeepdev-1995",
            "html_url": "https://github.com/pradeepdev-1995",
            "followers_url": "https://api.github.com/users/pradeepdev-1995/followers",
            "following_url": "https://api.github.com/users/pradeepdev-1995/following{/other_user}",
            "gists_url": "https://api.github.com/users/pradeepdev-1995/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pradeepdev-1995/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pradeepdev-1995/subscriptions",
            "organizations_url": "https://api.github.com/users/pradeepdev-1995/orgs",
            "repos_url": "https://api.github.com/users/pradeepdev-1995/repos",
            "events_url": "https://api.github.com/users/pradeepdev-1995/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pradeepdev-1995/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-07-03T09:34:16Z",
        "updated_at": "2023-10-09T16:03:11Z",
        "closed_at": "2023-10-09T16:03:10Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nWhat is the difference between **vector store** and index in **llamaindex**. \r\nWhat is the basic conecpt behind this? Also, share the situations to use each over other.\r\n\r\nvectore stores found at\r\n```\r\nllama_index.vector_stores\r\n```\r\nIndices found at\r\n```\r\nfrom llama_index.indices.vector_store\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6700/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6700/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6699",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6699/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6699/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6699/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6699",
        "id": 1785704289,
        "node_id": "I_kwDOIWuq585qb69h",
        "number": 6699,
        "title": "Sentencebert/Bert/Spacy/Doc2vec/Word2vec/Glove/FastText support",
        "user": {
            "login": "pradeepdev-1995",
            "id": 41164884,
            "node_id": "MDQ6VXNlcjQxMTY0ODg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/41164884?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pradeepdev-1995",
            "html_url": "https://github.com/pradeepdev-1995",
            "followers_url": "https://api.github.com/users/pradeepdev-1995/followers",
            "following_url": "https://api.github.com/users/pradeepdev-1995/following{/other_user}",
            "gists_url": "https://api.github.com/users/pradeepdev-1995/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pradeepdev-1995/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pradeepdev-1995/subscriptions",
            "organizations_url": "https://api.github.com/users/pradeepdev-1995/orgs",
            "repos_url": "https://api.github.com/users/pradeepdev-1995/repos",
            "events_url": "https://api.github.com/users/pradeepdev-1995/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pradeepdev-1995/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-03T09:05:38Z",
        "updated_at": "2023-10-12T16:04:22Z",
        "closed_at": "2023-10-12T16:04:21Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nIs there any way to store the **Sentencebert/Bert/Spacy/Doc2vec/Word2vec/Glove/FastText** based embeddings in the vector database using llamaindex\r\n\r\nNow I can see only 3 methods of embeddings in the source code\r\n```\r\nfrom llama_index.embeddings.google import GoogleUnivSentEncoderEmbedding\r\nfrom llama_index.embeddings.langchain import LangchainEmbedding\r\nfrom llama_index.embeddings.openai import OpenAIEmbedding\r\n\r\n__all__ = [\r\n    \"GoogleUnivSentEncoderEmbedding\",\r\n    \"LangchainEmbedding\",\r\n    \"OpenAIEmbedding\",\r\n]\r\n```\n\n### Reason\n\nFor using native embedding formats like **Sentencebert/Bert/Spacy/Doc2vec/Word2vec/Glove/FastText**  in llamaindex\n\n### Value of Feature\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6699/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6699/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6698",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6698/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6698/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6698/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6698",
        "id": 1785544095,
        "node_id": "I_kwDOIWuq585qbT2f",
        "number": 6698,
        "title": "[Feature Request]: Query the LLM in parallel ",
        "user": {
            "login": "JViggiani",
            "id": 41336980,
            "node_id": "MDQ6VXNlcjQxMzM2OTgw",
            "avatar_url": "https://avatars.githubusercontent.com/u/41336980?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/JViggiani",
            "html_url": "https://github.com/JViggiani",
            "followers_url": "https://api.github.com/users/JViggiani/followers",
            "following_url": "https://api.github.com/users/JViggiani/following{/other_user}",
            "gists_url": "https://api.github.com/users/JViggiani/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/JViggiani/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/JViggiani/subscriptions",
            "organizations_url": "https://api.github.com/users/JViggiani/orgs",
            "repos_url": "https://api.github.com/users/JViggiani/repos",
            "events_url": "https://api.github.com/users/JViggiani/events{/privacy}",
            "received_events_url": "https://api.github.com/users/JViggiani/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-07-03T07:43:39Z",
        "updated_at": "2023-10-09T16:03:20Z",
        "closed_at": "2023-10-09T16:03:20Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nFrom what I can tell, when a index is constructed, it processes the chunks in serial. That is to say that, for example with a knowledge graph, it extracts the chunks, and then asks openai to produce triplets of a single chunk, waits for the response, and then queries the next chunk. \r\n\r\nInstead, chunks should be processed in parallel, limited by the user defining what their plan dictates their rate limit is. \r\n\r\nIn theory I should have access to 3500 requests per minute, but as it stands I'm only using about 10 when building the knowledge graph. \r\n\r\nExample call to be parallelized:\r\n\r\n>         GPTKnowledgeGraphIndex.from_documents( documents,\r\n>         max_triplets_per_chunk=50, service_context=service_context)\r\n\r\n### Reason\r\n\r\n_No response_\r\n\r\n### Value of Feature\r\n\r\nSignificantly increase the speed of index construction ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6698/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6698/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6697",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6697/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6697/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6697/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6697",
        "id": 1785439489,
        "node_id": "PR_kwDOIWuq585UfKB-",
        "number": 6697,
        "title": "Minor docs update for streaming",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-03T06:35:42Z",
        "updated_at": "2023-07-03T19:11:11Z",
        "closed_at": "2023-07-03T19:11:11Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6697",
            "html_url": "https://github.com/run-llama/llama_index/pull/6697",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6697.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6697.patch",
            "merged_at": "2023-07-03T19:11:11Z"
        },
        "body": "# Description\r\n\r\nUpdate notebook and docs for streaming, since we no longer require user to set streaming flag on the LLM itself.\r\nStreaming vs not is controlled by calling the `complete` vs. `stream_complete` endpoint.  \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6697/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6697/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6696",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6696/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6696/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6696/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6696",
        "id": 1785415034,
        "node_id": "PR_kwDOIWuq585UfEvW",
        "number": 6696,
        "title": "Update all notebooks to use native openai integration",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-03T06:15:51Z",
        "updated_at": "2023-07-04T06:46:04Z",
        "closed_at": "2023-07-04T06:46:03Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6696",
            "html_url": "https://github.com/run-llama/llama_index/pull/6696",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6696.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6696.patch",
            "merged_at": "2023-07-04T06:46:03Z"
        },
        "body": "# Description\r\n\r\nUpdate all notebooks to use native openai integration",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6696/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6696/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6695",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6695/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6695/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6695/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6695",
        "id": 1785315800,
        "node_id": "PR_kwDOIWuq585UevBd",
        "number": 6695,
        "title": "Update API reference for LLMs",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-03T04:52:44Z",
        "updated_at": "2023-07-03T05:36:06Z",
        "closed_at": "2023-07-03T05:36:05Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6695",
            "html_url": "https://github.com/run-llama/llama_index/pull/6695",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6695.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6695.patch",
            "merged_at": "2023-07-03T05:36:05Z"
        },
        "body": "# Description\r\n\r\nUpdate docs and API references for LLMs\r\n\r\nAdd `autodoc_pydantic` for prettier documentation for pydantic objects",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6695/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6695/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6694",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6694/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6694/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6694/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6694",
        "id": 1785284718,
        "node_id": "PR_kwDOIWuq585UeoWh",
        "number": 6694,
        "title": "Add Streaming To OpenAI Agents",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-07-03T04:21:36Z",
        "updated_at": "2023-07-04T19:03:26Z",
        "closed_at": "2023-07-04T19:03:25Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6694",
            "html_url": "https://github.com/run-llama/llama_index/pull/6694",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6694.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6694.patch",
            "merged_at": "2023-07-04T19:03:25Z"
        },
        "body": "# Description\r\n\r\nThis PR is an initial stab at adding `stream_chat` and `astream_chat` to our agents.\r\n\r\nSince agents can involve multiple steps, the idea here is to turn the streaming endpoints into generators of generators (i.e. a generator for each response).\r\n\r\nThis way, users can easily stream every step, or only the last step (as shown in the notebook)\r\n\r\n## Type of Change\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# TODO\r\n\r\n- [ ] confirm the best home for the `StreamingChatResponse` class\r\n- [ ] confirm this works for various backend usecases\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6694/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6694/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6693",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6693/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6693/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6693/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/6693",
        "id": 1785252246,
        "node_id": "PR_kwDOIWuq585UehXh",
        "number": 6693,
        "title": "Make prompt selector support non-langchain LLMs",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-03T03:47:24Z",
        "updated_at": "2023-07-03T04:27:22Z",
        "closed_at": "2023-07-03T04:27:21Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/6693",
            "html_url": "https://github.com/run-llama/llama_index/pull/6693",
            "diff_url": "https://github.com/run-llama/llama_index/pull/6693.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/6693.patch",
            "merged_at": "2023-07-03T04:27:21Z"
        },
        "body": "# Description\r\n\r\nUpdate prompt selector mechanism to support non-langchain LLMs. \r\n\r\nAlso very minor change to refine response builder to add default values in ctor.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6693/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6693/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6692",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6692/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6692/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6692/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6692",
        "id": 1785124387,
        "node_id": "I_kwDOIWuq585qZtYj",
        "number": 6692,
        "title": "[Feature Request]: Subprocess Local Model Service",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-07-03T01:42:47Z",
        "updated_at": "2023-10-15T16:04:09Z",
        "closed_at": "2023-10-15T16:04:08Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nhttps://github.com/jerryjliu/llama_index/pull/6615#discussion_r1249345549\r\n\r\nFor async handling of local models across multiple queries\n\n### Reason\n\n_No response_\n\n### Value of Feature\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6692/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6692/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6691",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6691/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6691/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6691/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6691",
        "id": 1785122504,
        "node_id": "I_kwDOIWuq585qZs7I",
        "number": 6691,
        "title": "[Feature Request]: Default Prometheus Stats",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-07-03T01:40:23Z",
        "updated_at": "2023-07-03T01:48:56Z",
        "closed_at": "2023-07-03T01:48:55Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nWe should expose default prometheus stats.\r\n\r\nIt should be as easy as:\r\n```python\r\nfrom llama_index import PrometheusTracer, start_grafana_dashboard;\r\n\r\nstart_grafana_dashboard(prometheus_port=3003,grafana_port=3002) # e.g. subprocess.popen(...)\r\nservice_context.add_callback_handler(PrometheusTracer(\"localhost:3003\"))\r\n\r\n\r\nTables:\r\n- tbd\r\n\r\n\n\n### Reason\n\n_No response_\n\n### Value of Feature\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6691/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6691/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6690",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6690/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6690/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6690/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6690",
        "id": 1785122165,
        "node_id": "I_kwDOIWuq585qZs11",
        "number": 6690,
        "title": "[Feature Request]: Default Prometheus Stats",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-03T01:40:10Z",
        "updated_at": "2023-10-12T16:04:31Z",
        "closed_at": "2023-10-12T16:04:30Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nWe should expose default prometheus stats.\r\n\r\nIt should be as easy as:\r\n```python\r\nfrom llama_index import PrometheusTracer, start_grafana_dashboard;\r\n\r\nstart_grafana_dashboard(prometheus_port=3003,grafana_port=3002) # e.g. subprocess.popen(...)\r\nservice_context.add_callback_handler(PrometheusTracer(\"localhost:3003\"))\r\n```\r\n\r\nTables:\r\n- tbd\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6690/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6690/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6689",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6689/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6689/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6689/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6689",
        "id": 1785115176,
        "node_id": "I_kwDOIWuq585qZrIo",
        "number": 6689,
        "title": "[Tracking]: Tracing/Callbacks",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-07-03T01:32:09Z",
        "updated_at": "2023-09-26T18:49:30Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nRefactor for async-compatible granular tracing\r\n- [x] https://github.com/jerryjliu/llama_index/issues/6602\r\n  - [ ] Possible subtasks (outdated): https://github.com/jerryjliu/llama_index/issues/6602#issuecomment-1608754763\r\n- [ ] https://github.com/jerryjliu/llama_index/issues/6688\r\n\r\nAdditional Callbacks\r\n- [ ] MLflow: https://github.com/jerryjliu/llama_index/issues/6617\r\n- [ ] Arize: https://github.com/jerryjliu/llama_index/issues/6918\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6689/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6689/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/6688",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/6688/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/6688/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/6688/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/6688",
        "id": 1785112826,
        "node_id": "I_kwDOIWuq585qZqj6",
        "number": 6688,
        "title": "[RFC]: Tracing Ontology (WIP)",
        "user": {
            "login": "jon-chuang",
            "id": 9093549,
            "node_id": "MDQ6VXNlcjkwOTM1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9093549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jon-chuang",
            "html_url": "https://github.com/jon-chuang",
            "followers_url": "https://api.github.com/users/jon-chuang/followers",
            "following_url": "https://api.github.com/users/jon-chuang/following{/other_user}",
            "gists_url": "https://api.github.com/users/jon-chuang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jon-chuang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jon-chuang/subscriptions",
            "organizations_url": "https://api.github.com/users/jon-chuang/orgs",
            "repos_url": "https://api.github.com/users/jon-chuang/repos",
            "events_url": "https://api.github.com/users/jon-chuang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jon-chuang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 11,
        "created_at": "2023-07-03T01:30:40Z",
        "updated_at": "2023-11-05T02:11:04Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nLlamaIndex can be thought of as an orchestrator and prompt management system across various subtasks.\r\n\r\nHere is a proposed ontology of a retrieval pipeline:\r\n1. `session`: a session is a multi-round interaction consisting of multiple runs\r\n2. `run`: a run is a single round trip from client back to client (e.g. REST endpoint, jupyter notebook cell run). It can consist of multiple tasks.\r\n   - <details>the reasons for this become clearer when we consider that we want run-level _triggers_ such as traceback/debug string upon completion. For runs, we will allow completion triggers to depend on subtasks. However, we will _not_ be providing triggers at the task level whose conditions depend on subtasks, as this is likely _too computationally prohibitive_.</details>\r\n   - <details>by getting rid of `StreamingResponse`, we may be able to side-step the above problem and provide a per-task completion handler. However, I am still not sure if per-task completion triggers are desirable.</details>\r\n3. `task` (currently called `Event`): A task is a basic unit of work. It can consist of multiple subtasks, also considered tasks. A task can occur at any level of granularity. A task granularity is defined by its `task_type` (currently `EventType`) and can instantiate its own callback handler. Examples\r\n   - `embed`\r\n   - `llm_predict`\r\n   - `retrieve`\r\n\r\nExample trace:\r\n```python\r\nsession_id=0\r\n  run_id=0\r\n    task=query_transformation\r\n      task=llm_predict\r\n        task_duration=420ms\r\n        tokens_used=1283\r\n    task=embed\r\n      task_duration=150ms\r\n    task=retrieve\r\n      task_duration=54ms\r\n    task=llm_predict\r\n run_id=1\r\n   ...\r\n```\r\n\r\nFlattened, this is:\r\n```python\r\n{trace: 'session_id=0,run_id=0,task=embed', labels='embedding_model=openai[text-embedding-ada-002]', measurements:'task_duration=150ms'}\r\n{trace: 'session_id=0,run_id=0,task=retrieve', labels='index=vector_index[weaviate[localhost:3003]]', measurements:'task_duration=330ms'}\r\n{trace: 'session_id=0,run_id=0,task=llm_predict', labels='llm_model=openai[text-davinci-003]', measurements:'task_duration=450ms'}\r\n```\r\n\r\nAdditional concepts:\r\n1. `label`: a label is an identifier for a task. Labels are stored in event payloads with a fallback to defaults in `session/run/task`-level `CallbackHandler`s. Examples: \r\n   - `llm_model`: `openai[text-davinci-003]`, `custom[ggml-int4-q4]`\r\n   - `embedding_model`: `openai[text-embedding-ada-002]`\r\n2. \r\n\r\n### Example Consumers\r\n1. Prometheus\r\n2. MLFlow\r\n3. WandB\r\n\r\n### Example Aggregations\r\nHere we use SQL. One should use their imagination for how these may be expressed in other query languages.\r\n\r\n```sql\r\n# Find the average task duration for each task\r\nselect task, avg(task_duration) as task_ms from llm_metrics where session_id=0 group by task;\r\ntask  |  task_ms\r\n----------------------------\r\nembed    | 155\r\nretrieve  |  32\r\nllm_predict | 425\r\n\r\n# Find the total cost for each session\r\n# One can define a specialized callback handler to precompute these\r\n# But this way (dump in storage then analyze) works too\r\nselect \r\n  session_id,\r\n  case\r\n    when task='embed' and labels['embedding_model'] = 'openai[text-embedding-ada-002]' \r\n    then float(measurements['token_count']) * 0.00002\r\n    when task='llm_predict' and labels['llm_model'] = 'openai[text-davinci-003]' \r\n    then float(measurements['token_count']) * 0.0002\r\n  as dollar_cost\r\nfrom \r\n  llm_metrics\r\ngroup by\r\n  session_id;\r\n```\r\n\r\n\r\n### References\r\n\r\n### Questions\r\n1. Instant **(trigger)** v.s. Eventual **(collection)**: \r\n   - Implementation of callbacks seems to assume instantaneous actions. However, to my understanding, many use cases like tracing and logging only need eventual actions.\r\n2. Equivalence / Conversion of tracing <-> logging?\r\n   - For instance, generically convert spans (event.start <-> event.end) into duration (e.g. prometheus/MLFlow metrics)",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/6688/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/6688/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    }
]