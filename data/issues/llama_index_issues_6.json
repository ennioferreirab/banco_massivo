[
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9011",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9011/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9011/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9011/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9011",
        "id": 2001120625,
        "node_id": "I_kwDOIWuq5853Rq1x",
        "number": 9011,
        "title": "[Bug]: AttributeError: 'Message' object has no attribute 'pop'",
        "user": {
            "login": "khushpatel2002",
            "id": 66012546,
            "node_id": "MDQ6VXNlcjY2MDEyNTQ2",
            "avatar_url": "https://avatars.githubusercontent.com/u/66012546?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/khushpatel2002",
            "html_url": "https://github.com/khushpatel2002",
            "followers_url": "https://api.github.com/users/khushpatel2002/followers",
            "following_url": "https://api.github.com/users/khushpatel2002/following{/other_user}",
            "gists_url": "https://api.github.com/users/khushpatel2002/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/khushpatel2002/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/khushpatel2002/subscriptions",
            "organizations_url": "https://api.github.com/users/khushpatel2002/orgs",
            "repos_url": "https://api.github.com/users/khushpatel2002/repos",
            "events_url": "https://api.github.com/users/khushpatel2002/events{/privacy}",
            "received_events_url": "https://api.github.com/users/khushpatel2002/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-19T23:56:46Z",
        "updated_at": "2023-11-20T00:48:25Z",
        "closed_at": "2023-11-20T00:48:24Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nimplemented a simple RAG, got this error\n\n### Version\n\n0.9.3\n\n### Steps to Reproduce\n\nfrom llama_index.llms import LiteLLM\r\n\r\nfrom llama_index.indices.query.base import BaseQueryEngine\r\n\r\n\r\nllm = LiteLLM(model=\"gpt-3.5-turbo\", **kwargs)\r\n\r\n\r\n service_context = LyzrService.from_defaults(\r\n        llm=llm,\r\n        embed_model=embed_model,\r\n        system_prompt=system_prompt,\r\n        query_wrapper_prompt=query_wrapper_prompt,\r\n        **service_context_params,\r\n    )\r\n\r\n\r\n\r\nvector_store_index = VectorStoreIndex.from_documents(\r\n                    documents=documents,\r\n                    storage_context=storage_context,\r\n                    service_context=service_context,\r\n                    show_progress=True,\r\n                )\r\n\r\nvector_store_index.as_query_engine(**query_engine_params, similarity_top_k=5)\r\n\r\n\r\n_query = \"what does googler do?\"\r\n\r\nrag = rag.query(_query)\r\n\r\npprint(rag.response)\r\n\n\n### Relevant Logs/Tracbacks\n\n```shell\nUserWarning: Pydantic serializer warnings:\r\n  Expected `Usage` but got `dict` - serialized value may not be as expected\r\n  return self.__pydantic_serializer__.to_python(\r\nTraceback (most recent call last):\r\n  File \"/home/.venv/lib/python3.10/site-packages/pydantic/main.py\", line 753, in __getattr__\r\n    return pydantic_extra[item]\r\nKeyError: 'pop'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/cookbook/question_answering/qa_from_webpage.py\", line 21, in <module>\r\n    rag = rag.query(_query)\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/core/base_query_engine.py\", line 30, in query\r\n    return self._query(str_or_query_bundle)\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py\", line 180, in _query\r\n    response = self._response_synthesizer.synthesize(\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/response_synthesizers/base.py\", line 146, in synthesize\r\n    response_str = self.get_response(\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/response_synthesizers/compact_and_refine.py\", line 38, in get_response\r\n    return super().get_response(\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py\", line 127, in get_response\r\n    response = self._give_response_single(\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py\", line 182, in _give_response_single\r\n    program(\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py\", line 53, in __call__\r\n    answer = self._llm_predictor.predict(\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/llm_predictor/base.py\", line 219, in predict\r\n    chat_response = self._llm.chat(messages)\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/llms/base.py\", line 187, in wrapped_llm_chat\r\n    f_return_val = f(_self, messages, **kwargs)\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/llms/litellm.py\", line 138, in chat\r\n    return chat_fn(messages, **kwargs)\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/llms/litellm.py\", line 210, in _chat\r\n    message = from_openai_message_dict(message_dict)\r\n  File \"/home/.venv/lib/python3.10/site-packages/llama_index/llms/litellm_utils.py\", line 167, in from_openai_message_dict\r\n    additional_kwargs.pop(\"role\")\r\n  File \"/home/.venv/lib/python3.10/site-packages/pydantic/main.py\", line 755, in __getattr__\r\n    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}') from exc\r\nAttributeError: 'Message' object has no attribute 'pop'\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9011/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9011/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9010",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9010/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9010/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9010/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9010",
        "id": 2001069563,
        "node_id": "PR_kwDOIWuq585f2bGB",
        "number": 9010,
        "title": "Docs & code example: Fixed a typo in IngestionPipeline (e was missing)",
        "user": {
            "login": "matthias",
            "id": 16365,
            "node_id": "MDQ6VXNlcjE2MzY1",
            "avatar_url": "https://avatars.githubusercontent.com/u/16365?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/matthias",
            "html_url": "https://github.com/matthias",
            "followers_url": "https://api.github.com/users/matthias/followers",
            "following_url": "https://api.github.com/users/matthias/following{/other_user}",
            "gists_url": "https://api.github.com/users/matthias/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/matthias/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/matthias/subscriptions",
            "organizations_url": "https://api.github.com/users/matthias/orgs",
            "repos_url": "https://api.github.com/users/matthias/repos",
            "events_url": "https://api.github.com/users/matthias/events{/privacy}",
            "received_events_url": "https://api.github.com/users/matthias/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-19T22:28:28Z",
        "updated_at": "2023-11-20T00:44:16Z",
        "closed_at": "2023-11-20T00:44:16Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9010",
            "html_url": "https://github.com/run-llama/llama_index/pull/9010",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9010.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9010.patch",
            "merged_at": "2023-11-20T00:44:16Z"
        },
        "body": "# Description\r\n\r\nThere where several occasions where \"pipeline\" was missing an \"e\" (was pipline)\r\nNotably at `/docs/module_guides/indexing/metadata_extraction.md` where it caused the example to fail.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9010/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9010/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9009",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9009/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9009/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9009/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9009",
        "id": 2001032778,
        "node_id": "I_kwDOIWuq5853RVZK",
        "number": 9009,
        "title": "[Feature Request]: Implicit service_context creation",
        "user": {
            "login": "hemanth",
            "id": 18315,
            "node_id": "MDQ6VXNlcjE4MzE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/18315?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hemanth",
            "html_url": "https://github.com/hemanth",
            "followers_url": "https://api.github.com/users/hemanth/followers",
            "following_url": "https://api.github.com/users/hemanth/following{/other_user}",
            "gists_url": "https://api.github.com/users/hemanth/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hemanth/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hemanth/subscriptions",
            "organizations_url": "https://api.github.com/users/hemanth/orgs",
            "repos_url": "https://api.github.com/users/hemanth/repos",
            "events_url": "https://api.github.com/users/hemanth/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hemanth/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-11-19T20:56:25Z",
        "updated_at": "2023-11-20T18:54:49Z",
        "closed_at": "2023-11-20T04:53:19Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nIt would be beneficial if we could implicitly create a `service_context` using the `llm` and `embedding` model for non-OpenAI based API in `VectorStoreIndex`, or any other place where `service_context` is necessary.\r\n\r\n\n\n### Reason\n\nOverhead of creating `service_context`.\r\n\r\nFor example: \r\n\r\nInstead of:\r\n\r\n```python\r\nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-base-en-v1.5\",chunk_size=800, chunk_overlap=20)\r\n\r\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\r\n```\r\n\r\nthe below will be useful? \r\n```python\r\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store,llm=llm, embed_model=\"local:BAAI/bge-base-en-v1.5\",chunk_size=800, chunk_overlap=20)\r\n```\r\n\r\nI totally understand \n\n### Value of Feature\n\nI totally understand that `service_context` container is a utility container and is an integral part of the design, thinking out lout on how we can make the API bit more simpler. ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9009/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9009/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9008",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9008/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9008/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9008/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9008",
        "id": 2001024470,
        "node_id": "PR_kwDOIWuq585f2R-W",
        "number": 9008,
        "title": "Fix: missing docs dependencies",
        "user": {
            "login": "seldo",
            "id": 185893,
            "node_id": "MDQ6VXNlcjE4NTg5Mw==",
            "avatar_url": "https://avatars.githubusercontent.com/u/185893?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/seldo",
            "html_url": "https://github.com/seldo",
            "followers_url": "https://api.github.com/users/seldo/followers",
            "following_url": "https://api.github.com/users/seldo/following{/other_user}",
            "gists_url": "https://api.github.com/users/seldo/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/seldo/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/seldo/subscriptions",
            "organizations_url": "https://api.github.com/users/seldo/orgs",
            "repos_url": "https://api.github.com/users/seldo/repos",
            "events_url": "https://api.github.com/users/seldo/events{/privacy}",
            "received_events_url": "https://api.github.com/users/seldo/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-19T20:33:20Z",
        "updated_at": "2023-11-20T00:21:36Z",
        "closed_at": "2023-11-20T00:21:35Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9008",
            "html_url": "https://github.com/run-llama/llama_index/pull/9008",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9008.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9008.patch",
            "merged_at": "2023-11-20T00:21:35Z"
        },
        "body": "# Description\r\n\r\nDocs was missing two necessary dependencies.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9008/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9008/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9007",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9007/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9007/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9007/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9007",
        "id": 2000984753,
        "node_id": "PR_kwDOIWuq585f2KPV",
        "number": 9007,
        "title": "[Bug]: ChromaReader incorrect indexing of result in create_documents",
        "user": {
            "login": "dylan1218",
            "id": 28288488,
            "node_id": "MDQ6VXNlcjI4Mjg4NDg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/28288488?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dylan1218",
            "html_url": "https://github.com/dylan1218",
            "followers_url": "https://api.github.com/users/dylan1218/followers",
            "following_url": "https://api.github.com/users/dylan1218/following{/other_user}",
            "gists_url": "https://api.github.com/users/dylan1218/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dylan1218/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dylan1218/subscriptions",
            "organizations_url": "https://api.github.com/users/dylan1218/orgs",
            "repos_url": "https://api.github.com/users/dylan1218/repos",
            "events_url": "https://api.github.com/users/dylan1218/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dylan1218/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-19T18:40:47Z",
        "updated_at": "2023-11-22T18:05:46Z",
        "closed_at": "2023-11-19T19:09:16Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9007",
            "html_url": "https://github.com/run-llama/llama_index/pull/9007",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9007.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9007.patch",
            "merged_at": "2023-11-19T19:09:16Z"
        },
        "body": "fix incorrect indexing of result object in chroma.py\r\n\r\n# Description\r\n\r\nThe result of a chroma.query method are key values pairs where the values are nested arrays. In order to appropriately zip/loop through the values you need to first access the list of values themselves.\r\n\r\nchroma.query id sample results:\r\n![image](https://github.com/run-llama/llama_index/assets/28288488/e5caf348-2af5-45e5-8469-7264597b88e8)\r\n\r\nCurrentlly create_documents zips through the unested list, resulting in only one document being returned from the method call.\r\n\r\n\r\nFixes # (issue)\r\n#9006 \r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] My changes generate no new warnings\r\n- [x] created a notebook and manually tested expected results\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9007/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9007/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9006",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9006/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9006/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9006/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9006",
        "id": 2000984501,
        "node_id": "I_kwDOIWuq5853RJm1",
        "number": 9006,
        "title": "[Bug]: ChromaReader incorrect indexing of result in create_documents",
        "user": {
            "login": "dylan1218",
            "id": 28288488,
            "node_id": "MDQ6VXNlcjI4Mjg4NDg4",
            "avatar_url": "https://avatars.githubusercontent.com/u/28288488?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dylan1218",
            "html_url": "https://github.com/dylan1218",
            "followers_url": "https://api.github.com/users/dylan1218/followers",
            "following_url": "https://api.github.com/users/dylan1218/following{/other_user}",
            "gists_url": "https://api.github.com/users/dylan1218/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dylan1218/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dylan1218/subscriptions",
            "organizations_url": "https://api.github.com/users/dylan1218/orgs",
            "repos_url": "https://api.github.com/users/dylan1218/repos",
            "events_url": "https://api.github.com/users/dylan1218/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dylan1218/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-19T18:40:06Z",
        "updated_at": "2023-11-20T00:48:52Z",
        "closed_at": "2023-11-20T00:48:52Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nThe result of a chroma.query method are key values pairs where the values are nested arrays. In order to appropriately zip/loop through the values you need to first access the list of values themselves.\r\n\r\nchroma.query id sample results:\r\n![image](https://github.com/run-llama/llama_index/assets/28288488/e5caf348-2af5-45e5-8469-7264597b88e8)\r\n\r\nCurrentlly create_documents zips through the unested list, resulting in only one document being returned from the method call.\r\n\n\n### Version\n\nv0.9.3\n\n### Steps to Reproduce\n\nCall .load_data method on an initialized ChromaReader object where your chroma db contains more than one result.\r\n\r\nWhat happens:\r\n- only one document is returned\r\n\r\nWhat is expected:\r\n- a document for each record result returned from the chroma.query method \n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9006/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9006/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9005",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9005/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9005/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9005/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9005",
        "id": 2000974479,
        "node_id": "PR_kwDOIWuq585f2IFA",
        "number": 9005,
        "title": "add transforms experimentation notebook ",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-19T18:22:31Z",
        "updated_at": "2023-11-19T19:27:31Z",
        "closed_at": "2023-11-19T19:27:30Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9005",
            "html_url": "https://github.com/run-llama/llama_index/pull/9005",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9005.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9005.patch",
            "merged_at": "2023-11-19T19:27:30Z"
        },
        "body": "notebook showing how you can use the ingestion pipeline to more easily experiment with different data parameters e.g. chunk size, metadata \r\n\r\n(not clear to me where this will live in the docs yet)",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9005/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9005/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9004",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9004/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9004/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9004/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9004",
        "id": 2000942173,
        "node_id": "I_kwDOIWuq5853Q_Rd",
        "number": 9004,
        "title": "[Bug]: vector.control: No such file or directory",
        "user": {
            "login": "hemanth",
            "id": 18315,
            "node_id": "MDQ6VXNlcjE4MzE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/18315?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hemanth",
            "html_url": "https://github.com/hemanth",
            "followers_url": "https://api.github.com/users/hemanth/followers",
            "following_url": "https://api.github.com/users/hemanth/following{/other_user}",
            "gists_url": "https://api.github.com/users/hemanth/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hemanth/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hemanth/subscriptions",
            "organizations_url": "https://api.github.com/users/hemanth/orgs",
            "repos_url": "https://api.github.com/users/hemanth/repos",
            "events_url": "https://api.github.com/users/hemanth/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hemanth/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 10,
        "created_at": "2023-11-19T16:59:50Z",
        "updated_at": "2023-11-20T03:28:14Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nUnable to connect to remote pd DB via connection string\r\n\r\n### Version\r\n\r\nv0.9.3\r\n\r\n### Steps to Reproduce\r\n\r\n```python\r\n# !pip install llama_index asyncpg pgvector -q\r\n\r\nfrom llama_index.indices.vector_store import VectorStoreIndex\r\nfrom llama_index.vector_stores import PGVectorStore\r\nfrom llama_index import ServiceContext\r\n\r\nvector_store = PGVectorStore.from_params(connection_string=\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\", table_name=\"foobar\")\r\n\r\nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-base-en-v1.5\",chunk_size=800, chunk_overlap=20)\r\n\r\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\r\n\r\nquery_engine = index.as_query_engine()\r\n\r\nprint(query_engine.query(\"Describe the table\"))\r\n```\r\n\r\nresults in :\r\n\r\n```\r\nUndefinedFile: could not open extension control file \"/usr/share/postgresql/11/extension/vector.control\": No such file or directory\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nOperationalError                          Traceback (most recent call last)\r\n[/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py](https://localhost:8080/#) in do_execute(self, cursor, statement, parameters, context)\r\n    920 \r\n    921     def do_execute(self, cursor, statement, parameters, context=None):\r\n--> 922         cursor.execute(statement, parameters)\r\n    923 \r\n    924     def do_execute_no_params(self, cursor, statement, context=None):\r\n\r\nOperationalError: (psycopg2.errors.UndefinedFile) could not open extension control file \"/usr/share/postgresql/11/extension/vector.control\": No such file or directory\r\n\r\n[SQL: CREATE EXTENSION IF NOT EXISTS vector]\r\n\r\n```\r\n\r\n^ The server is on elephantsql. \r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9004/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9004/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9003",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9003/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9003/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9003/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9003",
        "id": 2000925706,
        "node_id": "PR_kwDOIWuq585f1-Xu",
        "number": 9003,
        "title": "fix bugs for litellm==1.1.1",
        "user": {
            "login": "fcakyon",
            "id": 34196005,
            "node_id": "MDQ6VXNlcjM0MTk2MDA1",
            "avatar_url": "https://avatars.githubusercontent.com/u/34196005?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fcakyon",
            "html_url": "https://github.com/fcakyon",
            "followers_url": "https://api.github.com/users/fcakyon/followers",
            "following_url": "https://api.github.com/users/fcakyon/following{/other_user}",
            "gists_url": "https://api.github.com/users/fcakyon/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fcakyon/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fcakyon/subscriptions",
            "organizations_url": "https://api.github.com/users/fcakyon/orgs",
            "repos_url": "https://api.github.com/users/fcakyon/repos",
            "events_url": "https://api.github.com/users/fcakyon/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fcakyon/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-19T16:24:10Z",
        "updated_at": "2023-11-20T16:10:25Z",
        "closed_at": "2023-11-19T18:47:27Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9003",
            "html_url": "https://github.com/run-llama/llama_index/pull/9003",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9003.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9003.patch",
            "merged_at": "2023-11-19T18:47:27Z"
        },
        "body": "# Description\r\n\r\nCurrently, LiteLLM class in llama-index must be fixed due to breaking changes in litellm.\r\n\r\nThis PR fixes it with minimal changes.\r\n\r\nFixes https://github.com/run-llama/llama_index/issues/8906\r\nFixes https://github.com/BerriAI/litellm/issues/816\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nCurrent tests pass.\r\n\r\ncc: @krrishdholakia @ishaan-jaff @logan-markewich \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9003/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9003/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9002",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9002/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9002/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9002/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9002",
        "id": 2000881582,
        "node_id": "PR_kwDOIWuq585f11Q9",
        "number": 9002,
        "title": "[Docs] grammar",
        "user": {
            "login": "roryfahy",
            "id": 42627486,
            "node_id": "MDQ6VXNlcjQyNjI3NDg2",
            "avatar_url": "https://avatars.githubusercontent.com/u/42627486?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/roryfahy",
            "html_url": "https://github.com/roryfahy",
            "followers_url": "https://api.github.com/users/roryfahy/followers",
            "following_url": "https://api.github.com/users/roryfahy/following{/other_user}",
            "gists_url": "https://api.github.com/users/roryfahy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/roryfahy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/roryfahy/subscriptions",
            "organizations_url": "https://api.github.com/users/roryfahy/orgs",
            "repos_url": "https://api.github.com/users/roryfahy/repos",
            "events_url": "https://api.github.com/users/roryfahy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/roryfahy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-19T14:49:27Z",
        "updated_at": "2023-11-19T19:17:04Z",
        "closed_at": "2023-11-19T19:17:03Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9002",
            "html_url": "https://github.com/run-llama/llama_index/pull/9002",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9002.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9002.patch",
            "merged_at": "2023-11-19T19:17:03Z"
        },
        "body": "# Description\r\nJust a simple grammar change on the main documentation page\r\n`which may private or specific to the problem you\u2019re trying to solve.` ->\r\n`which may be private or specific to the problem you\u2019re trying to solve.`\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9002/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9002/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9001",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9001/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9001/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9001/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9001",
        "id": 2000867074,
        "node_id": "I_kwDOIWuq5853Qs8C",
        "number": 9001,
        "title": "[Question]: document already tokenized",
        "user": {
            "login": "patrickocal",
            "id": 45278848,
            "node_id": "MDQ6VXNlcjQ1Mjc4ODQ4",
            "avatar_url": "https://avatars.githubusercontent.com/u/45278848?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/patrickocal",
            "html_url": "https://github.com/patrickocal",
            "followers_url": "https://api.github.com/users/patrickocal/followers",
            "following_url": "https://api.github.com/users/patrickocal/following{/other_user}",
            "gists_url": "https://api.github.com/users/patrickocal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/patrickocal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/patrickocal/subscriptions",
            "organizations_url": "https://api.github.com/users/patrickocal/orgs",
            "repos_url": "https://api.github.com/users/patrickocal/repos",
            "events_url": "https://api.github.com/users/patrickocal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/patrickocal/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-19T14:10:05Z",
        "updated_at": "2023-11-20T00:51:43Z",
        "closed_at": "2023-11-20T00:51:43Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nIf my document is already tokenised by BART, T5 or Llama2, how do I now use LlamaIndex to create a datastore or KnowledgeGraphIndex?\r\n\r\nAlternatively, if the document is already chunked, how would I use LlamaIndex to create a KG and store in a KGIndex?\r\n\r\nAlso, are the KG triples (head, relation, tail) stored as a single vector in a KGIndex class?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9001/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9001/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9000",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9000/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9000/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9000/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9000",
        "id": 2000697450,
        "node_id": "PR_kwDOIWuq585f1RGX",
        "number": 9000,
        "title": "fix openai agent",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-19T04:52:49Z",
        "updated_at": "2023-11-19T05:30:45Z",
        "closed_at": "2023-11-19T05:30:44Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9000",
            "html_url": "https://github.com/run-llama/llama_index/pull/9000",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9000.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9000.patch",
            "merged_at": "2023-11-19T05:30:44Z"
        },
        "body": "allow error handling from tool outputs",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9000/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9000/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8999",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8999/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8999/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8999/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8999",
        "id": 2000684151,
        "node_id": "I_kwDOIWuq5853QAR3",
        "number": 8999,
        "title": "[Bug]: 'Perplexity' object has no attribute 'context_window'",
        "user": {
            "login": "PhiBrandon",
            "id": 24278041,
            "node_id": "MDQ6VXNlcjI0Mjc4MDQx",
            "avatar_url": "https://avatars.githubusercontent.com/u/24278041?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PhiBrandon",
            "html_url": "https://github.com/PhiBrandon",
            "followers_url": "https://api.github.com/users/PhiBrandon/followers",
            "following_url": "https://api.github.com/users/PhiBrandon/following{/other_user}",
            "gists_url": "https://api.github.com/users/PhiBrandon/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PhiBrandon/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PhiBrandon/subscriptions",
            "organizations_url": "https://api.github.com/users/PhiBrandon/orgs",
            "repos_url": "https://api.github.com/users/PhiBrandon/repos",
            "events_url": "https://api.github.com/users/PhiBrandon/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PhiBrandon/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-19T03:51:10Z",
        "updated_at": "2023-11-20T01:06:51Z",
        "closed_at": "2023-11-20T01:06:51Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nWhen utilizing `LLMTextCompletionProgram.from_defaults` with the llm set to a Mistral Perplexity LLM, I get the error 'Perplexity' object has no attribute 'context_window'. \n\n### Version\n\nllama-index==0.9.3.post1\n\n### Steps to Reproduce\n\n## Load Perplexity module\r\n```\r\nfrom llama_index.llms import Perplexity\r\nllm = Perplexity(api_key=PPLX_API_KEY, model=\"mistral-7b-instruct\", max_tokens=3000)\r\n```\r\n\r\n## Create your Model, template and LLMTextCompletionProgram.\r\n```\r\nfrom llama_index.llms.base import ChatMessage\r\nfrom llama_index import PromptTemplate\r\nfrom pydantic import BaseModel\r\nfrom typing import List\r\nfrom llama_index.output_parsers import PydanticOutputParser\r\nfrom llama_index.program import LLMTextCompletionProgram\r\n\r\n\r\nclass Track(BaseModel):\r\n    name: str\r\n    play_time: int\r\n    track_id: int\r\n\r\n\r\nclass Album(BaseModel):\r\n    name: str\r\n    tracks: List[Track]\r\n\r\n\r\nmessages_dict = [\r\n    {\"role\": \"system\", \"content\": \"You're an expert at outputting Pydantic Models. Your output always conforms to the pydantic json model you're given.\"},\r\n    {\"role\": \"user\", \"content\": \"Tell me 5 sentences about Perplexity.\"},\r\n]\r\nmessages = [ChatMessage(**msg) for msg in messages_dict]\r\n\r\ntemplate = PromptTemplate(\r\n    \"Give me a creative album based on the following movie title: {movie_title}\")\r\nprogram = LLMTextCompletionProgram.from_defaults(\r\n    llm=llm,\r\n    output_parser=PydanticOutputParser(Album),\r\n    verbose=True,\r\n    prompt=template\r\n)\r\n```\r\n\r\n## Then you call the program.\r\n`output = program(movie_title=\"The Matrix\")`\r\n\n\n### Relevant Logs/Tracbacks\n\n```shell\n{\r\n\t\"name\": \"AttributeError\",\r\n\t\"message\": \"'Perplexity' object has no attribute 'context_window'\",\r\n\t\"stack\": \"---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/home/bphil/youtube/hormozi_bot/hermes_test.ipynb Cell 6 line 1\r\n----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/bphil/youtube/hormozi_bot/hermes_test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a> output = program(movie_title=\\\"The Matrix\\\")\r\n\r\nFile ~/.local/lib/python3.10/site-packages/llama_index/program/llm_program.py:74, in LLMTextCompletionProgram.__call__(self, *args, **kwargs)\r\n     69 def __call__(\r\n     70     self,\r\n     71     *args: Any,\r\n     72     **kwargs: Any,\r\n     73 ) -> BaseModel:\r\n---> 74     if self._llm.metadata.is_chat_model:\r\n     75         messages = self._prompt.format_messages(llm=self._llm, **kwargs)\r\n     77         response = self._llm.chat(messages)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/llama_index/llms/perplexity.py:88, in Perplexity.metadata(self)\r\n     84 @property\r\n     85 def metadata(self) -> LLMMetadata:\r\n     86     return LLMMetadata(\r\n     87         context_window=self.context_window\r\n---> 88         if self.context_window is not None\r\n     89         else self._get_context_window(),\r\n     90         num_output=self.max_tokens\r\n     91         or -1,  # You can replace this with the appropriate value\r\n     92         is_chat_model=self._is_chat_model(),\r\n     93         model_name=self.model,\r\n     94     )\r\n\r\nAttributeError: 'Perplexity' object has no attribute 'context_window'\"\r\n}\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8999/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8999/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8998",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8998/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8998/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8998/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8998",
        "id": 2000612263,
        "node_id": "PR_kwDOIWuq585f1Alq",
        "number": 8998,
        "title": "Refactor MM retriever classes",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-18T23:04:27Z",
        "updated_at": "2023-11-19T04:24:04Z",
        "closed_at": "2023-11-19T04:24:03Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8998",
            "html_url": "https://github.com/run-llama/llama_index/pull/8998",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8998.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8998.patch",
            "merged_at": "2023-11-19T04:24:03Z"
        },
        "body": "# Description\r\n\r\nReorg the MM retriever a bit\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8998/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8998/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8997",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8997/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8997/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8997/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8997",
        "id": 2000588107,
        "node_id": "PR_kwDOIWuq585f08AO",
        "number": 8997,
        "title": "remove perplexity key",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-18T21:30:37Z",
        "updated_at": "2023-11-18T22:18:28Z",
        "closed_at": "2023-11-18T22:18:27Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8997",
            "html_url": "https://github.com/run-llama/llama_index/pull/8997",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8997.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8997.patch",
            "merged_at": "2023-11-18T22:18:27Z"
        },
        "body": "(note: will also need to fully deactivate the key) ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8997/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8997/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8996",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8996/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8996/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8996/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8996",
        "id": 2000440649,
        "node_id": "I_kwDOIWuq5853PE1J",
        "number": 8996,
        "title": "[Bug]: Doubled TokenCountingHandler() results for LLM completions when used via async aquery()",
        "user": {
            "login": "vizovitin",
            "id": 73231049,
            "node_id": "MDQ6VXNlcjczMjMxMDQ5",
            "avatar_url": "https://avatars.githubusercontent.com/u/73231049?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vizovitin",
            "html_url": "https://github.com/vizovitin",
            "followers_url": "https://api.github.com/users/vizovitin/followers",
            "following_url": "https://api.github.com/users/vizovitin/following{/other_user}",
            "gists_url": "https://api.github.com/users/vizovitin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vizovitin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vizovitin/subscriptions",
            "organizations_url": "https://api.github.com/users/vizovitin/orgs",
            "repos_url": "https://api.github.com/users/vizovitin/repos",
            "events_url": "https://api.github.com/users/vizovitin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vizovitin/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-11-18T14:20:43Z",
        "updated_at": "2023-11-18T17:25:21Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\n`TokenCountingHandler()` produces **incorrect (duplicate) results** for LLM completions (but not embeddings) when used via async APIs, such as `await index.as_query_engine(...).aquery(query)`.\r\n\r\nThe second issue is that **async APIs appear to be fake** - they simply delegate to sync APIs. E.g.:\r\n```py\r\n    @llm_chat_callback()\r\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\r\n        from llama_index.llms.langchain_utils import (\r\n            from_lc_messages,\r\n            to_lc_messages,\r\n        )\r\n\r\n        lc_messages = to_lc_messages(messages)\r\n        lc_message = self._llm.predict_messages(messages=lc_messages, **kwargs)\r\n        message = from_lc_messages([lc_message])[0]\r\n        return ChatResponse(message=message)\r\n\r\n    @llm_chat_callback()\r\n    async def achat(\r\n        self, messages: Sequence[ChatMessage], **kwargs: Any\r\n    ) -> ChatResponse:\r\n        # TODO: Implement async chat\r\n        return self.chat(messages, **kwargs)\r\n\r\n```\r\n\r\n**Root cause:** `@llm_chat_callback()` decorator that delegates event processing to a `CallbackManager()` is used two times in the respective stack of calls, resulting in duplicate events (although they have different `event_id` values). This, in particular, results in doubled total LLM token usage counts reported by a `TokenCountingHandler()`.\r\n\r\n**Workaround:** use sync APIs, e.g. `index.as_query_engine(...).query(query)`.\n\n### Version\n\n0.9.2\n\n### Steps to Reproduce\n\nHere's a code excerpt (adapted from my application, didn't test it individually, but should work mostly as is).\r\n\r\n```py\r\nfrom typing import Any\r\n\r\nfrom loguru import logger\r\nimport openai\r\nimport tiktoken\r\nfrom langchain.chat_models import ChatOpenAI\r\nfrom llama_index import Document, GPTVectorStoreIndex, LLMPredictor, ServiceContext, StorageContext, Prompt, load_index_from_storage\r\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\r\nfrom llama_index.callbacks.simple_llm_handler import SimpleLLMHandler\r\n\r\n\r\nclass StackTraceLlmHandler(SimpleLLMHandler):\r\n    def on_event_end(self,\r\n                     event_type: Any,\r\n                     payload: dict[str, Any] | None = None,\r\n                     event_id: str = \"\",\r\n                     **kwargs: Any,\r\n                     ) -> None:\r\n        super().on_event_end(event_type, payload, event_id, **kwargs)\r\n        try:\r\n            raise RuntimeError(\"trace point\")\r\n        except RuntimeError:\r\n            logger.exception(\"LLM Handler on_event_end(event_type={!r}, event_id={!r})\", event_type, event_id)\r\n\r\n\r\ndef get_index(model_name: str = \"gpt-3.5-turbo-1106\", model_temperature: float = 0.1) -> (GPTVectorStoreIndex, TokenCountingHandler):\r\n    llm = ChatOpenAI(model_name=model_name, temperature=model_temperature)\r\n    token_counter = TokenCountingHandler(\r\n        tokenizer=tiktoken.encoding_for_model(model_name).encode,\r\n        verbose=True,\r\n    )\r\n    callback_manager = CallbackManager([token_counter, StackTraceLlmHandler()])\r\n    service_context = ServiceContext.from_defaults(\r\n        llm_predictor=LLMPredictor(llm=llm),\r\n        callback_manager=callback_manager,\r\n    )\r\n\r\n    index = GPTVectorStoreIndex([], service_context=service_context, use_async=True)\r\n    return index, token_counter\r\n\r\nasync def generate_llm_answer(query: str) -> str:\r\n    index, token_counter = get_index()\r\n    query_engine = index.as_query_engine(\r\n        response_mode=\"compact\",\r\n        similarity_top_k=3,\r\n    )\r\n    response = await query_engine.aquery(query)\r\n    # inspect token_counter (e.g. token_counter.total_llm_token_count) or the logged messages here\r\n    token_counter.reset_counts()\r\n    return str(response)\r\n```\r\n\r\nRun the `generate_llm_answer()` with any argument to reproduce the issue. You may also compare the reported tokens usage with the actually billed tokens on the OpenAI dashboard.\n\n### Relevant Logs/Tracbacks\n\n```shell\nStack trace of the first end event (both decorated functions are visible here):\r\n\r\n\r\n  File \"src/llm/answer.py\", line 43, in generate_llm_answer\r\n    response = await query_engine.aquery(query)\r\n                     \u2502            \u2502      \u2514 'Please introduce yourself and tell others how you work\\n'\r\n                     \u2502            \u2514 <function BaseQueryEngine.aquery at 0x7fb66647e200>\r\n                     \u2514 <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x7fb664109780>\r\n\r\n  File \".venv/lib/python3.10/site-packages/llama_index/core/base_query_engine.py\", line 36, in aquery\r\n    return await self._aquery(str_or_query_bundle)\r\n                 \u2502    \u2502       \u2514 QueryBundle(query_str='Please introduce yourself and tell others how you work\\n', custom_embedding_strs=None, embedding=[0.010...\r\n                 \u2502    \u2514 <function RetrieverQueryEngine._aquery at 0x7fb64f861d80>\r\n                 \u2514 <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x7fb664109780>\r\n  File \".venv/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py\", line 204, in _aquery\r\n    response = await self._response_synthesizer.asynthesize(\r\n                     \u2502    \u2502                     \u2514 <function BaseSynthesizer.asynthesize at 0x7fb6657432e0>\r\n                     \u2502    \u2514 <llama_index.response_synthesizers.compact_and_refine.CompactAndRefine object at 0x7fb664108d00>\r\n                     \u2514 <llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x7fb664109780>\r\n  File \".venv/lib/python3.10/site-packages/llama_index/response_synthesizers/base.py\", line 179, in asynthesize\r\n    response_str = await self.aget_response(\r\n                         \u2502    \u2514 <function CompactAndRefine.aget_response at 0x7fb665743910>\r\n                         \u2514 <llama_index.response_synthesizers.compact_and_refine.CompactAndRefine object at 0x7fb664108d00>\r\n  File \".venv/lib/python3.10/site-packages/llama_index/response_synthesizers/compact_and_refine.py\", line 19, in aget_response\r\n    return await super().aget_response(\r\n  File \".venv/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py\", line 309, in aget_response\r\n    response = await self._agive_response_single(\r\n                     \u2502    \u2514 <function Refine._agive_response_single at 0x7fb664c957e0>\r\n                     \u2514 <llama_index.response_synthesizers.compact_and_refine.CompactAndRefine object at 0x7fb664108d00>\r\n  File \".venv/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py\", line 415, in _agive_response_single\r\n    structured_response = await program.acall(\r\n                                \u2502       \u2514 <function DefaultRefineProgram.acall at 0x7fb664c95240>\r\n                                \u2514 <llama_index.response_synthesizers.refine.DefaultRefineProgram object at 0x7fb64f84a4a0>\r\n  File \".venv/lib/python3.10/site-packages/llama_index/response_synthesizers/refine.py\", line 60, in acall\r\n    answer = await self._llm_predictor.apredict(\r\n                   \u2502    \u2502              \u2514 <function LLMPredictor.apredict at 0x7fb6664a8040>\r\n                   \u2502    \u2514 LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>)\r\n                   \u2514 <llama_index.response_synthesizers.refine.DefaultRefineProgram object at 0x7fb64f84a4a0>\r\n  File \".venv/lib/python3.10/site-packages/llama_index/llm_predictor/base.py\", line 269, in apredict\r\n    chat_response = await self._llm.achat(messages)\r\n                          \u2502    \u2502          \u2514 [ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are a helpful assistant for questions about ...\r\n                          \u2502    \u2514 <member '_llm' of 'LLMPredictor' objects>\r\n                          \u2514 LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>)\r\n  File \".venv/lib/python3.10/site-packages/llama_index/llms/base.py\", line 144, in wrapped_async_llm_chat\r\n    f_return_val = await f(_self, messages, **kwargs)\r\n                         \u2502 \u2502      \u2502           \u2514 {}\r\n                         \u2502 \u2502      \u2514 [ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are a helpful assistant for questions about ...\r\n                         \u2502 \u2514 LangChainLLM(callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fb664b189a0>)\r\n                         \u2514 <function LangChainLLM.achat at 0x7fb6666c75b0>\r\n  File \".venv/lib/python3.10/site-packages/llama_index/llms/langchain.py\", line 136, in achat\r\n    return self.chat(messages, **kwargs)\r\n           \u2502    \u2502    \u2502           \u2514 {}\r\n           \u2502    \u2502    \u2514 [ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are a helpful assistant for questions about ...\r\n           \u2502    \u2514 <function llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat at 0x7fb6666c6dd0>\r\n           \u2514 LangChainLLM(callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fb664b189a0>)\r\n  File \".venv/lib/python3.10/site-packages/llama_index/llms/base.py\", line 208, in wrapped_llm_chat\r\n    callback_manager.on_event_end(\r\n    \u2502                \u2514 <function CallbackManager.on_event_end at 0x7fb66706a680>\r\n    \u2514 <llama_index.callbacks.base.CallbackManager object at 0x7fb664b189a0>\r\n  File \".venv/lib/python3.10/site-packages/llama_index/callbacks/base.py\", line 116, in on_event_end\r\n    handler.on_event_end(event_type, payload, event_id=event_id, **kwargs)\r\n    \u2502       \u2502            \u2502           \u2502                 \u2502           \u2514 {}\r\n    \u2502       \u2502            \u2502           \u2502                 \u2514 '7d6928a4-f377-4d3d-a682-e56252c34649'\r\n    \u2502       \u2502            \u2502           \u2514 {<EventPayload.MESSAGES: 'messages'>: [ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are a helpful assistant for ...\r\n    \u2502       \u2502            \u2514 <CBEventType.LLM: 'llm'>\r\n    \u2502       \u2514 <function StackTraceLlmHandler.on_event_end at 0x7fb664acf5b0>\r\n    \u2514 <llm.engine.StackTraceLlmHandler object at 0x7fb664b18d30>\r\n\r\n> File \"src/llm/engine.py\", line 27, in on_event_end\r\n    raise RuntimeError(\"trace point\")\r\n\r\n\r\nMeanwhile, the token counter will also log to console:\r\n\r\n\r\nEmbedding Token Usage: 10\r\nLLM Prompt Token Usage: 835\r\nLLM Completion Token Usage: 124\r\nLLM Prompt Token Usage: 835\r\nLLM Completion Token Usage: 124\r\n\r\n\r\ntoken_counter.total_llm_token_count == 1918\r\nBut it should be 835+124=959, which is exactly half.\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8996/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8996/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8995",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8995/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8995/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8995/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8995",
        "id": 2000440363,
        "node_id": "I_kwDOIWuq5853PEwr",
        "number": 8995,
        "title": "[Bug]: Azure OpenAI sometimes sends responses lacking a delta or choice key in the response dict - this causes a silent failure.",
        "user": {
            "login": "MikeRenwick-ICG",
            "id": 141156239,
            "node_id": "U_kgDOCGnfjw",
            "avatar_url": "https://avatars.githubusercontent.com/u/141156239?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/MikeRenwick-ICG",
            "html_url": "https://github.com/MikeRenwick-ICG",
            "followers_url": "https://api.github.com/users/MikeRenwick-ICG/followers",
            "following_url": "https://api.github.com/users/MikeRenwick-ICG/following{/other_user}",
            "gists_url": "https://api.github.com/users/MikeRenwick-ICG/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/MikeRenwick-ICG/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/MikeRenwick-ICG/subscriptions",
            "organizations_url": "https://api.github.com/users/MikeRenwick-ICG/orgs",
            "repos_url": "https://api.github.com/users/MikeRenwick-ICG/repos",
            "events_url": "https://api.github.com/users/MikeRenwick-ICG/events{/privacy}",
            "received_events_url": "https://api.github.com/users/MikeRenwick-ICG/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 8,
        "created_at": "2023-11-18T14:19:49Z",
        "updated_at": "2023-11-22T20:06:14Z",
        "closed_at": "2023-11-21T00:55:50Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nAzure OpenAI (for example GPT-4-32k-0613) sometimes returns packets that are missing choices & delta keys.  This causes a silent failure.\r\n\r\n\n\n### Version\n\nllama-index-0.9.3.post1\n\n### Steps to Reproduce\n\n1) Setup an [Azure OpenAI deployment and llamaindex example](https://docs.llamaindex.ai/en/stable/examples/llm/azure_openai.html)\r\n2) Configure [content filtering](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter?tabs=warning%2Cpython\r\n\r\nRun a query and you'll note that it fails silently.\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8995/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8995/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8994",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8994/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8994/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8994/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8994",
        "id": 2000341072,
        "node_id": "PR_kwDOIWuq585f0LKm",
        "number": 8994,
        "title": "Fix TextNode instantiation on SupabaseVectorIndexDemo",
        "user": {
            "login": "albertlieyingadrian",
            "id": 12984659,
            "node_id": "MDQ6VXNlcjEyOTg0NjU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/12984659?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/albertlieyingadrian",
            "html_url": "https://github.com/albertlieyingadrian",
            "followers_url": "https://api.github.com/users/albertlieyingadrian/followers",
            "following_url": "https://api.github.com/users/albertlieyingadrian/following{/other_user}",
            "gists_url": "https://api.github.com/users/albertlieyingadrian/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/albertlieyingadrian/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/albertlieyingadrian/subscriptions",
            "organizations_url": "https://api.github.com/users/albertlieyingadrian/orgs",
            "repos_url": "https://api.github.com/users/albertlieyingadrian/repos",
            "events_url": "https://api.github.com/users/albertlieyingadrian/events{/privacy}",
            "received_events_url": "https://api.github.com/users/albertlieyingadrian/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-18T09:38:53Z",
        "updated_at": "2023-11-19T04:31:28Z",
        "closed_at": "2023-11-19T04:31:28Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8994",
            "html_url": "https://github.com/run-llama/llama_index/pull/8994",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8994.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8994.patch",
            "merged_at": "2023-11-19T04:31:28Z"
        },
        "body": "# Description\r\nTextNode is a derived class from [Pydantic](https://docs.pydantic.dev/latest/api/base_model/) that only accepts 1 argument instead of multiple arguments. The current way of instantiating TextNode is causing an `__init__()` error and the demo will break\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/albertlieadrian/Documents/Random/streamlit-chatbot-playground/llamaindex_supabase.py\", line 42, in <module>\r\n    TextNode(\r\n  File \"pydantic/main.py\", line 332, in pydantic.main.BaseModel.__init__\r\nTypeError: __init__() takes exactly 1 positional argument (2 given)\r\n```\r\n\r\nFixes # (issue)\r\nAn example of correct way to instantiate Pydantic model is as follows:\r\n```python\r\nfrom pydantic import BaseModel, ValidationError\r\n \r\nclass Person(BaseModel):\r\n    age: int\r\n    name: str\r\n    is_married: bool\r\n \r\ndata = {\r\n    'name': 'John',\r\n    'age': 20,\r\n    'is_married': False\r\n}\r\n \r\ntry:\r\n    person = Person(**data)\r\n    print(person.dict())\r\n \r\nexcept ValidationError as e:\r\n    print(e)\r\n```\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] My changes generate no new warnings",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8994/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8994/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8993",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8993/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8993/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8993/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8993",
        "id": 2000334175,
        "node_id": "I_kwDOIWuq5853Oq1f",
        "number": 8993,
        "title": "[Bug]: AttributeError: 'Document' object has no attribute 'get_doc_id'",
        "user": {
            "login": "newsta-ai",
            "id": 115063446,
            "node_id": "U_kgDOBtu6lg",
            "avatar_url": "https://avatars.githubusercontent.com/u/115063446?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/newsta-ai",
            "html_url": "https://github.com/newsta-ai",
            "followers_url": "https://api.github.com/users/newsta-ai/followers",
            "following_url": "https://api.github.com/users/newsta-ai/following{/other_user}",
            "gists_url": "https://api.github.com/users/newsta-ai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/newsta-ai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/newsta-ai/subscriptions",
            "organizations_url": "https://api.github.com/users/newsta-ai/orgs",
            "repos_url": "https://api.github.com/users/newsta-ai/repos",
            "events_url": "https://api.github.com/users/newsta-ai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/newsta-ai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "assignees": [
            {
                "login": "nerdai",
                "id": 92402603,
                "node_id": "U_kgDOBYHzqw",
                "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
                "gravatar_id": "",
                "url": "https://api.github.com/users/nerdai",
                "html_url": "https://github.com/nerdai",
                "followers_url": "https://api.github.com/users/nerdai/followers",
                "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
                "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
                "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
                "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
                "organizations_url": "https://api.github.com/users/nerdai/orgs",
                "repos_url": "https://api.github.com/users/nerdai/repos",
                "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
                "received_events_url": "https://api.github.com/users/nerdai/received_events",
                "type": "User",
                "site_admin": false
            }
        ],
        "milestone": null,
        "comments": 8,
        "created_at": "2023-11-18T09:15:34Z",
        "updated_at": "2023-11-20T15:09:52Z",
        "closed_at": "2023-11-20T15:09:06Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nIn the file \r\nllama_index/indices/base.py\", line 97, in from_documents\r\n\r\ndocstore.set_document_hash(doc.get_doc_id(), doc.hash) gives an attribute error:\r\nAttributeError: 'Document' object has no attribute 'get_doc_id'\r\n\r\nThis is triggered by calling the following using default values.\r\nVectorStoreIndex.from_documents(documents=documents, storage_context=storage_context)\n\n### Version\n\nv0.9.3.post1\n\n### Steps to Reproduce\n\nThere has been a change in the Document class in schemas file, where a new property doc_id has been added. However, in the file indices/base.py it still references get_doc_id(), which as a matter of fact exists in the Document class in schema, but still throws an error.\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8993/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8993/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8992",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8992/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8992/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8992/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8992",
        "id": 2000252819,
        "node_id": "I_kwDOIWuq5853OW-T",
        "number": 8992,
        "title": "[Bug]: ServiceContext' object has no attribute 'node_parser'",
        "user": {
            "login": "albertlieyingadrian",
            "id": 12984659,
            "node_id": "MDQ6VXNlcjEyOTg0NjU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/12984659?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/albertlieyingadrian",
            "html_url": "https://github.com/albertlieyingadrian",
            "followers_url": "https://api.github.com/users/albertlieyingadrian/followers",
            "following_url": "https://api.github.com/users/albertlieyingadrian/following{/other_user}",
            "gists_url": "https://api.github.com/users/albertlieyingadrian/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/albertlieyingadrian/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/albertlieyingadrian/subscriptions",
            "organizations_url": "https://api.github.com/users/albertlieyingadrian/orgs",
            "repos_url": "https://api.github.com/users/albertlieyingadrian/repos",
            "events_url": "https://api.github.com/users/albertlieyingadrian/events{/privacy}",
            "received_events_url": "https://api.github.com/users/albertlieyingadrian/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-11-18T05:38:59Z",
        "updated_at": "2023-11-20T01:22:55Z",
        "closed_at": "2023-11-20T01:22:55Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI got an error that says ServiceContext object has no node_parser when trying this example https://github.com/run-llama/llama_index/blob/adb054429f642cc7bbfcb66d4c232e072325eeab/docs/examples/query_engine/pdf_tables/recursive_retriever.ipynb#L647\r\n\r\n```\r\n  llm = OpenAI(temperature=0, model=\"gpt-4\")\r\n\r\n  service_context = ServiceContext.from_defaults(\r\n      llm=llm,\r\n  )\r\n\r\n  doc_nodes = service_context.node_parser.get_nodes_from_documents(docs)\r\n\r\n  summaries = [\r\n    \"This node provides information about the world's richest billionaires in 2023\",\r\n    \"This node provides information on the number of billionaires and their combined net worth from 2000 to 2023.\",\r\n  ]\r\n```\n\n### Version\n\n0.9.3.post1\n\n### Steps to Reproduce\n\nJust run the example or run it from this Google Colab https://colab.research.google.com/drive/1DldMhszgSI4KKI2UziNHHM4w8Cb5OxEL#scrollTo=Ht4oSN2PvzUJ\n\n### Relevant Logs/Tracbacks\n\n```shell\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-22-78805de77ca5> in <cell line: 1>()\r\n----> 1 doc_nodes = service_context.node_parser.get_nodes_from_documents(docs)\r\n\r\nAttributeError: 'ServiceContext' object has no attribute 'node_parser'\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8992/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8992/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8991",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8991/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8991/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8991/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8991",
        "id": 2000203241,
        "node_id": "PR_kwDOIWuq585fzuKs",
        "number": 8991,
        "title": "Milvus support for Hybrid Search",
        "user": {
            "login": "MaliahRajan",
            "id": 16203679,
            "node_id": "MDQ6VXNlcjE2MjAzNjc5",
            "avatar_url": "https://avatars.githubusercontent.com/u/16203679?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/MaliahRajan",
            "html_url": "https://github.com/MaliahRajan",
            "followers_url": "https://api.github.com/users/MaliahRajan/followers",
            "following_url": "https://api.github.com/users/MaliahRajan/following{/other_user}",
            "gists_url": "https://api.github.com/users/MaliahRajan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/MaliahRajan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/MaliahRajan/subscriptions",
            "organizations_url": "https://api.github.com/users/MaliahRajan/orgs",
            "repos_url": "https://api.github.com/users/MaliahRajan/repos",
            "events_url": "https://api.github.com/users/MaliahRajan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/MaliahRajan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-18T03:54:03Z",
        "updated_at": "2023-11-18T04:49:10Z",
        "closed_at": "2023-11-18T04:49:09Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8991",
            "html_url": "https://github.com/run-llama/llama_index/pull/8991",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8991.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8991.patch",
            "merged_at": null
        },
        "body": "# Description\r\nIn Milvus, a hybrid search is a vector search that incorporates the attribute filtering. Using specific Boolean expressions to filter either the scalar fields or the primary key field, you can apply specific conditions to narrow down your search.\r\n\r\nhttps://milvus.io/docs/hybridsearch.md\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8991/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8991/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8990",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8990/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8990/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8990/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8990",
        "id": 2000171465,
        "node_id": "I_kwDOIWuq5853ODHJ",
        "number": 8990,
        "title": "[Feature Request]: Pass ssl certificate to AzureOpenAi",
        "user": {
            "login": "fcasalen",
            "id": 119017871,
            "node_id": "U_kgDOBxgRjw",
            "avatar_url": "https://avatars.githubusercontent.com/u/119017871?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/fcasalen",
            "html_url": "https://github.com/fcasalen",
            "followers_url": "https://api.github.com/users/fcasalen/followers",
            "following_url": "https://api.github.com/users/fcasalen/following{/other_user}",
            "gists_url": "https://api.github.com/users/fcasalen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/fcasalen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/fcasalen/subscriptions",
            "organizations_url": "https://api.github.com/users/fcasalen/orgs",
            "repos_url": "https://api.github.com/users/fcasalen/repos",
            "events_url": "https://api.github.com/users/fcasalen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/fcasalen/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-18T02:36:14Z",
        "updated_at": "2023-11-18T19:00:02Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\r\n\r\nhability to pass a SSL certificate or a http client with a valid certificate to AzureOpenAI class\r\n\r\n### Reason\r\n\r\nSome models in Azure require a SSL certificate to be passed or a http client with a valid certificate\r\n\r\n### Value of Feature\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8990/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8990/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8989",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8989/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8989/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8989/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8989",
        "id": 2000035343,
        "node_id": "I_kwDOIWuq5853Nh4P",
        "number": 8989,
        "title": "Azure OpenAI REST URLs constructed incorrectly [Bug]: ",
        "user": {
            "login": "hjfeldy",
            "id": 71898679,
            "node_id": "MDQ6VXNlcjcxODk4Njc5",
            "avatar_url": "https://avatars.githubusercontent.com/u/71898679?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hjfeldy",
            "html_url": "https://github.com/hjfeldy",
            "followers_url": "https://api.github.com/users/hjfeldy/followers",
            "following_url": "https://api.github.com/users/hjfeldy/following{/other_user}",
            "gists_url": "https://api.github.com/users/hjfeldy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hjfeldy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hjfeldy/subscriptions",
            "organizations_url": "https://api.github.com/users/hjfeldy/orgs",
            "repos_url": "https://api.github.com/users/hjfeldy/repos",
            "events_url": "https://api.github.com/users/hjfeldy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hjfeldy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-17T22:38:33Z",
        "updated_at": "2023-11-17T23:29:39Z",
        "closed_at": "2023-11-17T23:29:38Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI just updated to the recent release, and all the API calls were failing. I looked at the debug logs and saw this:\r\nDEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/deployments/<my-resource>/chat/completions', 'files': ...\r\n\r\nI was able to get everything working by patching the low level request functions in the openai library, but that is far from ideal. Should be a quick fix for anyone familiar with the codebase.\n\n### Version\n\nv0.9.3.post1\n\n### Steps to Reproduce\n\nInstantiate an AzureOpenAI object according to the documentation specs (existing code which worked in the prior version), and make any LLM request\n\n### Relevant Logs/Tracbacks\n\n```shell\nDEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/deployments/<my-resource>/chat/completions', 'files': ...\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8989/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8989/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8988",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8988/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8988/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8988/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8988",
        "id": 1999946395,
        "node_id": "PR_kwDOIWuq585fy2e4",
        "number": 8988,
        "title": "patch v0.9.3.post1",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-17T21:21:31Z",
        "updated_at": "2023-11-17T21:27:09Z",
        "closed_at": "2023-11-17T21:27:08Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8988",
            "html_url": "https://github.com/run-llama/llama_index/pull/8988",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8988.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8988.patch",
            "merged_at": "2023-11-17T21:27:08Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8988/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8988/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8987",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8987/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8987/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8987/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8987",
        "id": 1999858595,
        "node_id": "PR_kwDOIWuq585fyjHp",
        "number": 8987,
        "title": "Add more CLIP models",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-17T20:15:18Z",
        "updated_at": "2023-11-17T20:22:11Z",
        "closed_at": "2023-11-17T20:22:10Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8987",
            "html_url": "https://github.com/run-llama/llama_index/pull/8987",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8987.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8987.patch",
            "merged_at": "2023-11-17T20:22:10Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8987/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8987/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8986",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8986/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8986/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8986/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8986",
        "id": 1999811803,
        "node_id": "PR_kwDOIWuq585fyYqd",
        "number": 8986,
        "title": "Fix pickle (last time?) and import errors",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-17T19:49:22Z",
        "updated_at": "2023-11-17T21:16:21Z",
        "closed_at": "2023-11-17T21:16:20Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8986",
            "html_url": "https://github.com/run-llama/llama_index/pull/8986",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8986.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8986.patch",
            "merged_at": "2023-11-17T21:16:20Z"
        },
        "body": "The `getstate()` was removing private variables, which is FINE technically, but since we aren't calling `init` in setstate, this was causing issues (i.e. with LLMs)\r\n\r\nThis fix introduces a fix to make both worlds happy\r\n\r\nThe function `df_make_pretty()` introduced some imports that we can't support right now, so removing that",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8986/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8986/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8985",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8985/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8985/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8985/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8985",
        "id": 1999758802,
        "node_id": "PR_kwDOIWuq585fyM-B",
        "number": 8985,
        "title": "LlamaIndex support for Amazon Bedrock Llama 2 Chat 13b",
        "user": {
            "login": "windson",
            "id": 1826682,
            "node_id": "MDQ6VXNlcjE4MjY2ODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1826682?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/windson",
            "html_url": "https://github.com/windson",
            "followers_url": "https://api.github.com/users/windson/followers",
            "following_url": "https://api.github.com/users/windson/following{/other_user}",
            "gists_url": "https://api.github.com/users/windson/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/windson/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/windson/subscriptions",
            "organizations_url": "https://api.github.com/users/windson/orgs",
            "repos_url": "https://api.github.com/users/windson/repos",
            "events_url": "https://api.github.com/users/windson/events{/privacy}",
            "received_events_url": "https://api.github.com/users/windson/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "assignees": [
            {
                "login": "hatianzhang",
                "id": 2142132,
                "node_id": "MDQ6VXNlcjIxNDIxMzI=",
                "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
                "gravatar_id": "",
                "url": "https://api.github.com/users/hatianzhang",
                "html_url": "https://github.com/hatianzhang",
                "followers_url": "https://api.github.com/users/hatianzhang/followers",
                "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
                "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
                "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
                "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
                "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
                "repos_url": "https://api.github.com/users/hatianzhang/repos",
                "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
                "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
                "type": "User",
                "site_admin": false
            }
        ],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-17T19:12:40Z",
        "updated_at": "2023-11-20T09:11:39Z",
        "closed_at": "2023-11-17T19:38:50Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8985",
            "html_url": "https://github.com/run-llama/llama_index/pull/8985",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8985.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8985.patch",
            "merged_at": "2023-11-17T19:38:50Z"
        },
        "body": "# Description\r\n\r\nLlama Index now supports Llama 2 LLM available on Amazon Bedrock.\r\nBedrock Model identifier for Llama 2 chat 13b: `meta.llama2-13b-chat-v1`\r\nImplementation covers support for\r\n- `complete` endpoint response,\r\n-  Chat messages `ChatMessage` and\r\n- Streaming support for `stream_complete` endpoint\r\n\r\n\r\nReferences: \r\n\r\n\ud83d\udc49 [Amazon Bedrock now provides access to Meta\u2019s Llama 2 Chat 13B model](https://aws.amazon.com/blogs/aws/amazon-bedrock-now-provides-access-to-llama-2-chat-13b-model/)\r\n\r\n\ud83d\udc49 How To Blog: [LlamaIndex support for Amazon Bedrock Llama 2 Chat 13B](https://www.linkedin.com/pulse/llamaindex-support-amazon-bedrock-llama-2-chat-13b-pavan-kumar-rao-xtync/)\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [x] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8985/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8985/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8984",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8984/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8984/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8984/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8984",
        "id": 1999617872,
        "node_id": "PR_kwDOIWuq585fxt_1",
        "number": 8984,
        "title": "[version] bump to v0.9.3",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-17T17:31:38Z",
        "updated_at": "2023-11-21T01:07:43Z",
        "closed_at": "2023-11-17T17:38:20Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8984",
            "html_url": "https://github.com/run-llama/llama_index/pull/8984",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8984.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8984.patch",
            "merged_at": "2023-11-17T17:38:20Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8984/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8984/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8983",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8983/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8983/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8983/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8983",
        "id": 1999564744,
        "node_id": "PR_kwDOIWuq585fxid8",
        "number": 8983,
        "title": "fix hierarchical node parser bugs",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-17T16:57:57Z",
        "updated_at": "2023-11-17T17:03:10Z",
        "closed_at": "2023-11-17T17:03:09Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8983",
            "html_url": "https://github.com/run-llama/llama_index/pull/8983",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8983.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8983.patch",
            "merged_at": "2023-11-17T17:03:09Z"
        },
        "body": "# Description\r\n\r\nSmall bug with chunk overlap and doc ids\r\n\r\nFixes https://github.com/run-llama/llama_index/issues/8982\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8983/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8983/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8982",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8982/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8982/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8982/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8982",
        "id": 1999530646,
        "node_id": "I_kwDOIWuq5853LmqW",
        "number": 8982,
        "title": "[Bug]:  HierarchicalNodeParser no longer functionnal",
        "user": {
            "login": "krsamuel",
            "id": 48834870,
            "node_id": "MDQ6VXNlcjQ4ODM0ODcw",
            "avatar_url": "https://avatars.githubusercontent.com/u/48834870?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/krsamuel",
            "html_url": "https://github.com/krsamuel",
            "followers_url": "https://api.github.com/users/krsamuel/followers",
            "following_url": "https://api.github.com/users/krsamuel/following{/other_user}",
            "gists_url": "https://api.github.com/users/krsamuel/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/krsamuel/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/krsamuel/subscriptions",
            "organizations_url": "https://api.github.com/users/krsamuel/orgs",
            "repos_url": "https://api.github.com/users/krsamuel/repos",
            "events_url": "https://api.github.com/users/krsamuel/events{/privacy}",
            "received_events_url": "https://api.github.com/users/krsamuel/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-17T16:45:40Z",
        "updated_at": "2023-11-17T17:03:10Z",
        "closed_at": "2023-11-17T17:03:10Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nWhen using the [Auto Merging Retriever](https://gpt-index.readthedocs.io/en/latest/examples/retrievers/auto_merging_retriever.html) several bugs appear:\r\n\r\n-  Bug 1: calling the **`HierarchicalNodeParser`** class method `_from_defaults_ fails`, the default chunk sizes causing a clash with the default chunk overlap\r\n-  Bug 2: calling  the **`HierarchicalNodeParser`** method `_get_nodes_from_documents_` fails due to an error in `NodeParser`\r\n\r\nI maybe identified the errors:\r\n\r\n-  Bug 1: could be solved by updating the value for `SENTENCE_CHUNK_OVERLAP =  200` in _llama_index/node_parser/text/sentence.py_ (line 18) or updating the default `chunk_sizes = [2048, 512, 128]` in _llama_index/node_parser/relational/hierarchical.py_ (line 91) so that chunk_overlap is no longer > chunk size when defaulting, when assigned chunk_size 128.\r\n\r\n- Bug 2: could be solved by replacing at line 56 `doc_id_to_document = {doc.doc_id: doc for doc in documents}` by  `doc_id_to_document = {doc.id_: doc for doc in documents}` in _llama_index/node_parser/interface.py_, because sometimes the object is a TextNode depending on the recursivity step in the method `__recursively_get_nodes_from_nodes_` from _llama_index/node_parser/relational/hierarchical.py_\r\n\r\nIf somebody with more experience of use with the package can check, would be great.\r\n\r\nThanks a lot !\n\n### Version\n\nv0.9.2\n\n### Steps to Reproduce\n\nCode on [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/examples/retrievers/auto_merging_retriever.html) page with example of the Auto Merging Retriever.\r\n\r\n```\r\n!mkdir -p 'data/'\r\n!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\"\r\n\r\nfrom pathlib import Path\r\nfrom llama_hub.file.pymu_pdf.base import PyMuPDFReader\r\n\r\nloader = PyMuPDFReader()\r\ndocs0 = loader.load(file_path=Path(\"./data/llama2.pdf\"))\r\n\r\nfrom llama_index import Document\r\n\r\ndoc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\r\ndocs = [Document(text=doc_text)]\r\n\r\nfrom llama_index.node_parser import (\r\n    HierarchicalNodeParser,\r\n    SentenceSplitter,\r\n)\r\n\r\nnode_parser = HierarchicalNodeParser.from_defaults()\r\n# node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes = [2048, 512]) allows to see following bug\r\n\r\nnodes = node_parser.get_nodes_from_documents(docs)\r\n```\n\n### Relevant Logs/Tracbacks\n\n```shell\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[11], line 1\r\n----> 1 node_parser = HierarchicalNodeParser.from_defaults()\r\n\r\nFile ~/.pyenv/versions/3.10.13/envs/myenv/lib/python3.10/site-packages/llama_index/node_parser/relational/hierarchical.py:96, in HierarchicalNodeParser.from_defaults(cls, chunk_sizes, node_parser_ids, node_parser_map, include_metadata, include_prev_next_rel, callback_manager)\r\n     94     node_parser_map = {}\r\n     95     for chunk_size, node_parser_id in zip(chunk_sizes, node_parser_ids):\r\n---> 96         node_parser_map[node_parser_id] = SentenceSplitter(\r\n     97             chunk_size=chunk_size,\r\n     98             callback_manager=callback_manager,\r\n     99         )\r\n    100 else:\r\n    101     if chunk_sizes is not None:\r\n\r\nFile ~/.pyenv/versions/3.10.13/envs/myenv/lib/python3.10/site-packages/llama_index/node_parser/text/sentence.py:77, in SentenceSplitter.__init__(self, separator, chunk_size, chunk_overlap, tokenizer, paragraph_separator, chunking_tokenizer_fn, secondary_chunking_regex, callback_manager, include_metadata, include_prev_next_rel)\r\n     75 \"\"\"Initialize with parameters.\"\"\"\r\n     76 if chunk_overlap > chunk_size:\r\n---> 77     raise ValueError(\r\n     78         f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\r\n     79         f\"({chunk_size}), should be smaller.\"\r\n     80     )\r\n     82 callback_manager = callback_manager or CallbackManager([])\r\n     83 self._chunking_tokenizer_fn = (\r\n     84     chunking_tokenizer_fn or split_by_sentence_tokenizer()\r\n     85 )\r\n\r\nValueError: Got a larger chunk overlap (200) than chunk size (128), should be smaller.\r\n\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[14], line 1\r\n----> 1 nodes = node_parser.get_nodes_from_documents(docs, show_progress=True)\r\n\r\nFile ~/.pyenv/versions/3.10.13/envs/myenv/lib/python3.10/site-packages/llama_index/node_parser/relational/hierarchical.py:191, in HierarchicalNodeParser.get_nodes_from_documents(self, documents, show_progress, **kwargs)\r\n    189 # TODO: a bit of a hack rn for tqdm\r\n    190 for doc in documents_with_progress:\r\n--> 191     nodes_from_doc = self._recursively_get_nodes_from_nodes([doc], 0)\r\n    192     all_nodes.extend(nodes_from_doc)\r\n    194 event.on_end(payload={EventPayload.NODES: all_nodes})\r\n\r\nFile ~/.pyenv/versions/3.10.13/envs/myenv/lib/python3.10/site-packages/llama_index/node_parser/relational/hierarchical.py:158, in HierarchicalNodeParser._recursively_get_nodes_from_nodes(self, nodes, level, show_progress)\r\n    156 # now for each sub-node, recursively split into sub-sub-nodes, and add\r\n    157 if level < len(self.node_parser_ids) - 1:\r\n--> 158     sub_sub_nodes = self._recursively_get_nodes_from_nodes(\r\n    159         sub_nodes,\r\n    160         level + 1,\r\n    161         show_progress=show_progress,\r\n    162     )\r\n    163 else:\r\n    164     sub_sub_nodes = []\r\n\r\nFile ~/.pyenv/versions/3.10.13/envs/myenv/lib/python3.10/site-packages/llama_index/node_parser/relational/hierarchical.py:142, in HierarchicalNodeParser._recursively_get_nodes_from_nodes(self, nodes, level, show_progress)\r\n    138 sub_nodes = []\r\n    139 for node in nodes_with_progress:\r\n    140     cur_sub_nodes = self.node_parser_map[\r\n    141         self.node_parser_ids[level]\r\n--> 142     ].get_nodes_from_documents([node])\r\n    143     # add parent relationship from sub node to parent node\r\n    144     # add child relationship from parent node to sub node\r\n    145     # NOTE: Only add relationships if level > 0, since we don't want to add\r\n    146     # relationships for the top-level document objects that we are splitting\r\n    147     if level > 0:\r\n\r\nFile ~/.pyenv/versions/3.10.13/envs/myenv/lib/python3.10/site-packages/llama_index/node_parser/interface.py:56, in NodeParser.get_nodes_from_documents(self, documents, show_progress, **kwargs)\r\n     43 def get_nodes_from_documents(\r\n     44     self,\r\n     45     documents: Sequence[Document],\r\n     46     show_progress: bool = False,\r\n     47     **kwargs: Any,\r\n     48 ) -> List[BaseNode]:\r\n     49     \"\"\"Parse documents into nodes.\r\n     50 \r\n     51     Args:\r\n   (...)\r\n     54 \r\n     55     \"\"\"\r\n---> 56     doc_id_to_document = {doc.doc_id: doc for doc in documents}\r\n     58     with self.callback_manager.event(\r\n     59         CBEventType.NODE_PARSING, payload={EventPayload.DOCUMENTS: documents}\r\n     60     ) as event:\r\n     61         nodes = self._parse_nodes(documents, show_progress=show_progress, **kwargs)\r\n\r\nFile ~/.pyenv/versions/3.10.13/envs/myenv/lib/python3.10/site-packages/llama_index/node_parser/interface.py:56, in <dictcomp>(.0)\r\n     43 def get_nodes_from_documents(\r\n     44     self,\r\n     45     documents: Sequence[Document],\r\n     46     show_progress: bool = False,\r\n     47     **kwargs: Any,\r\n     48 ) -> List[BaseNode]:\r\n     49     \"\"\"Parse documents into nodes.\r\n     50 \r\n     51     Args:\r\n   (...)\r\n     54 \r\n     55     \"\"\"\r\n---> 56     doc_id_to_document = {doc.doc_id: doc for doc in documents}\r\n     58     with self.callback_manager.event(\r\n     59         CBEventType.NODE_PARSING, payload={EventPayload.DOCUMENTS: documents}\r\n     60     ) as event:\r\n     61         nodes = self._parse_nodes(documents, show_progress=show_progress, **kwargs)\r\n\r\nAttributeError: 'TextNode' object has no attribute 'doc_id'\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8982/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8982/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8981",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8981/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8981/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8981/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8981",
        "id": 1999511185,
        "node_id": "PR_kwDOIWuq585fxW5f",
        "number": 8981,
        "title": "fix token counting for new openai client",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-17T16:34:32Z",
        "updated_at": "2023-11-17T16:47:16Z",
        "closed_at": "2023-11-17T16:47:15Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8981",
            "html_url": "https://github.com/run-llama/llama_index/pull/8981",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8981.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8981.patch",
            "merged_at": "2023-11-17T16:47:15Z"
        },
        "body": "# Description\r\n\r\nThe new openai client changed where to find token counts, causing the token counter to be very bad for OpenAI function calling.\r\n\r\nFixes https://github.com/run-llama/llama_index/issues/8978\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8981/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8981/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8980",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8980/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8980/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8980/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8980",
        "id": 1999487616,
        "node_id": "I_kwDOIWuq5853LcKA",
        "number": 8980,
        "title": "[Bug]: Text Splitter removed but needed for loaders",
        "user": {
            "login": "ignaciocolussi",
            "id": 72051072,
            "node_id": "MDQ6VXNlcjcyMDUxMDcy",
            "avatar_url": "https://avatars.githubusercontent.com/u/72051072?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ignaciocolussi",
            "html_url": "https://github.com/ignaciocolussi",
            "followers_url": "https://api.github.com/users/ignaciocolussi/followers",
            "following_url": "https://api.github.com/users/ignaciocolussi/following{/other_user}",
            "gists_url": "https://api.github.com/users/ignaciocolussi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ignaciocolussi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ignaciocolussi/subscriptions",
            "organizations_url": "https://api.github.com/users/ignaciocolussi/orgs",
            "repos_url": "https://api.github.com/users/ignaciocolussi/repos",
            "events_url": "https://api.github.com/users/ignaciocolussi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ignaciocolussi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-17T16:23:00Z",
        "updated_at": "2023-11-17T16:56:14Z",
        "closed_at": "2023-11-17T16:56:14Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nIn PR #8814 was removed the class TextSplitter in the file llama_index/text_splitter/__init__.py wich is imported in file llama_index/langchain_helpers/text_splitter.py needed for some integratios such as ReadabilityWebReader. \r\n\n\n### Version\n\n0.9.2\n\n### Steps to Reproduce\n\nTry to dwnload loader using \r\n`ReadabilityWebPageReader = download_loader(\"ReadabilityWebPageReader\")`\n\n### Relevant Logs/Tracbacks\n\n```shell\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n/tmp/ipykernel_23200/1765377065.py in <module>\r\n      9 #Inicializo reader de llama_index\r\n     10 SitemapReader = download_loader(\"SitemapReader\")\r\n---> 11 ReadabilityWebPageReader = download_loader(\"ReadabilityWebPageReader\")\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/readers/download.py in download_loader(loader_class, loader_hub_url, refresh_cache, use_gpt_index_import, custom_path)\r\n     42         custom_dir = \"llamahub_modules\"\r\n     43 \r\n---> 44     reader_cls = download_llama_module(\r\n     45         loader_class,\r\n     46         llama_hub_url=loader_hub_url,\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/download/download_utils.py in download_llama_module(module_class, llama_hub_url, refresh_cache, custom_dir, custom_path, library_path, base_file_name, use_gpt_index_import)\r\n    287         )\r\n    288     module = util.module_from_spec(spec)\r\n--> 289     spec.loader.exec_module(module)  # type: ignore\r\n    290 \r\n    291     return getattr(module, module_class)\r\n\r\n/usr/lib/python3.10/importlib/_bootstrap_external.py in exec_module(self, module)\r\n\r\n/usr/lib/python3.10/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/download/llamahub_modules/web/readability_web/base.py in <module>\r\n      3 from typing import Any, Callable, Dict, List, Literal, Optional, cast\r\n      4 \r\n----> 5 from llama_index.langchain_helpers.text_splitter import TextSplitter\r\n      6 from llama_index.readers.base import BaseReader\r\n      7 from llama_index.readers.schema.base import Document\r\n\r\nImportError: cannot import name 'TextSplitter' from 'llama_index.langchain_helpers.text_splitter' (/home/ignac/.local/lib/python3.10/site-packages/llama_index/langchain_helpers/text_splitter.py)\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8980/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8980/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8979",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8979/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8979/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8979/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8979",
        "id": 1999225669,
        "node_id": "I_kwDOIWuq5853KcNF",
        "number": 8979,
        "title": "[Feature Request]: Upgrade conda-forge with newest package",
        "user": {
            "login": "mikekuzak",
            "id": 18675587,
            "node_id": "MDQ6VXNlcjE4Njc1NTg3",
            "avatar_url": "https://avatars.githubusercontent.com/u/18675587?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mikekuzak",
            "html_url": "https://github.com/mikekuzak",
            "followers_url": "https://api.github.com/users/mikekuzak/followers",
            "following_url": "https://api.github.com/users/mikekuzak/following{/other_user}",
            "gists_url": "https://api.github.com/users/mikekuzak/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mikekuzak/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mikekuzak/subscriptions",
            "organizations_url": "https://api.github.com/users/mikekuzak/orgs",
            "repos_url": "https://api.github.com/users/mikekuzak/repos",
            "events_url": "https://api.github.com/users/mikekuzak/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mikekuzak/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "assignees": [
            {
                "login": "nerdai",
                "id": 92402603,
                "node_id": "U_kgDOBYHzqw",
                "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
                "gravatar_id": "",
                "url": "https://api.github.com/users/nerdai",
                "html_url": "https://github.com/nerdai",
                "followers_url": "https://api.github.com/users/nerdai/followers",
                "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
                "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
                "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
                "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
                "organizations_url": "https://api.github.com/users/nerdai/orgs",
                "repos_url": "https://api.github.com/users/nerdai/repos",
                "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
                "received_events_url": "https://api.github.com/users/nerdai/received_events",
                "type": "User",
                "site_admin": false
            }
        ],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-11-17T14:11:54Z",
        "updated_at": "2023-11-24T21:05:45Z",
        "closed_at": "2023-11-24T21:05:44Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nHi,\r\n\r\nCan you pls push the latest libs into conda-forge ? \r\nThanks\r\n\r\nhttps://anaconda.org/conda-forge/llama-index/files\n\n### Reason\n\nOld version is not working with openai=1.2.4\n\n### Value of Feature\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8979/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8979/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8978",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8978/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8978/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8978/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8978",
        "id": 1999122283,
        "node_id": "I_kwDOIWuq5853KC9r",
        "number": 8978,
        "title": "[Bug]: TokenCountingHandler reports incorrect completion token numbers with PydanticProgram",
        "user": {
            "login": "sapountzis",
            "id": 48756095,
            "node_id": "MDQ6VXNlcjQ4NzU2MDk1",
            "avatar_url": "https://avatars.githubusercontent.com/u/48756095?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sapountzis",
            "html_url": "https://github.com/sapountzis",
            "followers_url": "https://api.github.com/users/sapountzis/followers",
            "following_url": "https://api.github.com/users/sapountzis/following{/other_user}",
            "gists_url": "https://api.github.com/users/sapountzis/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sapountzis/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sapountzis/subscriptions",
            "organizations_url": "https://api.github.com/users/sapountzis/orgs",
            "repos_url": "https://api.github.com/users/sapountzis/repos",
            "events_url": "https://api.github.com/users/sapountzis/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sapountzis/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-17T13:22:41Z",
        "updated_at": "2023-11-17T16:47:16Z",
        "closed_at": "2023-11-17T16:47:16Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nAs the title says, the `TokenCountingHandler` reports clearly wrong values when used in pydantic program scenario (at least this is how I am using it)\r\n\r\nThe number it reports is usually 3 while the generation is significantly bigger.\r\n\r\n### Version\r\n\r\n0.9.2\r\n\r\n### Steps to Reproduce\r\n\r\n```python\r\nimport tiktoken\r\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\r\nfrom llama_index.llms import OpenAI\r\nfrom llama_index.program import OpenAIPydanticProgram\r\n\r\ntoken_counter = TokenCountingHandler(\r\n    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode,\r\n    verbose=True,\r\n)\r\ncallback_manager = CallbackManager([token_counter])\r\nllm = OpenAI(model=\"gpt-4\", temperature=0, callback_manager=callback_manager)\r\n\r\nprogram = OpenAIPydanticProgram.from_defaults(\r\n    output_cls=\"your_pydantic_class\",\r\n    prompt_template_str=\"your_prompt_template\",\r\n    llm=llm,\r\n    verbose=True,\r\n)\r\n\r\nresult = program(**kwargs)\r\n\r\nprint(token_counter.completion_llm_token_count)\r\n```\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8978/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8978/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8977",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8977/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8977/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8977/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8977",
        "id": 1999112509,
        "node_id": "I_kwDOIWuq5853KAk9",
        "number": 8977,
        "title": "Get another SQL wrapper than SQLAlchemy",
        "user": {
            "login": "tekntrash",
            "id": 102220086,
            "node_id": "U_kgDOBhfBNg",
            "avatar_url": "https://avatars.githubusercontent.com/u/102220086?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tekntrash",
            "html_url": "https://github.com/tekntrash",
            "followers_url": "https://api.github.com/users/tekntrash/followers",
            "following_url": "https://api.github.com/users/tekntrash/following{/other_user}",
            "gists_url": "https://api.github.com/users/tekntrash/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tekntrash/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tekntrash/subscriptions",
            "organizations_url": "https://api.github.com/users/tekntrash/orgs",
            "repos_url": "https://api.github.com/users/tekntrash/repos",
            "events_url": "https://api.github.com/users/tekntrash/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tekntrash/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-17T13:17:25Z",
        "updated_at": "2023-11-17T19:56:30Z",
        "closed_at": "2023-11-17T19:56:29Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nGet another SQL wrapper than SQLAlchemy\n\n### Reason\n\nSQLAlchemy sucks\n\n### Value of Feature\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8977/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8977/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8976",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8976/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8976/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8976/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8976",
        "id": 1999065592,
        "node_id": "I_kwDOIWuq5853J1H4",
        "number": 8976,
        "title": "Is there a way to connect to mysql without using sqlalchemy?",
        "user": {
            "login": "tekntrash",
            "id": 102220086,
            "node_id": "U_kgDOBhfBNg",
            "avatar_url": "https://avatars.githubusercontent.com/u/102220086?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tekntrash",
            "html_url": "https://github.com/tekntrash",
            "followers_url": "https://api.github.com/users/tekntrash/followers",
            "following_url": "https://api.github.com/users/tekntrash/following{/other_user}",
            "gists_url": "https://api.github.com/users/tekntrash/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tekntrash/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tekntrash/subscriptions",
            "organizations_url": "https://api.github.com/users/tekntrash/orgs",
            "repos_url": "https://api.github.com/users/tekntrash/repos",
            "events_url": "https://api.github.com/users/tekntrash/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tekntrash/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-17T12:53:41Z",
        "updated_at": "2023-11-17T13:00:07Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nsqlalchemy is really annoying and slow, so is there a way to connect to mysql without using sqlalchemy?\r\n\r\nI tried this \r\nimport mysql.connector as mariadb\r\ndb_connection = mariadb.connect(bunchofstuff)\r\ndb_cursor = db_connection.cursor(buffered=True)\r\nsql_database = SQLDatabase(db_cursor)\r\n\r\nbut I get the message\r\n_sqlalchemy.exc.NoInspectionAvailable: No inspection system is available for object of type <class 'mysql.connector.cursor.MySQLCursorBuffered'>_\r\n\r\nwhich clearly shows it is still trying to use sqlalchemy as backend",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8976/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8976/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8975",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8975/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8975/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8975/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8975",
        "id": 1998780808,
        "node_id": "I_kwDOIWuq5853IvmI",
        "number": 8975,
        "title": "[Bug]: OpenAI agent call gives API Connection Error when connecting with AzureOpenAI key",
        "user": {
            "login": "sapekshsuman",
            "id": 58976261,
            "node_id": "MDQ6VXNlcjU4OTc2MjYx",
            "avatar_url": "https://avatars.githubusercontent.com/u/58976261?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sapekshsuman",
            "html_url": "https://github.com/sapekshsuman",
            "followers_url": "https://api.github.com/users/sapekshsuman/followers",
            "following_url": "https://api.github.com/users/sapekshsuman/following{/other_user}",
            "gists_url": "https://api.github.com/users/sapekshsuman/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sapekshsuman/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sapekshsuman/subscriptions",
            "organizations_url": "https://api.github.com/users/sapekshsuman/orgs",
            "repos_url": "https://api.github.com/users/sapekshsuman/repos",
            "events_url": "https://api.github.com/users/sapekshsuman/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sapekshsuman/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 16,
        "created_at": "2023-11-17T10:28:04Z",
        "updated_at": "2023-11-21T16:52:43Z",
        "closed_at": "2023-11-21T16:52:42Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nI am trying to run the Multi Document Agent example provided in here using Azure OpenAI key - https://docs.llamaindex.ai/en/latest/examples/agent/multi_document_agents.html\r\n\r\nBut Top agent when called gives below error - \r\n<img width=\"735\" alt=\"image\" src=\"https://github.com/run-llama/llama_index/assets/58976261/eb9c01d5-f11e-4f59-87da-8287440d035e\">\r\n\r\nTop agent is defined as below - \r\n<img width=\"404\" alt=\"image\" src=\"https://github.com/run-llama/llama_index/assets/58976261/68050e36-c9ca-47d7-bf9d-f94a6966dd7b\">\r\n\r\nI also defined Top agent using openAIAgent but the same error comes up.\r\n\r\nFYI - Base agent query runs and produces the results as given in the example\r\n<img width=\"308\" alt=\"image\" src=\"https://github.com/run-llama/llama_index/assets/58976261/8973e25e-7728-4cb3-9e90-31a21276906b\">\r\n\r\nAlso, I have set global service context as below -  \r\n\r\nfrom llama_index import set_global_service_context\r\n\r\nset_global_service_context(service_context)\r\n\r\n### Version\r\n\r\n0.8.69.post1\r\n\r\n### Steps to Reproduce\r\n\r\nllm = AzureOpenAI(\r\n    model=\"gpt-35-turbo\",\r\n    deployment_name=\"<>\",\r\n    api_key=api_key,\r\n    azure_endpoint=azure_endpoint,\r\n    api_version=api_version,\r\n)\r\n\r\nembed_model = AzureOpenAIEmbedding(\r\n    model=\"text-embedding-ada-002\",\r\n    deployment_name=\"<>\",\r\n    api_key=api_key,\r\n    azure_endpoint=azure_endpoint,\r\n    api_version=api_version,\r\n)\r\n\r\nfrom llama_index import set_global_service_context\r\n\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=llm,\r\n    embed_model=embed_model,\r\n)\r\n\r\nset_global_service_context(service_context)\r\n\r\n###\r\n###Skipping between codes\r\n###\r\n\r\nfrom llama_index.agent import FnRetrieverOpenAIAgent\r\n\r\ntop_agent = FnRetrieverOpenAIAgent.from_retriever(\r\n    obj_index.as_retriever(similarity_top_k=3),\r\n    system_prompt=\"\"\" \\\r\nYou are an agent designed to answer queries about a set of given cities.\r\nPlease always use the tools provided to answer a question. Do not rely on prior knowledge.\\\r\n\r\n\"\"\",\r\n    verbose=True,\r\n)\r\n\r\nresponse = top_agent.query(\"Tell me about the arts and culture in Boston\")\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8975/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8975/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8974",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8974/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8974/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8974/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8974",
        "id": 1998670592,
        "node_id": "I_kwDOIWuq5853IUsA",
        "number": 8974,
        "title": "[Bug]: ",
        "user": {
            "login": "ShyamFaguna",
            "id": 52468359,
            "node_id": "MDQ6VXNlcjUyNDY4MzU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/52468359?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ShyamFaguna",
            "html_url": "https://github.com/ShyamFaguna",
            "followers_url": "https://api.github.com/users/ShyamFaguna/followers",
            "following_url": "https://api.github.com/users/ShyamFaguna/following{/other_user}",
            "gists_url": "https://api.github.com/users/ShyamFaguna/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ShyamFaguna/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ShyamFaguna/subscriptions",
            "organizations_url": "https://api.github.com/users/ShyamFaguna/orgs",
            "repos_url": "https://api.github.com/users/ShyamFaguna/repos",
            "events_url": "https://api.github.com/users/ShyamFaguna/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ShyamFaguna/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-17T09:27:41Z",
        "updated_at": "2023-11-17T15:17:19Z",
        "closed_at": "2023-11-17T15:17:19Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nHi,\r\nI'm having trouble importing the LangchainEmbedding, ServiceContext classes from the llama_index package. I've installed the package using pip, but I'm getting an ImportError when trying to import the classes, it was working fine till yesterday. Is there any known issue or workaround for this problem? I've followed the documentation, but I can't seem to resolve the ImportError.\r\n\r\nUsed :\r\npip install llama-index\r\nfrom llama_index import LangchainEmbedding, ServiceContext\r\n\r\nError:\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-12-cf78bfce5182> in <cell line: 3>()\r\n      1 import torch\r\n      2 from langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\n----> 3 from llama_index import LangchainEmbedding, ServiceContext\r\n      4 from llama_index.llms import HuggingFaceLLM\r\n      5 \r\n\r\nImportError: cannot import name 'LangchainEmbedding' from 'llama_index' (/usr/local/lib/python3.10/dist-packages/llama_index/__init__.py)\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\n\n### Version\n\n0.9.2\n\n### Steps to Reproduce\n\npip install llama-index\r\nfrom llama_index import LangchainEmbedding, ServiceContext\n\n### Relevant Logs/Tracbacks\n\n```shell\nImportError                               Traceback (most recent call last)\r\n<ipython-input-12-cf78bfce5182> in <cell line: 3>()\r\n      1 import torch\r\n      2 from langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\n----> 3 from llama_index import LangchainEmbedding, ServiceContext\r\n      4 from llama_index.llms import HuggingFaceLLM\r\n      5 \r\n\r\nImportError: cannot import name 'LangchainEmbedding' from 'llama_index' (/usr/local/lib/python3.10/dist-packages/llama_index/__init__.py)\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8974/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8974/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8973",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8973/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8973/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8973/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8973",
        "id": 1998262953,
        "node_id": "I_kwDOIWuq5853GxKp",
        "number": 8973,
        "title": "[Question]: Getting empty response while retrieving answer from pdfs",
        "user": {
            "login": "AvisP",
            "id": 10809866,
            "node_id": "MDQ6VXNlcjEwODA5ODY2",
            "avatar_url": "https://avatars.githubusercontent.com/u/10809866?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/AvisP",
            "html_url": "https://github.com/AvisP",
            "followers_url": "https://api.github.com/users/AvisP/followers",
            "following_url": "https://api.github.com/users/AvisP/following{/other_user}",
            "gists_url": "https://api.github.com/users/AvisP/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/AvisP/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/AvisP/subscriptions",
            "organizations_url": "https://api.github.com/users/AvisP/orgs",
            "repos_url": "https://api.github.com/users/AvisP/repos",
            "events_url": "https://api.github.com/users/AvisP/events{/privacy}",
            "received_events_url": "https://api.github.com/users/AvisP/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-17T04:36:04Z",
        "updated_at": "2023-11-17T17:03:34Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am getting an empty response with the following example developed based on sample demo code provided by llama_index documentation. I am getting an empty response to a simple question but the nodes have value in them. Any advise would be helpful.\r\n\r\n```python\r\nimport logging\r\nimport sys\r\nimport os\r\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\r\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\r\n\r\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext, load_index_from_storage\r\n\r\nimport torch\r\n\r\nfrom llama_index.llms import LlamaCPP\r\nfrom llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\r\nllm = LlamaCPP(\r\n    # You can pass in the URL to a GGML model to download it automatically\r\n    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q8_0.gguf',\r\n    # model_path = \"/Users/paula/Projects/Text Gen/Llama2/models/Mistral-7B-GGUF/mistral-7b-v0.1.Q8_0.gguf\",\r\n    temperature=0.1,\r\n    max_new_tokens=256,\r\n    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\r\n    context_window=3900,\r\n    # kwargs to pass to __call__()\r\n    generate_kwargs={},\r\n    # kwargs to pass to __init__()\r\n    # set to at least 1 to use GPU\r\n    model_kwargs={\"n_gpu_layers\": 2},\r\n    # transform inputs into Llama2 format\r\n    messages_to_prompt=messages_to_prompt,\r\n    completion_to_prompt=completion_to_prompt,\r\n    verbose=True,\r\n)\r\n\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\nfrom llama_index.embeddings import LangchainEmbedding\r\n# from llama_index.service_context import ServiceContext\r\n\r\nembed_model = LangchainEmbedding(\r\n  HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")\r\n)\r\n\r\nservice_context = ServiceContext.from_defaults(\r\n    chunk_size=256,\r\n    llm=llm,\r\n    embed_model=embed_model\r\n)\r\n\r\n# check if storage already exists\r\nif not os.path.exists(\"./storage\"):\r\n    # load the documents and create the index\r\n    documents = SimpleDirectoryReader(\"./PDFData/\").load_data()\r\n    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\r\n    # store it for later\r\n    index.storage_context.persist()\r\nelse:\r\n    # load the existing index\r\n    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\r\n    index = load_index_from_storage(storage_context, service_context=service_context)\r\n\r\n\r\nquery_engine = index.as_query_engine()\r\nresponse = query_engine.query(\"What is Fibromyalgia?\")\r\n\r\nprint(response)\r\n\r\nprint(response.source_nodes[0].text)\r\nprint(response.source_nodes[1].text)\r\n```\r\n\r\nPDF file can be found [here](https://www.versusarthritis.org/media/24901/fibromyalgia-information-booklet-july2021.pdf)",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8973/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8973/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8972",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8972/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8972/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8972/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8972",
        "id": 1998254692,
        "node_id": "I_kwDOIWuq5853GvJk",
        "number": 8972,
        "title": "[Question]: About multiple users chat slow",
        "user": {
            "login": "dinhan92",
            "id": 86275789,
            "node_id": "MDQ6VXNlcjg2Mjc1Nzg5",
            "avatar_url": "https://avatars.githubusercontent.com/u/86275789?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dinhan92",
            "html_url": "https://github.com/dinhan92",
            "followers_url": "https://api.github.com/users/dinhan92/followers",
            "following_url": "https://api.github.com/users/dinhan92/following{/other_user}",
            "gists_url": "https://api.github.com/users/dinhan92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dinhan92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dinhan92/subscriptions",
            "organizations_url": "https://api.github.com/users/dinhan92/orgs",
            "repos_url": "https://api.github.com/users/dinhan92/repos",
            "events_url": "https://api.github.com/users/dinhan92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dinhan92/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-11-17T04:25:44Z",
        "updated_at": "2023-11-22T17:58:56Z",
        "closed_at": "2023-11-22T17:58:56Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have try using llama index not stream and with flask and publish on windows. I tested 2 computers using postman. However I find out that when I only test 1 computer, it response in 1 minute. However if with 2 computers, it response in 2 minutes.\r\n\r\nSo, do I have to change the system to Linux to fix this, or is there anything that I missed in llama index docs?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8972/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8972/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8971",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8971/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8971/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8971/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8971",
        "id": 1998251521,
        "node_id": "I_kwDOIWuq5853GuYB",
        "number": 8971,
        "title": "[Question]: Why knowledgeGraphIndex return only english KG?",
        "user": {
            "login": "JinSeoung-Oh",
            "id": 78573459,
            "node_id": "MDQ6VXNlcjc4NTczNDU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/78573459?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/JinSeoung-Oh",
            "html_url": "https://github.com/JinSeoung-Oh",
            "followers_url": "https://api.github.com/users/JinSeoung-Oh/followers",
            "following_url": "https://api.github.com/users/JinSeoung-Oh/following{/other_user}",
            "gists_url": "https://api.github.com/users/JinSeoung-Oh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/JinSeoung-Oh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/JinSeoung-Oh/subscriptions",
            "organizations_url": "https://api.github.com/users/JinSeoung-Oh/orgs",
            "repos_url": "https://api.github.com/users/JinSeoung-Oh/repos",
            "events_url": "https://api.github.com/users/JinSeoung-Oh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/JinSeoung-Oh/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 8,
        "created_at": "2023-11-17T04:22:58Z",
        "updated_at": "2023-11-17T07:24:37Z",
        "closed_at": "2023-11-17T07:08:02Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI'm building GraphRAG with nebular graph.\r\nI made a module for extracting triplet from korean chunk data\r\nAnd pass it KnowledgeGraphIndex, and it return english KG not korean KG.\r\nI don't want translate my korean triplet. I just want to make korean KG.\r\n\r\nI searched NebularGraph just only support English, but I cannot find out.\r\nBasically, NebularGarph Query is English(at least, keyword)\r\nSo, maybe, because of this my korean triplet translate to english\r\n\r\nBut I can not find out where my korean triplt translate to english\r\nSo, please let me know this part and what LLM used for this.\r\n\r\nThanks!",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8971/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8971/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8970",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8970/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8970/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8970/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8970",
        "id": 1998075040,
        "node_id": "PR_kwDOIWuq585fsgaw",
        "number": 8970,
        "title": "Add input_type to VoyageEmbedding",
        "user": {
            "login": "thomas0809",
            "id": 11373553,
            "node_id": "MDQ6VXNlcjExMzczNTUz",
            "avatar_url": "https://avatars.githubusercontent.com/u/11373553?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thomas0809",
            "html_url": "https://github.com/thomas0809",
            "followers_url": "https://api.github.com/users/thomas0809/followers",
            "following_url": "https://api.github.com/users/thomas0809/following{/other_user}",
            "gists_url": "https://api.github.com/users/thomas0809/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/thomas0809/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/thomas0809/subscriptions",
            "organizations_url": "https://api.github.com/users/thomas0809/orgs",
            "repos_url": "https://api.github.com/users/thomas0809/repos",
            "events_url": "https://api.github.com/users/thomas0809/events{/privacy}",
            "received_events_url": "https://api.github.com/users/thomas0809/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-17T01:22:06Z",
        "updated_at": "2023-11-17T21:50:23Z",
        "closed_at": "2023-11-17T01:45:15Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8970",
            "html_url": "https://github.com/run-llama/llama_index/pull/8970",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8970.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8970.patch",
            "merged_at": "2023-11-17T01:45:15Z"
        },
        "body": "# Description\r\n\r\nAdd input_type to VoyageEmbedding\r\n\r\n\r\n## Type of Change\r\n\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8970/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8970/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8969",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8969/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8969/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8969/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8969",
        "id": 1997993708,
        "node_id": "I_kwDOIWuq5853Fvbs",
        "number": 8969,
        "title": "[Bug]: The starter tutorial failed with \"ModuleNotFoundError: No module named 'jinja2'\"",
        "user": {
            "login": "eoinoreilly30",
            "id": 33495030,
            "node_id": "MDQ6VXNlcjMzNDk1MDMw",
            "avatar_url": "https://avatars.githubusercontent.com/u/33495030?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/eoinoreilly30",
            "html_url": "https://github.com/eoinoreilly30",
            "followers_url": "https://api.github.com/users/eoinoreilly30/followers",
            "following_url": "https://api.github.com/users/eoinoreilly30/following{/other_user}",
            "gists_url": "https://api.github.com/users/eoinoreilly30/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/eoinoreilly30/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/eoinoreilly30/subscriptions",
            "organizations_url": "https://api.github.com/users/eoinoreilly30/orgs",
            "repos_url": "https://api.github.com/users/eoinoreilly30/repos",
            "events_url": "https://api.github.com/users/eoinoreilly30/events{/privacy}",
            "received_events_url": "https://api.github.com/users/eoinoreilly30/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-16T23:54:51Z",
        "updated_at": "2023-11-19T16:14:00Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nCreated a venv, ran pip install llama-index.\r\n\r\nFollowed the steps in the starter tutorial: https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html\r\n\r\nRan the python script and it failed with \"ModuleNotFoundError: No module named 'jinja2'\"\r\n\r\npip install jinja2 fixed it.\r\n\r\n\n\n### Version\n\n0.9.2\n\n### Steps to Reproduce\n\npython  -m venv venv\r\n. venv/bin/activate\r\npip install llama-index\r\n\r\nmain.py:\r\n`\r\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\r\n\r\nprint(\"Loading data...\")\r\ndocuments = SimpleDirectoryReader(\"data\").load_data()\r\n\r\nprint(\"Indexing...\")\r\nindex = VectorStoreIndex.from_documents(documents)\r\n\r\nquery_engine = index.as_query_engine()\r\n\r\nprint(\"Querying...\")\r\nresponse = query_engine.query(\"What did the author do growing up?\")\r\nprint(response)\r\n`\r\n\r\npython main.py\n\n### Relevant Logs/Tracbacks\n\n```shell\nTraceback (most recent call last):\r\n  File \"/Users/github/llm/venv/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 132, in import_optional_dependency\r\n    module = importlib.import_module(name)\r\n  File \"/Users/.pyenv/versions/3.10.12/lib/python3.10/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'jinja2'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/github/llm/main.py\", line 9, in <module>\r\n    query_engine = index.as_query_engine()\r\n  File \"/Users/github/llm/venv/lib/python3.10/site-packages/llama_index/indices/base.py\", line 344, in as_query_engine\r\n    from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\r\n  File \"/Users/github/llm/venv/lib/python3.10/site-packages/llama_index/query_engine/__init__.py\", line 21, in <module>\r\n    from llama_index.query_engine.retry_query_engine import (\r\n  File \"/Users/github/llm/venv/lib/python3.10/site-packages/llama_index/query_engine/retry_query_engine.py\", line 6, in <module>\r\n    from llama_index.evaluation.base import BaseEvaluator\r\n  File \"/Users/github/llm/venv/lib/python3.10/site-packages/llama_index/evaluation/__init__.py\", line 15, in <module>\r\n    from llama_index.evaluation.notebook_utils import get_retrieval_results_df\r\n  File \"/Users/github/llm/venv/lib/python3.10/site-packages/llama_index/evaluation/notebook_utils.py\", line 7, in <module>\r\n    from pandas.io.formats.style import Styler\r\n  File \"/Users/github/llm/venv/lib/python3.10/site-packages/pandas/io/formats/style.py\", line 44, in <module>\r\n    jinja2 = import_optional_dependency(\"jinja2\", extra=\"DataFrame.style requires jinja2.\")\r\n  File \"/Users/github/llm/venv/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\r\n    raise ImportError(msg)\r\nImportError: Missing optional dependency 'Jinja2'. DataFrame.style requires jinja2. Use pip or conda to install Jinja2.\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8969/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8969/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8968",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8968/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8968/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8968/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8968",
        "id": 1997983283,
        "node_id": "I_kwDOIWuq5853Fs4z",
        "number": 8968,
        "title": "[Question]: Getting encoding error when initializing VectorStoreIndex",
        "user": {
            "login": "zimaRed",
            "id": 14878610,
            "node_id": "MDQ6VXNlcjE0ODc4NjEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/14878610?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zimaRed",
            "html_url": "https://github.com/zimaRed",
            "followers_url": "https://api.github.com/users/zimaRed/followers",
            "following_url": "https://api.github.com/users/zimaRed/following{/other_user}",
            "gists_url": "https://api.github.com/users/zimaRed/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zimaRed/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zimaRed/subscriptions",
            "organizations_url": "https://api.github.com/users/zimaRed/orgs",
            "repos_url": "https://api.github.com/users/zimaRed/repos",
            "events_url": "https://api.github.com/users/zimaRed/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zimaRed/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-16T23:44:10Z",
        "updated_at": "2023-11-17T01:18:14Z",
        "closed_at": "2023-11-17T01:18:14Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHello,\r\n\r\nI am using Weavite as my vector store and attempting to create a new VectorStoreIndex. I'm getting the error \"TypeError: Object of type datetime is not JSON serializable\" at this line of code:\r\n\r\n`index = VectorStoreIndex(nodes, storage_context = storage_context) `\r\n\r\nHere's my full code:\r\n\r\n```\r\n    import weaviate\r\n    from llama_index.vector_stores import WeaviateVectorStore\r\n    from llama_index import VectorStoreIndex, StorageContext\r\n    from llama_index.node_parser import SimpleNodeParser\r\n\r\n    auth_config = weaviate.AuthApiKey(api_key=\"<MY API KEY>\")\r\n    client = weaviate.Client(\r\n      url=\"<MY CLUSTER URL>\",\r\n      auth_client_secret=auth_config\r\n    )\r\n\r\n    def create_index(uid, documents):\r\n    \r\n        vector_store = WeaviateVectorStore(weaviate_client=client)\r\n    \r\n        storage_context = StorageContext.from_defaults(vector_store = vector_store)\r\n    \r\n        # documents to nodes\r\n        parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\r\n        nodes = parser.get_nodes_from_documents(documents)\r\n    \r\n        index = VectorStoreIndex(nodes, storage_context = storage_context) \r\n```\r\n\r\nI'm new to both LlamaIndex and Weaviate, so I may be missing something obvious here. Thanks in advance for you help.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8968/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8968/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8967",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8967/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8967/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8967/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8967",
        "id": 1997974500,
        "node_id": "I_kwDOIWuq5853Fqvk",
        "number": 8967,
        "title": "[Bug]: 'NodeWithEmbedding' object has no attribute 'metadata'",
        "user": {
            "login": "ignaciocolussi",
            "id": 72051072,
            "node_id": "MDQ6VXNlcjcyMDUxMDcy",
            "avatar_url": "https://avatars.githubusercontent.com/u/72051072?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ignaciocolussi",
            "html_url": "https://github.com/ignaciocolussi",
            "followers_url": "https://api.github.com/users/ignaciocolussi/followers",
            "following_url": "https://api.github.com/users/ignaciocolussi/following{/other_user}",
            "gists_url": "https://api.github.com/users/ignaciocolussi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ignaciocolussi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ignaciocolussi/subscriptions",
            "organizations_url": "https://api.github.com/users/ignaciocolussi/orgs",
            "repos_url": "https://api.github.com/users/ignaciocolussi/repos",
            "events_url": "https://api.github.com/users/ignaciocolussi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ignaciocolussi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-16T23:33:48Z",
        "updated_at": "2023-11-17T22:02:06Z",
        "closed_at": "2023-11-17T22:02:06Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nWhile following the guide to use the CosmosDB mongodb vcore in [here](https://docs.llamaindex.ai/en/stable/examples/vector_stores/AzureCosmosDBMongoDBvCoreDemo.html) the index generation step keep crashing saying 'NodeWithEmbedding' object has no attribute 'metadata' in this step \"index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\"\r\n\r\n\r\n### Version\r\n\r\n0.9.2\r\n\r\n### Steps to Reproduce\r\n\r\nFollow the guide in [here](https://docs.llamaindex.ai/en/stable/examples/vector_stores/AzureCosmosDBMongoDBvCoreDemo.html)\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n```shell\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/tmp/ipykernel_5661/3716501382.py in <module>\r\n----> 1 index = VectorStoreIndex.from_documents(\r\n      2     documents, storage_context=storage_context\r\n      3 )\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/indices/base.py in from_documents(cls, documents, storage_context, service_context, show_progress, **kwargs)\r\n     98 \r\n     99             nodes = run_transformations(\r\n--> 100                 documents,  # type: ignore\r\n    101                 service_context.transformations,\r\n    102                 show_progress=show_progress,\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py in __init__(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, show_progress, **kwargs)\r\n     44         **kwargs: Any,\r\n     45     ) -> None:\r\n---> 46         \"\"\"Initialize params.\"\"\"\r\n     47         self._use_async = use_async\r\n     48         self._store_nodes_override = store_nodes_override\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/indices/base.py in __init__(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\r\n     67 \r\n     68         with self._service_context.callback_manager.as_trace(\"index_construction\"):\r\n---> 69             if index_struct is None:\r\n     70                 assert nodes is not None\r\n     71                 index_struct = self.build_index_from_nodes(nodes)\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py in build_index_from_nodes(self, nodes)\r\n    239                 **insert_kwargs,\r\n    240             )\r\n--> 241         return index_struct\r\n    242 \r\n    243     def build_index_from_nodes(\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py in _build_index_from_nodes(self, nodes)\r\n    227                     index_struct,\r\n    228                     nodes,\r\n--> 229                     show_progress=self._show_progress,\r\n    230                     **insert_kwargs,\r\n    231                 )\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py in _add_nodes_to_index(self, index_struct, nodes, show_progress)\r\n    200                 self._docstore.add_documents(\r\n    201                     [node_without_embedding], allow_update=True\r\n--> 202                 )\r\n    203         else:\r\n    204             # NOTE: if the vector store keeps text,\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/vector_stores/azurecosmosmongo.py in add(self, nodes, **add_kwargs)\r\n    138         data_to_insert = []\r\n    139         for node in nodes:\r\n--> 140             metadata = node_to_metadata_dict(\r\n    141                 node, remove_text=True, flat_metadata=self.flat_metadata\r\n    142             )\r\n\r\n~/.local/lib/python3.10/site-packages/llama_index/vector_stores/utils.py in node_to_metadata_dict(node, remove_text, text_field, flat_metadata)\r\n     28                 f\"Value for metadata {key} must be one of (str, int, float, None)\"\r\n     29             )\r\n---> 30 \r\n     31 \r\n     32 def node_to_metadata_dict(\r\n\r\nAttributeError: 'NodeWithEmbedding' object has no attribute 'metadata'\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8967/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8967/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8966",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8966/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8966/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8966/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8966",
        "id": 1997779717,
        "node_id": "PR_kwDOIWuq585frgHF",
        "number": 8966,
        "title": "send nodes to callback",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-16T21:03:58Z",
        "updated_at": "2023-11-16T21:09:55Z",
        "closed_at": "2023-11-16T21:09:54Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8966",
            "html_url": "https://github.com/run-llama/llama_index/pull/8966",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8966.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8966.patch",
            "merged_at": "2023-11-16T21:09:54Z"
        },
        "body": "Small fix for router retriever callbacks\r\n\r\nFixes #8576 ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8966/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8966/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8965",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8965/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8965/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8965/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8965",
        "id": 1997719866,
        "node_id": "PR_kwDOIWuq585frSsP",
        "number": 8965,
        "title": "Update MM RAG Eval NB to use `is_image_to_text` flag",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-16T20:29:26Z",
        "updated_at": "2023-11-16T20:36:49Z",
        "closed_at": "2023-11-16T20:36:03Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8965",
            "html_url": "https://github.com/run-llama/llama_index/pull/8965",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8965.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8965.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nThis should have been included in the last PR #8945 but I guess I didn't add/stage these last final, minor changes to the nb.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] minor, non-breaking update to a nb guide\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8965/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8965/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8964",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8964/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8964/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8964/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8964",
        "id": 1997714740,
        "node_id": "PR_kwDOIWuq585frRkS",
        "number": 8964,
        "title": "Changed import path from honeyhive.sdk to honeyhive.utils",
        "user": {
            "login": "michael-hhai",
            "id": 138688447,
            "node_id": "U_kgDOCEQ3vw",
            "avatar_url": "https://avatars.githubusercontent.com/u/138688447?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/michael-hhai",
            "html_url": "https://github.com/michael-hhai",
            "followers_url": "https://api.github.com/users/michael-hhai/followers",
            "following_url": "https://api.github.com/users/michael-hhai/following{/other_user}",
            "gists_url": "https://api.github.com/users/michael-hhai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/michael-hhai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/michael-hhai/subscriptions",
            "organizations_url": "https://api.github.com/users/michael-hhai/orgs",
            "repos_url": "https://api.github.com/users/michael-hhai/repos",
            "events_url": "https://api.github.com/users/michael-hhai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/michael-hhai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-16T20:25:47Z",
        "updated_at": "2023-11-16T20:33:24Z",
        "closed_at": "2023-11-16T20:31:42Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8964",
            "html_url": "https://github.com/run-llama/llama_index/pull/8964",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8964.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8964.patch",
            "merged_at": "2023-11-16T20:31:42Z"
        },
        "body": "# Description\r\n\r\nThe import path of the HoneyHive LlamaIndex tracer changed from `honeyhive.sdk` to `honeyhive.utils`. Just making appropriate changes to reflect that.\r\n\r\n## Type of Change\r\n\r\n- [X] Bug fix (non-breaking change which fixes an issue)\r\n- [X] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nI stared at the code and made sure it makes sense.\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [X] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8964/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8964/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8963",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8963/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8963/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8963/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8963",
        "id": 1997654571,
        "node_id": "PR_kwDOIWuq585frEDF",
        "number": 8963,
        "title": "[version] bump version to 0.9.2",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-16T19:54:10Z",
        "updated_at": "2023-11-16T20:07:39Z",
        "closed_at": "2023-11-16T20:07:38Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8963",
            "html_url": "https://github.com/run-llama/llama_index/pull/8963",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8963.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8963.patch",
            "merged_at": "2023-11-16T20:07:38Z"
        },
        "body": "# Description\r\n\r\nversion bump to 0.9.2\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8963/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8963/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8962",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8962/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8962/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8962/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8962",
        "id": 1997598693,
        "node_id": "PR_kwDOIWuq585fq3iW",
        "number": 8962,
        "title": "Update postgres.py",
        "user": {
            "login": "bogdanalexandrutreica",
            "id": 78951696,
            "node_id": "MDQ6VXNlcjc4OTUxNjk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/78951696?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bogdanalexandrutreica",
            "html_url": "https://github.com/bogdanalexandrutreica",
            "followers_url": "https://api.github.com/users/bogdanalexandrutreica/followers",
            "following_url": "https://api.github.com/users/bogdanalexandrutreica/following{/other_user}",
            "gists_url": "https://api.github.com/users/bogdanalexandrutreica/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/bogdanalexandrutreica/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/bogdanalexandrutreica/subscriptions",
            "organizations_url": "https://api.github.com/users/bogdanalexandrutreica/orgs",
            "repos_url": "https://api.github.com/users/bogdanalexandrutreica/repos",
            "events_url": "https://api.github.com/users/bogdanalexandrutreica/events{/privacy}",
            "received_events_url": "https://api.github.com/users/bogdanalexandrutreica/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-16T19:26:43Z",
        "updated_at": "2023-11-17T17:07:38Z",
        "closed_at": "2023-11-16T19:53:24Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8962",
            "html_url": "https://github.com/run-llama/llama_index/pull/8962",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8962.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8962.patch",
            "merged_at": "2023-11-16T19:53:24Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8962/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8962/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8961",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8961/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8961/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8961/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8961",
        "id": 1997577427,
        "node_id": "I_kwDOIWuq5853EJzT",
        "number": 8961,
        "title": "[Bug]: postgres use_jsonb",
        "user": {
            "login": "bogdanalexandrutreica",
            "id": 78951696,
            "node_id": "MDQ6VXNlcjc4OTUxNjk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/78951696?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bogdanalexandrutreica",
            "html_url": "https://github.com/bogdanalexandrutreica",
            "followers_url": "https://api.github.com/users/bogdanalexandrutreica/followers",
            "following_url": "https://api.github.com/users/bogdanalexandrutreica/following{/other_user}",
            "gists_url": "https://api.github.com/users/bogdanalexandrutreica/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/bogdanalexandrutreica/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/bogdanalexandrutreica/subscriptions",
            "organizations_url": "https://api.github.com/users/bogdanalexandrutreica/orgs",
            "repos_url": "https://api.github.com/users/bogdanalexandrutreica/repos",
            "events_url": "https://api.github.com/users/bogdanalexandrutreica/events{/privacy}",
            "received_events_url": "https://api.github.com/users/bogdanalexandrutreica/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-16T19:17:20Z",
        "updated_at": "2023-11-21T06:46:22Z",
        "closed_at": "2023-11-21T06:46:22Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nPostgresq vectore store is not working after use_jsonb PR\r\n\r\nLocally I solved the problem by adding\r\n\r\nuse_jsonb=use_jsonb\r\n\r\non line 188 in postgres.py\r\n\n\n### Version\n\n0.9.1\n\n### Steps to Reproduce\n\nCreate a vector store with pgvector\n\n### Relevant Logs/Tracbacks\n\n```shell\nhttps://github.com/run-llama/llama_index/pull/8910\r\n\r\nas it is mentioned in this PR by sunnybak\r\n\r\n\r\nValidationError: 1 validation error for PGVectorStore\r\nuse_jsonb\r\nfield required (type=value_error.missing)\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8961/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8961/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8960",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8960/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8960/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8960/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8960",
        "id": 1997359065,
        "node_id": "PR_kwDOIWuq585fqCcU",
        "number": 8960,
        "title": "Replace plain pydantic imports with compat layer",
        "user": {
            "login": "martinkozle",
            "id": 48385621,
            "node_id": "MDQ6VXNlcjQ4Mzg1NjIx",
            "avatar_url": "https://avatars.githubusercontent.com/u/48385621?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martinkozle",
            "html_url": "https://github.com/martinkozle",
            "followers_url": "https://api.github.com/users/martinkozle/followers",
            "following_url": "https://api.github.com/users/martinkozle/following{/other_user}",
            "gists_url": "https://api.github.com/users/martinkozle/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martinkozle/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martinkozle/subscriptions",
            "organizations_url": "https://api.github.com/users/martinkozle/orgs",
            "repos_url": "https://api.github.com/users/martinkozle/repos",
            "events_url": "https://api.github.com/users/martinkozle/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martinkozle/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-16T17:14:50Z",
        "updated_at": "2023-11-16T21:09:17Z",
        "closed_at": "2023-11-16T19:32:04Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8960",
            "html_url": "https://github.com/run-llama/llama_index/pull/8960",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8960.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8960.patch",
            "merged_at": "2023-11-16T19:32:04Z"
        },
        "body": "# Description\r\n\r\nFixes #8959\r\n\r\nI had to add an inline type ignore for a test. The code works at runtime, it passes pytest. I think for the type error to not show up the mypy Pydantic plugin needs to be installed, but I am guessing that is future work, so that's why it is a temporary inline type ignored for now.\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] I ran mypy\r\n- [x] I ran pytest\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8960/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8960/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8959",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8959/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8959/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8959/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8959",
        "id": 1997340837,
        "node_id": "I_kwDOIWuq5853DQCl",
        "number": 8959,
        "title": "[Bug]: Pydantic compatibility layer is not used everywhere in the codebase",
        "user": {
            "login": "martinkozle",
            "id": 48385621,
            "node_id": "MDQ6VXNlcjQ4Mzg1NjIx",
            "avatar_url": "https://avatars.githubusercontent.com/u/48385621?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/martinkozle",
            "html_url": "https://github.com/martinkozle",
            "followers_url": "https://api.github.com/users/martinkozle/followers",
            "following_url": "https://api.github.com/users/martinkozle/following{/other_user}",
            "gists_url": "https://api.github.com/users/martinkozle/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/martinkozle/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/martinkozle/subscriptions",
            "organizations_url": "https://api.github.com/users/martinkozle/orgs",
            "repos_url": "https://api.github.com/users/martinkozle/repos",
            "events_url": "https://api.github.com/users/martinkozle/events{/privacy}",
            "received_events_url": "https://api.github.com/users/martinkozle/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-16T17:09:12Z",
        "updated_at": "2023-11-16T19:32:05Z",
        "closed_at": "2023-11-16T19:32:05Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nIf I understand correctly the compatibility bridge `from llama_index.bridge.pydantic import ...` should be used everywhere in the code for Pydantic 2 compatibility, but there are 8 places in the repository where `from pydantic import ...` is used. This causes some parts of the library to not work at all with Pydantic 2. One such case that I came across was using `CustomQueryEngine`.\r\nI am aware that according to #8221 LlamaIndex will fully migrate to Pydantic v2 soon, but this should be temporarily fixed in the meantime.\n\n### Version\n\n0.9.1\n\n### Steps to Reproduce\n\n```console\r\n$ grep -r --exclude='*' --include='*.py' --exclude='bridge/pydantic.py' '^from pydantic' .\r\n./tests/indices/response/test_tree_summarize.py:from pydantic import BaseModel\r\n./llama_index/param_tuner/base.py:from pydantic import BaseModel, Field\r\n./llama_index/evaluation/dataset_generation.py:from pydantic import BaseModel, Field\r\n./llama_index/evaluation/retrieval/metrics_base.py:from pydantic import BaseModel, Field\r\n./llama_index/query_engine/custom.py:from pydantic import BaseModel\r\n./llama_index/node_parser/relational/unstructured_element.py:from pydantic import BaseModel, ValidationError\r\n./llama_index/prompts/base.py:from pydantic import Field\r\n./llama_index/finetuning/rerankers/dataset_gen.py:from pydantic import BaseMode\r\n```\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8959/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8959/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8958",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8958/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8958/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8958/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8958",
        "id": 1996935983,
        "node_id": "I_kwDOIWuq5853BtMv",
        "number": 8958,
        "title": "[Question]: Changing the host / baseURL",
        "user": {
            "login": "Danielg212",
            "id": 24668564,
            "node_id": "MDQ6VXNlcjI0NjY4NTY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/24668564?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Danielg212",
            "html_url": "https://github.com/Danielg212",
            "followers_url": "https://api.github.com/users/Danielg212/followers",
            "following_url": "https://api.github.com/users/Danielg212/following{/other_user}",
            "gists_url": "https://api.github.com/users/Danielg212/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Danielg212/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Danielg212/subscriptions",
            "organizations_url": "https://api.github.com/users/Danielg212/orgs",
            "repos_url": "https://api.github.com/users/Danielg212/repos",
            "events_url": "https://api.github.com/users/Danielg212/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Danielg212/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-11-16T14:05:54Z",
        "updated_at": "2023-11-16T14:39:11Z",
        "closed_at": "2023-11-16T14:30:44Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHow can I change the host to our internal gpt proxy server? I don't see this option in the class.\r\nWhat am I missing? ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8958/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8958/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8957",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8957/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8957/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8957/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8957",
        "id": 1996907928,
        "node_id": "I_kwDOIWuq5853BmWY",
        "number": 8957,
        "title": "[Question]: The speed of the Llama2 model wrapped with HuggingFaceLLM",
        "user": {
            "login": "Worien",
            "id": 11439011,
            "node_id": "MDQ6VXNlcjExNDM5MDEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/11439011?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Worien",
            "html_url": "https://github.com/Worien",
            "followers_url": "https://api.github.com/users/Worien/followers",
            "following_url": "https://api.github.com/users/Worien/following{/other_user}",
            "gists_url": "https://api.github.com/users/Worien/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Worien/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Worien/subscriptions",
            "organizations_url": "https://api.github.com/users/Worien/orgs",
            "repos_url": "https://api.github.com/users/Worien/repos",
            "events_url": "https://api.github.com/users/Worien/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Worien/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2023-11-16T13:53:18Z",
        "updated_at": "2023-11-16T18:58:50Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nHello, I'm trying to run llama_index with llama2 7b model. For now, I have only a few PDF documents, each of which contains a 1-2 pages of text. I'm trying to run it on a pretty powerful GPU (each 50 GB), but I faced the problem that each very simple question takes 30-80 seconds to get a response. I was able to make it closer to 30 by reducing chunk_size to 128, but the pure Llama2 model generated a response to the same question for 12 seconds. How can I speed up the query_engine?\r\n\r\n`\r\nimport logging\r\nimport sys\r\nimport os\r\n\r\nfrom src.constants import LLAMA_INDEX_PATH, PERSIST_INDEX_PATH\r\n\r\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\r\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\r\n\r\nimport torch\r\nfrom llama_index.llms import HuggingFaceLLM\r\nfrom llama_index.prompts import PromptTemplate\r\nfrom llama_index import SimpleDirectoryReader\r\nfrom llama_index import (\r\n    VectorStoreIndex,\r\n    ServiceContext,\r\n    set_global_service_context,\r\n)\r\n\r\nfrom huggingface_hub.commands.user import login\r\nfrom llama_index.node_parser import SimpleNodeParser\r\n\r\n\r\nlogin(token=\"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\")\r\n\r\nLLAMA2_7B = \"meta-llama/Llama-2-7b-hf\"\r\nLLAMA2_7B_CHAT = \"meta-llama/Llama-2-7b-chat-hf\"\r\nLLAMA2_13B = \"meta-llama/Llama-2-13b-hf\"\r\nLLAMA2_13B_CHAT = \"meta-llama/Llama-2-13b-chat-hf\"\r\nLLAMA2_70B = \"meta-llama/Llama-2-70b-hf\"\r\nLLAMA2_70B_CHAT = \"meta-llama/Llama-2-70b-chat-hf\"\r\n\r\nselected_model = LLAMA2_13B_CHAT\r\n\r\nSYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\r\n- Generate human readable output, avoid creating output with gibberish text.\r\n- Generate only the requested output, don't include any other language before or after the requested output.\r\n- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\r\n- Generate professional language typically used in business documents in North America.\r\n- Never generate offensive or foul language.\r\n\"\"\"\r\nSYSTEM_PROMPT2 = \"You are an AI assistant that answers questions in a friendly manner\"\r\nquery_wrapper_prompt = PromptTemplate(\r\n    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\r\n)\r\n\r\nllm = HuggingFaceLLM(\r\n    context_window=4096,\r\n    max_new_tokens=2048,\r\n    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\r\n    query_wrapper_prompt=query_wrapper_prompt,\r\n    tokenizer_name=selected_model,\r\n    model_name=selected_model,\r\n    device_map=\"cuda\",\r\n    # change these settings below depending on your GPU\r\n    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True,},\r\n)\r\ndocuments = SimpleDirectoryReader(\r\n    LLAMA_INDEX_PATH\r\n).load_data()\r\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=128, chunk_overlap=5)\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=llm, embed_model=\"local:BAAI/bge-small-en\", node_parser=node_parser\r\n)\r\nset_global_service_context(service_context)\r\n\r\nindex = VectorStoreIndex.from_documents(documents)\r\nquery_engine = index.as_query_engine()\r\nresponse = query_engine.query(\"How to setup an app?\")\r\nindex.storage_context.persist(persist_dir=PERSIST_INDEX_PATH)\r\nprint('respnse')\r\nprint(response)\r\n\r\n\r\ndef ask(question: str) -> str:\r\n      resp = query_engine.query(question)\r\n      return resp.response\r\n\r\n`",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8957/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8957/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8956",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8956/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8956/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8956/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8956",
        "id": 1996879800,
        "node_id": "PR_kwDOIWuq585foYGS",
        "number": 8956,
        "title": "fix: Replace dict with ChoiceDelta",
        "user": {
            "login": "joelbarmettlerUZH",
            "id": 24369532,
            "node_id": "MDQ6VXNlcjI0MzY5NTMy",
            "avatar_url": "https://avatars.githubusercontent.com/u/24369532?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/joelbarmettlerUZH",
            "html_url": "https://github.com/joelbarmettlerUZH",
            "followers_url": "https://api.github.com/users/joelbarmettlerUZH/followers",
            "following_url": "https://api.github.com/users/joelbarmettlerUZH/following{/other_user}",
            "gists_url": "https://api.github.com/users/joelbarmettlerUZH/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/joelbarmettlerUZH/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/joelbarmettlerUZH/subscriptions",
            "organizations_url": "https://api.github.com/users/joelbarmettlerUZH/orgs",
            "repos_url": "https://api.github.com/users/joelbarmettlerUZH/repos",
            "events_url": "https://api.github.com/users/joelbarmettlerUZH/events{/privacy}",
            "received_events_url": "https://api.github.com/users/joelbarmettlerUZH/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-16T13:40:06Z",
        "updated_at": "2023-11-16T16:38:10Z",
        "closed_at": "2023-11-16T16:38:10Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8956",
            "html_url": "https://github.com/run-llama/llama_index/pull/8956",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8956.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8956.patch",
            "merged_at": "2023-11-16T16:38:10Z"
        },
        "body": "# Description\r\n\r\nAzure OpenAI could not be used due to a bug where the delta object was initialized to be an empty dictionary instead of an empty ChoiceDelta. Quick fix. \r\n\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8956/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8956/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8955",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8955/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8955/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8955/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8955",
        "id": 1996860318,
        "node_id": "PR_kwDOIWuq585foTzo",
        "number": 8955,
        "title": "fix: AzureOpenAI pydantic object",
        "user": {
            "login": "joelbarmettlerUZH",
            "id": 24369532,
            "node_id": "MDQ6VXNlcjI0MzY5NTMy",
            "avatar_url": "https://avatars.githubusercontent.com/u/24369532?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/joelbarmettlerUZH",
            "html_url": "https://github.com/joelbarmettlerUZH",
            "followers_url": "https://api.github.com/users/joelbarmettlerUZH/followers",
            "following_url": "https://api.github.com/users/joelbarmettlerUZH/following{/other_user}",
            "gists_url": "https://api.github.com/users/joelbarmettlerUZH/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/joelbarmettlerUZH/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/joelbarmettlerUZH/subscriptions",
            "organizations_url": "https://api.github.com/users/joelbarmettlerUZH/orgs",
            "repos_url": "https://api.github.com/users/joelbarmettlerUZH/repos",
            "events_url": "https://api.github.com/users/joelbarmettlerUZH/events{/privacy}",
            "received_events_url": "https://api.github.com/users/joelbarmettlerUZH/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-16T13:29:14Z",
        "updated_at": "2023-11-16T15:39:29Z",
        "closed_at": "2023-11-16T15:39:28Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8955",
            "html_url": "https://github.com/run-llama/llama_index/pull/8955",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8955.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8955.patch",
            "merged_at": "2023-11-16T15:39:28Z"
        },
        "body": "# Description\r\n\r\nExtend pydantic field definition for AzureOpenAI class.\r\n\r\nFixes #8944\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8955/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8955/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8954",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8954/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8954/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8954/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8954",
        "id": 1996598157,
        "node_id": "I_kwDOIWuq5853AauN",
        "number": 8954,
        "title": "[Bug]: OpensearchVectorStore is not BasePydanticVectorStore",
        "user": {
            "login": "andrePankraz",
            "id": 2312884,
            "node_id": "MDQ6VXNlcjIzMTI4ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2312884?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/andrePankraz",
            "html_url": "https://github.com/andrePankraz",
            "followers_url": "https://api.github.com/users/andrePankraz/followers",
            "following_url": "https://api.github.com/users/andrePankraz/following{/other_user}",
            "gists_url": "https://api.github.com/users/andrePankraz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/andrePankraz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/andrePankraz/subscriptions",
            "organizations_url": "https://api.github.com/users/andrePankraz/orgs",
            "repos_url": "https://api.github.com/users/andrePankraz/repos",
            "events_url": "https://api.github.com/users/andrePankraz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/andrePankraz/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-16T10:59:16Z",
        "updated_at": "2023-11-19T15:44:21Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nAccording to blog this should work:\r\n\r\n`vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\r\npipeline = IngestionPipeline(\r\n    transformations=[\r\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\r\n        TitleExtractor(),\r\n        OpenAIEmbedding(),\r\n    ],\r\n    cache=IngestionCache(cache=RedisCache(), collection=\"test_cache\"),\r\n    vector_store=vector_store,\r\n)\r\npipeline.run(documents=[Document.example()])`\r\n\r\nThis doesn't work with OpensearchVectorStore, because it's still a normal VectorStore and not refactored into BasePydanticVectorStore\r\n\r\n### Version\r\n\r\n0.9.1\r\n\r\n### Steps to Reproduce\r\n\r\nJust try a pipeline with an OpensearchVectorStore\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8954/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8954/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8953",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8953/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8953/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8953/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8953",
        "id": 1996430558,
        "node_id": "I_kwDOIWuq5852_xze",
        "number": 8953,
        "title": "[Question]: How to get the raw SQL queries generated when using SubQuery Engine?",
        "user": {
            "login": "sumitsahay68",
            "id": 30551373,
            "node_id": "MDQ6VXNlcjMwNTUxMzcz",
            "avatar_url": "https://avatars.githubusercontent.com/u/30551373?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sumitsahay68",
            "html_url": "https://github.com/sumitsahay68",
            "followers_url": "https://api.github.com/users/sumitsahay68/followers",
            "following_url": "https://api.github.com/users/sumitsahay68/following{/other_user}",
            "gists_url": "https://api.github.com/users/sumitsahay68/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sumitsahay68/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sumitsahay68/subscriptions",
            "organizations_url": "https://api.github.com/users/sumitsahay68/orgs",
            "repos_url": "https://api.github.com/users/sumitsahay68/repos",
            "events_url": "https://api.github.com/users/sumitsahay68/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sumitsahay68/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-16T09:26:54Z",
        "updated_at": "2023-11-16T09:36:31Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have built Natural Language to SQL-based ChatBot for a dataset I'm having by utilizing LlamaIndex's `SubQuestionQueryEngine`. I'm able to get the response to my complex queries as well. However, even after analyzing the run logs and extracting from the response object, I'm NOT able to find the **raw SQL query** that was run against my table.\r\n\r\nI have tried setting `synthesize_response=False` while creating the `SQLTableRetrieverQueryEngine`, however, it is throwing a key error if I try to extract the query from metadata as suggested in this discord [thread](https://discord.com/channels/1059199217496772688/1131630431398281226/1131630575753633912). \r\n\r\nHere's my code for reference:\r\n\r\n```\r\n# Create an instance of the SQLTableRetrieverQueryEngine class using the SQLDatabase object and object index\r\nsql_query_engine = SQLTableRetrieverQueryEngine(sql_database, \r\n                                                obj_index.as_retriever(similarity_top_k=1),\r\n                                                synthesize_response=False\r\n                                                )\r\n\r\n# Define a list of query engine tools\r\nquery_engine_tools = [\r\n    QueryEngineTool(\r\n        query_engine=sql_query_engine,\r\n        metadata=ToolMetadata(\r\n            name=\"Customer Loan Database\",\r\n            description=\"Database containing the Loan Application Data table\"\r\n        )\r\n    )\r\n]\r\n\r\n# Create a subquery engine using the query engine tools, service context, and enabling asynchronous execution\r\nsubquery_engine = SubQuestionQueryEngine.from_defaults(\r\n    query_engine_tools=query_engine_tools,\r\n    service_context=service_context,\r\n    use_async=True\r\n)\r\n```\r\nPlease help me print the individual SQL queries too. Thanks!",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8953/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8953/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8951",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8951/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8951/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8951/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8951",
        "id": 1996292869,
        "node_id": "I_kwDOIWuq5852_QMF",
        "number": 8951,
        "title": "[Question]: How to convert vectorstore index to faiss index",
        "user": {
            "login": "SCUT-ChenBD",
            "id": 49072146,
            "node_id": "MDQ6VXNlcjQ5MDcyMTQ2",
            "avatar_url": "https://avatars.githubusercontent.com/u/49072146?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SCUT-ChenBD",
            "html_url": "https://github.com/SCUT-ChenBD",
            "followers_url": "https://api.github.com/users/SCUT-ChenBD/followers",
            "following_url": "https://api.github.com/users/SCUT-ChenBD/following{/other_user}",
            "gists_url": "https://api.github.com/users/SCUT-ChenBD/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SCUT-ChenBD/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SCUT-ChenBD/subscriptions",
            "organizations_url": "https://api.github.com/users/SCUT-ChenBD/orgs",
            "repos_url": "https://api.github.com/users/SCUT-ChenBD/repos",
            "events_url": "https://api.github.com/users/SCUT-ChenBD/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SCUT-ChenBD/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-16T08:06:23Z",
        "updated_at": "2023-11-16T15:15:40Z",
        "closed_at": "2023-11-16T15:15:40Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHow to convert vectorstore index to faiss index",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8951/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8951/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8950",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8950/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8950/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8950/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8950",
        "id": 1996284465,
        "node_id": "PR_kwDOIWuq585fmVpN",
        "number": 8950,
        "title": "Add is_image_to_text as MM index class attribution",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-16T08:02:22Z",
        "updated_at": "2023-11-16T15:55:49Z",
        "closed_at": "2023-11-16T15:55:48Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8950",
            "html_url": "https://github.com/run-llama/llama_index/pull/8950",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8950.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8950.patch",
            "merged_at": "2023-11-16T15:55:48Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8950/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8950/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8949",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8949/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8949/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8949/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8949",
        "id": 1996276931,
        "node_id": "PR_kwDOIWuq585fmT-d",
        "number": 8949,
        "title": "Implement condensed context conversation chat engine",
        "user": {
            "login": "kapil-malik",
            "id": 3902288,
            "node_id": "MDQ6VXNlcjM5MDIyODg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3902288?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kapil-malik",
            "html_url": "https://github.com/kapil-malik",
            "followers_url": "https://api.github.com/users/kapil-malik/followers",
            "following_url": "https://api.github.com/users/kapil-malik/following{/other_user}",
            "gists_url": "https://api.github.com/users/kapil-malik/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kapil-malik/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kapil-malik/subscriptions",
            "organizations_url": "https://api.github.com/users/kapil-malik/orgs",
            "repos_url": "https://api.github.com/users/kapil-malik/repos",
            "events_url": "https://api.github.com/users/kapil-malik/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kapil-malik/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-11-16T07:58:01Z",
        "updated_at": "2023-11-20T00:44:00Z",
        "closed_at": "2023-11-20T00:43:59Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8949",
            "html_url": "https://github.com/run-llama/llama_index/pull/8949",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8949.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8949.patch",
            "merged_at": "2023-11-20T00:43:59Z"
        },
        "body": "# Description\r\n\r\n__Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.__\r\nImplemented a Condensed Conversation & Context Chat Engine. This gets the goodness of both the context and condense_question chat engines.\r\n* First condense a conversation and latest user message to a standalone question\r\n* Then build a context for the standalone question from a retriever,\r\n* Then pass the context along with prompt and user message to LLM to generate a response.\r\n\r\nThis has been inspired by something which is available in AWS Kendra but lacking in llama_index\r\nRefer: [Kendra_solution_for_condensation_plus_context_retrieval](https://github.com/aws-samples/amazon-kendra-langchain-extensions/blob/main/kendra_retriever_samples/kendra_chat_open_ai.py)\r\n\r\nFixes # (issue)\r\nNA\r\n\r\n## Type of Change\r\n\r\nNew type of chat engine. Available directly via \r\n```\r\nfrom llama_index.chat_engine.condense_plus_context import CondensePlusContextChatEngine\r\n\r\nCondensePlusContextChatEngine.from_defaults( ... )\r\n```\r\nAnd also via \r\n```\r\nindex.as_chat_engine(chat_mode=ChatMode.CONDENSE_PLUS_CONTEXT ...)\r\n```\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\n__Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration__\r\n\r\nVerified via unit tests and also a new notebook.\r\n\r\n- [x] Added new unit/integration tests\r\n- [x] Added new notebook (that tests end-to-end)\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [x] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8949/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8949/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8948",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8948/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8948/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8948/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8948",
        "id": 1996192045,
        "node_id": "I_kwDOIWuq5852-3kt",
        "number": 8948,
        "title": "[Bug]: none is not an allowed value (type=type_error.none.not_allowed)",
        "user": {
            "login": "rzechen",
            "id": 24729413,
            "node_id": "MDQ6VXNlcjI0NzI5NDEz",
            "avatar_url": "https://avatars.githubusercontent.com/u/24729413?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rzechen",
            "html_url": "https://github.com/rzechen",
            "followers_url": "https://api.github.com/users/rzechen/followers",
            "following_url": "https://api.github.com/users/rzechen/following{/other_user}",
            "gists_url": "https://api.github.com/users/rzechen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rzechen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rzechen/subscriptions",
            "organizations_url": "https://api.github.com/users/rzechen/orgs",
            "repos_url": "https://api.github.com/users/rzechen/repos",
            "events_url": "https://api.github.com/users/rzechen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rzechen/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-16T06:58:53Z",
        "updated_at": "2023-11-16T07:07:16Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\n    message=ChatMessage(\r\n  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError: 1 validation error for ChatMessage\r\nrole\r\n  none is not an allowed value (type=type_error.none.not_allowed)\r\n\r\n<img width=\"1404\" alt=\"image\" src=\"https://github.com/run-llama/llama_index/assets/24729413/1d899140-68d5-48f4-b8e5-3189640bcd6a\">\r\n\n\n### Version\n\n0.8.61\n\n### Steps to Reproduce\n\nstream\u4e3afasle\u5c31\u6ca1\u6709\u95ee\u9898\uff0cstream\u4e3ature\uff0c\u4e5f\u8bb8\u662frole\u4e3aNone\u5c31\u4f1a\u62a5\u9519\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8948/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8948/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8947",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8947/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8947/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8947/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8947",
        "id": 1996181601,
        "node_id": "PR_kwDOIWuq585fl_L5",
        "number": 8947,
        "title": "Fix Multi Modal Image to Text encoder bug",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-16T06:50:07Z",
        "updated_at": "2023-11-16T07:08:44Z",
        "closed_at": "2023-11-16T07:08:43Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8947",
            "html_url": "https://github.com/run-llama/llama_index/pull/8947",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8947.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8947.patch",
            "merged_at": "2023-11-16T07:08:43Z"
        },
        "body": "# Description\r\n\r\n\r\nissue: ```is_image_to_text = all(node.text for node in image_nodes)```\r\nSometime this line will become True   even we don't want to encode image into text embedding\r\nwe should \r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8947/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8947/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8945",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8945/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8945/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8945/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8945",
        "id": 1996098144,
        "node_id": "PR_kwDOIWuq585fltM1",
        "number": 8945,
        "title": "LMM-As-A-Judge: Multi-Modal Faithfulness and Relevancy Evaluators",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-16T05:35:21Z",
        "updated_at": "2023-11-16T19:46:06Z",
        "closed_at": "2023-11-16T19:46:05Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8945",
            "html_url": "https://github.com/run-llama/llama_index/pull/8945",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8945.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8945.patch",
            "merged_at": "2023-11-16T19:46:05Z"
        },
        "body": "# Description\r\n\r\nIn this PR, we add the `evaluation.multi_modal` module, which includes implementations for the multi-modal variants of `faithfulness` and `relevancy` metrics. Specifically:\r\n- `MultiModalRelevancyEvaluator` added\r\n- `MultiModalFaithfulnessEvaluator` added\r\n- Multi-Modal RAG Eval notebook updated\r\n    - TODO: Run the judges on all of the saved generated responses and make final observations (currently rate-limited by GPT-4V)\r\n \r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8945/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8945/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8944",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8944/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8944/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8944/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8944",
        "id": 1996031876,
        "node_id": "I_kwDOIWuq5852-QeE",
        "number": 8944,
        "title": "[Bug][llama_index-0.9.1]: ValueError: \"AzureOpenAI\" object has no field \"_http_client\"",
        "user": {
            "login": "tonydeep",
            "id": 4631359,
            "node_id": "MDQ6VXNlcjQ2MzEzNTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4631359?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tonydeep",
            "html_url": "https://github.com/tonydeep",
            "followers_url": "https://api.github.com/users/tonydeep/followers",
            "following_url": "https://api.github.com/users/tonydeep/following{/other_user}",
            "gists_url": "https://api.github.com/users/tonydeep/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tonydeep/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tonydeep/subscriptions",
            "organizations_url": "https://api.github.com/users/tonydeep/orgs",
            "repos_url": "https://api.github.com/users/tonydeep/repos",
            "events_url": "https://api.github.com/users/tonydeep/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tonydeep/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 8,
        "created_at": "2023-11-16T04:26:32Z",
        "updated_at": "2023-11-18T04:27:25Z",
        "closed_at": "2023-11-16T15:39:30Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nValueError: \"AzureOpenAI\" object has no field \"_http_client\"\n\n### Version\n\n0.9.1\n\n### Steps to Reproduce\n\nLine 96 in ../site-packages/llama_index/llms/azure_openai.py:96).\r\n\r\n`# Use the custom httpx client if provided.`\r\n`# Otherwise the value will be None.`\r\n`self._http_client = http_client`\r\n\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8944/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8944/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8943",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8943/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8943/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8943/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8943",
        "id": 1995963900,
        "node_id": "I_kwDOIWuq58529_38",
        "number": 8943,
        "title": "chunk_overlap cannot be set to 0 via ServiceContext.from_defaults()",
        "user": {
            "login": "sx8ac0t6",
            "id": 63575777,
            "node_id": "MDQ6VXNlcjYzNTc1Nzc3",
            "avatar_url": "https://avatars.githubusercontent.com/u/63575777?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sx8ac0t6",
            "html_url": "https://github.com/sx8ac0t6",
            "followers_url": "https://api.github.com/users/sx8ac0t6/followers",
            "following_url": "https://api.github.com/users/sx8ac0t6/following{/other_user}",
            "gists_url": "https://api.github.com/users/sx8ac0t6/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sx8ac0t6/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sx8ac0t6/subscriptions",
            "organizations_url": "https://api.github.com/users/sx8ac0t6/orgs",
            "repos_url": "https://api.github.com/users/sx8ac0t6/repos",
            "events_url": "https://api.github.com/users/sx8ac0t6/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sx8ac0t6/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-16T03:08:14Z",
        "updated_at": "2023-11-16T03:35:01Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI tried to set `chunk_overlap` to `0` via `ServiceContext.from_defaults()` as the code below:\r\n```\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=None,\r\n    embed_model=embedding_llm,\r\n    chunk_overlap=0\r\n)\r\n```\r\n\r\nBut the output of `service_context.transformations` says `chunk_overlap=200`, which is the default value (`SENTENCE_CHUNK_OVERLAP`).\r\n```\r\n[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f8b896c8e80>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;\u3002\uff1f\uff01]+[,.;\u3002\uff1f\uff01]?')]\r\n```\r\n\r\nWhen I try with `chunk_size=128`, which is smaller than `SENTENCE_CHUNK_OVERLAP`, I encounter the error below:\r\n```\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=None,\r\n    embed_model=embedding_llm,\r\n    chunk_size=128, \r\n    chunk_overlap=0\r\n)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[14], line 1\r\n----> 1 service_context = ServiceContext.from_defaults(\r\n      2     llm=None,\r\n      3     embed_model=embedding_llm,\r\n      4     chunk_size=128, \r\n      5     chunk_overlap=0\r\n      6 )\r\n\r\nFile /usr/local/lib/python3.9/site-packages/llama_index/service_context.py:188, in ServiceContext.from_defaults(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, text_splitter, transformations, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)\r\n    182 if text_splitter is not None and node_parser is not None:\r\n    183     raise ValueError(\"Cannot specify both text_splitter and node_parser\")\r\n    185 node_parser = (\r\n    186     text_splitter  # text splitter extends node parser\r\n    187     or node_parser\r\n--> 188     or _get_default_node_parser(\r\n    189         chunk_size=chunk_size or DEFAULT_CHUNK_SIZE,\r\n    190         chunk_overlap=chunk_overlap or SENTENCE_CHUNK_OVERLAP,\r\n    191         callback_manager=callback_manager,\r\n    192     )\r\n    193 )\r\n    195 transformations = transformations or [node_parser]\r\n    197 llama_logger = llama_logger or LlamaLogger()\r\n\r\nFile /usr/local/lib/python3.9/site-packages/llama_index/service_context.py:35, in _get_default_node_parser(chunk_size, chunk_overlap, callback_manager)\r\n     29 def _get_default_node_parser(\r\n     30     chunk_size: int = DEFAULT_CHUNK_SIZE,\r\n     31     chunk_overlap: int = SENTENCE_CHUNK_OVERLAP,\r\n     32     callback_manager: Optional[CallbackManager] = None,\r\n     33 ) -> NodeParser:\r\n     34     \"\"\"Get default node parser.\"\"\"\r\n---> 35     return SentenceSplitter(\r\n     36         chunk_size=chunk_size,\r\n     37         chunk_overlap=chunk_overlap,\r\n     38         callback_manager=callback_manager or CallbackManager(),\r\n     39     )\r\n\r\nFile /usr/local/lib/python3.9/site-packages/llama_index/node_parser/text/sentence.py:77, in SentenceSplitter.__init__(self, separator, chunk_size, chunk_overlap, tokenizer, paragraph_separator, chunking_tokenizer_fn, secondary_chunking_regex, callback_manager, include_metadata, include_prev_next_rel)\r\n     75 \"\"\"Initialize with parameters.\"\"\"\r\n     76 if chunk_overlap > chunk_size:\r\n---> 77     raise ValueError(\r\n     78         f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\r\n     79         f\"({chunk_size}), should be smaller.\"\r\n     80     )\r\n     82 callback_manager = callback_manager or CallbackManager([])\r\n     83 self._chunking_tokenizer_fn = (\r\n     84     chunking_tokenizer_fn or split_by_sentence_tokenizer()\r\n     85 )\r\n\r\nValueError: Got a larger chunk overlap (200) than chunk size (128), should be smaller.\r\n```\r\n\r\nI guess that in line 190 of `service_context.py`, `SENTENCE_CHUNK_OVERLAP` is used for `chunk_overlap` when the passed value is `0`, as it is deemed as `False`.\n\n### Version\n\n0.9.1\n\n### Steps to Reproduce\n\nAs described above.\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8943/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8943/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8942",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8942/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8942/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8942/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8942",
        "id": 1995956953,
        "node_id": "PR_kwDOIWuq585flO-i",
        "number": 8942,
        "title": "Add vectorflow schema",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-16T03:02:14Z",
        "updated_at": "2023-11-16T03:08:23Z",
        "closed_at": "2023-11-16T03:08:22Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8942",
            "html_url": "https://github.com/run-llama/llama_index/pull/8942",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8942.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8942.patch",
            "merged_at": "2023-11-16T03:08:22Z"
        },
        "body": "Quick integration to send a document to vectorflow",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8942/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8942/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8941",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8941/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8941/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8941/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8941",
        "id": 1995910104,
        "node_id": "PR_kwDOIWuq585flFU0",
        "number": 8941,
        "title": "[version] bump to v0.9.1",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-16T02:14:01Z",
        "updated_at": "2023-11-16T02:19:21Z",
        "closed_at": "2023-11-16T02:19:20Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8941",
            "html_url": "https://github.com/run-llama/llama_index/pull/8941",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8941.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8941.patch",
            "merged_at": "2023-11-16T02:19:20Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8941/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8941/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8940",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8940/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8940/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8940/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8940",
        "id": 1995894914,
        "node_id": "PR_kwDOIWuq585flCF6",
        "number": 8940,
        "title": "patch global service context",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-16T02:00:43Z",
        "updated_at": "2023-11-16T02:12:06Z",
        "closed_at": "2023-11-16T02:12:05Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8940",
            "html_url": "https://github.com/run-llama/llama_index/pull/8940",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8940.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8940.patch",
            "merged_at": "2023-11-16T02:12:05Z"
        },
        "body": "# Description\r\n\r\nThe global service context was not passing down it's transformations properly, causing the transformations list to be empty when setting a global service context.\r\n\r\nFixes https://github.com/run-llama/llama_index/issues/8939\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8940/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8940/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8939",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8939/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8939/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8939/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8939",
        "id": 1995882143,
        "node_id": "I_kwDOIWuq58529r6f",
        "number": 8939,
        "title": "[Bug]: VectorStoreIndex.from_documents changes break examples",
        "user": {
            "login": "piotrm0",
            "id": 421701,
            "node_id": "MDQ6VXNlcjQyMTcwMQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/421701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/piotrm0",
            "html_url": "https://github.com/piotrm0",
            "followers_url": "https://api.github.com/users/piotrm0/followers",
            "following_url": "https://api.github.com/users/piotrm0/following{/other_user}",
            "gists_url": "https://api.github.com/users/piotrm0/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/piotrm0/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/piotrm0/subscriptions",
            "organizations_url": "https://api.github.com/users/piotrm0/orgs",
            "repos_url": "https://api.github.com/users/piotrm0/repos",
            "events_url": "https://api.github.com/users/piotrm0/events{/privacy}",
            "received_events_url": "https://api.github.com/users/piotrm0/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-16T01:47:53Z",
        "updated_at": "2023-11-16T02:12:06Z",
        "closed_at": "2023-11-16T02:12:06Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nVectorStoreIndex class method from_documents chunks things differently in current version as compared to prior versions leading to max token issues. This code worked in 0.8.69 but fails at 0.9.0.post1 (or earlier).\r\n\r\n```python\r\nfrom llama_index import VectorStoreIndex, ServiceContext, set_global_service_context\r\nfrom llama_index.readers.web import SimpleWebPageReader\r\nfrom llama_index.llms import OpenAI\r\n\r\nllm = OpenAI(temperature=0.0)\r\nservice_context = ServiceContext.from_defaults(llm=llm)\r\nset_global_service_context(service_context)\r\n\r\ndocuments = SimpleWebPageReader(\r\n    html_to_text=True, metadata_fn=lambda url: dict(url=url)\r\n).load_data([\"http://paulgraham.com/worked.html\"])\r\nindex = VectorStoreIndex.from_documents(documents, show_progress=True)\r\n```\r\n\r\nIt fails on the `from_documents` step with the included exception traceback.\r\n\r\n\n\n### Version\n\n0.9.0.post1\n\n### Steps to Reproduce\n\nRun the included code.\n\n### Relevant Logs/Tracbacks\n\n```shell\n---------------------------------------------------------------------------\r\nBadRequestError                           Traceback (most recent call last)\r\n[/Volumes/dev_new/trulens/trulens_eval/examples/experimental/appui_example.ipynb](https://file+.vscode-resource.vscode-cdn.net/Volumes/dev_new/trulens/trulens_eval/examples/experimental/appui_example.ipynb) Cell 2 line <cell line: 12>()\r\n      [7](vscode-notebook-cell:/Volumes/dev_new/trulens/trulens_eval/examples/experimental/appui_example.ipynb#X22sZmlsZQ%3D%3D?line=6) set_global_service_context(service_context)\r\n      [9](vscode-notebook-cell:/Volumes/dev_new/trulens/trulens_eval/examples/experimental/appui_example.ipynb#X22sZmlsZQ%3D%3D?line=8) documents = SimpleWebPageReader(\r\n     [10](vscode-notebook-cell:/Volumes/dev_new/trulens/trulens_eval/examples/experimental/appui_example.ipynb#X22sZmlsZQ%3D%3D?line=9)     html_to_text=True, metadata_fn=lambda url: dict(url=url)\r\n     [11](vscode-notebook-cell:/Volumes/dev_new/trulens/trulens_eval/examples/experimental/appui_example.ipynb#X22sZmlsZQ%3D%3D?line=10) ).load_data([\"http://paulgraham.com/worked.html\"])\r\n---> [12](vscode-notebook-cell:/Volumes/dev_new/trulens/trulens_eval/examples/experimental/appui_example.ipynb#X22sZmlsZQ%3D%3D?line=11) index = VectorStoreIndex.from_documents(documents, show_progress=True)\r\n     [14](vscode-notebook-cell:/Volumes/dev_new/trulens/trulens_eval/examples/experimental/appui_example.ipynb#X22sZmlsZQ%3D%3D?line=13) #query_engine = index.as_query_engine()\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:106](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:106), in BaseIndex.from_documents(cls, documents, storage_context, service_context, show_progress, **kwargs)\r\n     [97](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:97)     docstore.set_document_hash(doc.get_doc_id(), doc.hash)\r\n     [99](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:99) nodes = run_transformations(\r\n    [100](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:100)     documents,  # type: ignore\r\n    [101](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:101)     service_context.transformations,\r\n    [102](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:102)     show_progress=show_progress,\r\n    [103](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:103)     **kwargs,\r\n    [104](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:104) )\r\n--> [106](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:106) return cls(\r\n    [107](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:107)     nodes=nodes,\r\n    [108](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:108)     storage_context=storage_context,\r\n    [109](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:109)     service_context=service_context,\r\n    [110](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:110)     show_progress=show_progress,\r\n    [111](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:111)     **kwargs,\r\n    [112](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:112) )\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:49](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:49), in VectorStoreIndex.__init__(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, show_progress, **kwargs)\r\n     [47](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:47) self._use_async = use_async\r\n     [48](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:48) self._store_nodes_override = store_nodes_override\r\n---> [49](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:49) super().__init__(\r\n     [50](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:50)     nodes=nodes,\r\n     [51](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:51)     index_struct=index_struct,\r\n     [52](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:52)     service_context=service_context,\r\n     [53](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:53)     storage_context=storage_context,\r\n     [54](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:54)     show_progress=show_progress,\r\n     [55](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:55)     **kwargs,\r\n     [56](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:56) )\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:71](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:71), in BaseIndex.__init__(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\r\n     [69](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:69) if index_struct is None:\r\n     [70](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:70)     assert nodes is not None\r\n---> [71](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:71)     index_struct = self.build_index_from_nodes(nodes)\r\n     [72](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:72) self._index_struct = index_struct\r\n     [73](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/base.py:73) self._storage_context.index_store.add_index_struct(self._index_struct)\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:254](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:254), in VectorStoreIndex.build_index_from_nodes(self, nodes, **insert_kwargs)\r\n    [243](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:243) def build_index_from_nodes(\r\n    [244](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:244)     self,\r\n    [245](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:245)     nodes: Sequence[BaseNode],\r\n    [246](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:246)     **insert_kwargs: Any,\r\n    [247](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:247) ) -> IndexDict:\r\n    [248](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:248)     \"\"\"Build the index from nodes.\r\n    [249](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:249) \r\n    [250](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:250)     NOTE: Overrides BaseIndex.build_index_from_nodes.\r\n    [251](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:251)         VectorStoreIndex only stores nodes in document store\r\n    [252](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:252)         if vector store does not store text\r\n    [253](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:253)     \"\"\"\r\n--> [254](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:254)     return self._build_index_from_nodes(nodes, **insert_kwargs)\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:235](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:235), in VectorStoreIndex._build_index_from_nodes(self, nodes, **insert_kwargs)\r\n    [233](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:233)     run_async_tasks(tasks)\r\n    [234](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:234) else:\r\n--> [235](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:235)     self._add_nodes_to_index(\r\n    [236](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:236)         index_struct,\r\n    [237](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:237)         nodes,\r\n    [238](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:238)         show_progress=self._show_progress,\r\n    [239](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:239)         **insert_kwargs,\r\n    [240](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:240)     )\r\n    [241](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:241) return index_struct\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:188](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:188), in VectorStoreIndex._add_nodes_to_index(self, index_struct, nodes, show_progress, **insert_kwargs)\r\n    [185](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:185) if not nodes:\r\n    [186](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:186)     return\r\n--> [188](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:188) nodes = self._get_node_with_embedding(nodes, show_progress)\r\n    [189](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:189) new_ids = self._vector_store.add(nodes, **insert_kwargs)\r\n    [191](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:191) if not self._vector_store.stores_text or self._store_nodes_override:\r\n    [192](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:192)     # NOTE: if the vector store doesn't store text,\r\n    [193](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:193)     # we need to add the nodes to the index struct and document store\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:100](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:100), in VectorStoreIndex._get_node_with_embedding(self, nodes, show_progress)\r\n     [89](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:89) def _get_node_with_embedding(\r\n     [90](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:90)     self,\r\n     [91](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:91)     nodes: Sequence[BaseNode],\r\n     [92](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:92)     show_progress: bool = False,\r\n     [93](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:93) ) -> List[BaseNode]:\r\n     [94](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:94)     \"\"\"Get tuples of id, node, and embedding.\r\n     [95](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:95) \r\n     [96](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:96)     Allows us to store these nodes in a vector store.\r\n     [97](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:97)     Embeddings are called in batches.\r\n     [98](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:98) \r\n     [99](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:99)     \"\"\"\r\n--> [100](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:100)     id_to_embed_map = embed_nodes(\r\n    [101](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:101)         nodes, self._service_context.embed_model, show_progress=show_progress\r\n    [102](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:102)     )\r\n    [104](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:104)     results = []\r\n    [105](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/vector_store/base.py:105)     for node in nodes:\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/utils.py:137](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/utils.py:137), in embed_nodes(nodes, embed_model, show_progress)\r\n    [134](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/utils.py:134)     else:\r\n    [135](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/utils.py:135)         id_to_embed_map[node.node_id] = node.embedding\r\n--> [137](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/utils.py:137) new_embeddings = embed_model.get_text_embedding_batch(\r\n    [138](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/utils.py:138)     texts_to_embed, show_progress=show_progress\r\n    [139](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/utils.py:139) )\r\n    [141](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/utils.py:141) for new_id, text_embedding in zip(ids_to_embed, new_embeddings):\r\n    [142](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/indices/utils.py:142)     id_to_embed_map[new_id] = text_embedding\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:255](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:255), in BaseEmbedding.get_text_embedding_batch(self, texts, show_progress, **kwargs)\r\n    [249](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:249) if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:\r\n    [250](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:250)     # flush\r\n    [251](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:251)     with self.callback_manager.event(\r\n    [252](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:252)         CBEventType.EMBEDDING,\r\n    [253](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:253)         payload={EventPayload.SERIALIZED: self.to_dict()},\r\n    [254](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:254)     ) as event:\r\n--> [255](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:255)         embeddings = self._get_text_embeddings(cur_batch)\r\n    [256](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:256)         result_embeddings.extend(embeddings)\r\n    [257](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:257)         event.on_end(\r\n    [258](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:258)             payload={\r\n    [259](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:259)                 EventPayload.CHUNKS: cur_batch,\r\n    [260](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:260)                 EventPayload.EMBEDDINGS: embeddings,\r\n    [261](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:261)             },\r\n    [262](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/base.py:262)         )\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:349](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:349), in OpenAIEmbedding._get_text_embeddings(self, texts)\r\n    [342](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:342) def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\r\n    [343](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:343)     \"\"\"Get text embeddings.\r\n    [344](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:344) \r\n    [345](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:345)     By default, this is a wrapper around _get_text_embedding.\r\n    [346](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:346)     Can be overridden for batch queries.\r\n    [347](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:347) \r\n    [348](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:348)     \"\"\"\r\n--> [349](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:349)     return get_embeddings(\r\n    [350](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:350)         self._client,\r\n    [351](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:351)         texts,\r\n    [352](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:352)         engine=self._text_engine,\r\n    [353](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:353)         **self.additional_kwargs,\r\n    [354](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:354)     )\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:289](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:289), in BaseRetrying.wraps.<locals>.wrapped_f(*args, **kw)\r\n    [287](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:287) @functools.wraps(f)\r\n    [288](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:288) def wrapped_f(*args: t.Any, **kw: t.Any) -> t.Any:\r\n--> [289](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:289)     return self(f, *args, **kw)\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:379](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:379), in Retrying.__call__(self, fn, *args, **kwargs)\r\n    [377](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:377) retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\r\n    [378](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:378) while True:\r\n--> [379](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:379)     do = self.iter(retry_state=retry_state)\r\n    [380](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:380)     if isinstance(do, DoAttempt):\r\n    [381](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:381)         try:\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:325](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:325), in BaseRetrying.iter(self, retry_state)\r\n    [323](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:323)     retry_exc = self.retry_error_cls(fut)\r\n    [324](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:324)     if self.reraise:\r\n--> [325](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:325)         raise retry_exc.reraise()\r\n    [326](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:326)     raise retry_exc from fut.exception()\r\n    [328](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:328) if self.wait:\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:158](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:158), in RetryError.reraise(self)\r\n    [156](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:156) def reraise(self) -> t.NoReturn:\r\n    [157](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:157)     if self.last_attempt.failed:\r\n--> [158](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:158)         raise self.last_attempt.result()\r\n    [159](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:159)     raise self\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:437](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:437), in Future.result(self, timeout)\r\n    [435](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:435)     raise CancelledError()\r\n    [436](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:436) elif self._state == FINISHED:\r\n--> [437](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:437)     return self.__get_result()\r\n    [439](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:439) self._condition.wait(timeout)\r\n    [441](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:441) if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:389](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:389), in Future.__get_result(self)\r\n    [387](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:387) if self._exception:\r\n    [388](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:388)     try:\r\n--> [389](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:389)         raise self._exception\r\n    [390](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:390)     finally:\r\n    [391](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:391)         # Break a reference cycle with the exception in self._exception\r\n    [392](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/concurrent/futures/_base.py:392)         self = None\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:382](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:382), in Retrying.__call__(self, fn, *args, **kwargs)\r\n    [380](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:380) if isinstance(do, DoAttempt):\r\n    [381](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:381)     try:\r\n--> [382](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:382)         result = fn(*args, **kwargs)\r\n    [383](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:383)     except BaseException:  # noqa: B902\r\n    [384](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/tenacity/__init__.py:384)         retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:161](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:161), in get_embeddings(client, list_of_text, engine, **kwargs)\r\n    [157](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:157) assert len(list_of_text) <= 2048, \"The batch size should not be larger than 2048.\"\r\n    [159](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:159) list_of_text = [text.replace(\"\\n\", \" \") for text in list_of_text]\r\n--> [161](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:161) data = client.embeddings.create(input=list_of_text, model=engine, **kwargs).data\r\n    [162](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/llama_index/embeddings/openai.py:162) return [d.embedding for d in data]\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:105](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:105), in Embeddings.create(self, input, model, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\r\n     [99](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:99)         embedding.embedding = np.frombuffer(  # type: ignore[no-untyped-call]\r\n    [100](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:100)             base64.b64decode(data), dtype=\"float32\"\r\n    [101](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:101)         ).tolist()\r\n    [103](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:103)     return obj\r\n--> [105](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:105) return self._post(\r\n    [106](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:106)     \"[/embeddings](https://file+.vscode-resource.vscode-cdn.net/embeddings)\",\r\n    [107](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:107)     body=maybe_transform(params, embedding_create_params.EmbeddingCreateParams),\r\n    [108](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:108)     options=make_request_options(\r\n    [109](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:109)         extra_headers=extra_headers,\r\n    [110](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:110)         extra_query=extra_query,\r\n    [111](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:111)         extra_body=extra_body,\r\n    [112](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:112)         timeout=timeout,\r\n    [113](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:113)         post_parser=parser,\r\n    [114](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:114)     ),\r\n    [115](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:115)     cast_to=CreateEmbeddingResponse,\r\n    [116](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/resources/embeddings.py:116) )\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1055](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1055), in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\r\n   [1041](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1041) def post(\r\n   [1042](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1042)     self,\r\n   [1043](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1043)     path: str,\r\n   (...)\r\n   [1050](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1050)     stream_cls: type[_StreamT] | None = None,\r\n   [1051](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1051) ) -> ResponseT | _StreamT:\r\n   [1052](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1052)     opts = FinalRequestOptions.construct(\r\n   [1053](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1053)         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\r\n   [1054](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1054)     )\r\n-> [1055](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:1055)     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:834](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:834), in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    [825](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:825) def request(\r\n    [826](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:826)     self,\r\n    [827](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:827)     cast_to: Type[ResponseT],\r\n   (...)\r\n    [832](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:832)     stream_cls: type[_StreamT] | None = None,\r\n    [833](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:833) ) -> ResponseT | _StreamT:\r\n--> [834](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:834)     return self._request(\r\n    [835](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:835)         cast_to=cast_to,\r\n    [836](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:836)         options=options,\r\n    [837](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:837)         stream=stream,\r\n    [838](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:838)         stream_cls=stream_cls,\r\n    [839](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:839)         remaining_retries=remaining_retries,\r\n    [840](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:840)     )\r\n\r\nFile [/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:877](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:877), in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    [874](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:874)     # If the response is streamed then we need to explicitly read the response\r\n    [875](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:875)     # to completion before attempting to access the response text.\r\n    [876](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:876)     err.response.read()\r\n--> [877](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:877)     raise self._make_status_error_from_response(err.response) from None\r\n    [878](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:878) except httpx.TimeoutException as err:\r\n    [879](https://file+.vscode-resource.vscode-cdn.net/opt/homebrew/Caskroom/miniconda/base/envs/py38_trulens/lib/python3.8/site-packages/openai/_base_client.py:879)     if retries > 0:\r\n\r\nBadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 16976 tokens (16976 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\r\n```\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8939/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8939/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8938",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8938/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8938/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8938/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8938",
        "id": 1995839835,
        "node_id": "PR_kwDOIWuq585fk2oR",
        "number": 8938,
        "title": "Fix SentenceSplitter returning empty list",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-16T00:56:31Z",
        "updated_at": "2023-11-16T01:36:27Z",
        "closed_at": "2023-11-16T01:36:26Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8938",
            "html_url": "https://github.com/run-llama/llama_index/pull/8938",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8938.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8938.patch",
            "merged_at": "2023-11-16T01:36:26Z"
        },
        "body": "# Description\r\n\r\n`SentenceSplitter` returns an empty list when invoking `get_nodes_from_documents` with `ImageDocuments` that have no text.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8938/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8938/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8937",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8937/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8937/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8937/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8937",
        "id": 1995756035,
        "node_id": "PR_kwDOIWuq585fkkUk",
        "number": 8937,
        "title": "Add Astra DB batched insertion for large multi-doc inserts",
        "user": {
            "login": "erichare",
            "id": 700235,
            "node_id": "MDQ6VXNlcjcwMDIzNQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/700235?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/erichare",
            "html_url": "https://github.com/erichare",
            "followers_url": "https://api.github.com/users/erichare/followers",
            "following_url": "https://api.github.com/users/erichare/following{/other_user}",
            "gists_url": "https://api.github.com/users/erichare/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/erichare/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/erichare/subscriptions",
            "organizations_url": "https://api.github.com/users/erichare/orgs",
            "repos_url": "https://api.github.com/users/erichare/repos",
            "events_url": "https://api.github.com/users/erichare/events{/privacy}",
            "received_events_url": "https://api.github.com/users/erichare/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-15T23:32:37Z",
        "updated_at": "2023-11-17T14:27:56Z",
        "closed_at": "2023-11-17T06:12:31Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8937",
            "html_url": "https://github.com/run-llama/llama_index/pull/8937",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8937.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8937.patch",
            "merged_at": "2023-11-17T06:12:31Z"
        },
        "body": "# Description\r\n\r\nThis update supports the latest Astra DB API interface, and ensures that bulk document insertions above the Astra-defined limit proceed in a batched fashion. It also cleans up a few components of the Astra DB example notebook accordingly.\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n(Note that an existing test and notebook already exist and are updated accordingly)\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [x] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8937/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8937/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8936",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8936/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8936/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8936/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8936",
        "id": 1995582780,
        "node_id": "PR_kwDOIWuq585fj-cT",
        "number": 8936,
        "title": "Bug Fix: GooglePaLM Embeddings",
        "user": {
            "login": "ravi03071991",
            "id": 12198101,
            "node_id": "MDQ6VXNlcjEyMTk4MTAx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12198101?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ravi03071991",
            "html_url": "https://github.com/ravi03071991",
            "followers_url": "https://api.github.com/users/ravi03071991/followers",
            "following_url": "https://api.github.com/users/ravi03071991/following{/other_user}",
            "gists_url": "https://api.github.com/users/ravi03071991/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ravi03071991/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ravi03071991/subscriptions",
            "organizations_url": "https://api.github.com/users/ravi03071991/orgs",
            "repos_url": "https://api.github.com/users/ravi03071991/repos",
            "events_url": "https://api.github.com/users/ravi03071991/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ravi03071991/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-15T21:18:57Z",
        "updated_at": "2023-11-15T21:27:49Z",
        "closed_at": "2023-11-15T21:27:49Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8936",
            "html_url": "https://github.com/run-llama/llama_index/pull/8936",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8936.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8936.patch",
            "merged_at": "2023-11-15T21:27:49Z"
        },
        "body": "# Description\r\n\r\nPR to fix the bug related to GooglePaLM Embeddings while indexing.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8936/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 1,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8936/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8935",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8935/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8935/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8935/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8935",
        "id": 1995458952,
        "node_id": "PR_kwDOIWuq585fjjMP",
        "number": 8935,
        "title": "[version] bump version to 0.9.0.post1",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-15T19:51:30Z",
        "updated_at": "2023-11-15T21:51:16Z",
        "closed_at": "2023-11-15T21:51:15Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8935",
            "html_url": "https://github.com/run-llama/llama_index/pull/8935",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8935.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8935.patch",
            "merged_at": "2023-11-15T21:51:15Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8935/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8935/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8934",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8934/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8934/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8934/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8934",
        "id": 1995430569,
        "node_id": "PR_kwDOIWuq585fjc3b",
        "number": 8934,
        "title": "GPT4-V Experiments with COT prompting technique.",
        "user": {
            "login": "ravi03071991",
            "id": 12198101,
            "node_id": "MDQ6VXNlcjEyMTk4MTAx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12198101?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ravi03071991",
            "html_url": "https://github.com/ravi03071991",
            "followers_url": "https://api.github.com/users/ravi03071991/followers",
            "following_url": "https://api.github.com/users/ravi03071991/following{/other_user}",
            "gists_url": "https://api.github.com/users/ravi03071991/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ravi03071991/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ravi03071991/subscriptions",
            "organizations_url": "https://api.github.com/users/ravi03071991/orgs",
            "repos_url": "https://api.github.com/users/ravi03071991/repos",
            "events_url": "https://api.github.com/users/ravi03071991/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ravi03071991/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-15T19:37:30Z",
        "updated_at": "2023-11-16T21:39:03Z",
        "closed_at": "2023-11-16T21:39:03Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8934",
            "html_url": "https://github.com/run-llama/llama_index/pull/8934",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8934.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8934.patch",
            "merged_at": "2023-11-16T21:39:03Z"
        },
        "body": "# Description\r\n\r\nPR to showcase the capabilities of GPT4-V with more pointed, granular questions and COT technique.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8934/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8934/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8933",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8933/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8933/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8933/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8933",
        "id": 1995364695,
        "node_id": "PR_kwDOIWuq585fjOYc",
        "number": 8933,
        "title": "fix query engine tool in openai agent",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-15T19:07:50Z",
        "updated_at": "2023-11-15T19:50:22Z",
        "closed_at": "2023-11-15T19:50:21Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8933",
            "html_url": "https://github.com/run-llama/llama_index/pull/8933",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8933.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8933.patch",
            "merged_at": "2023-11-15T19:50:21Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8933/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8933/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8932",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8932/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8932/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8932/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8932",
        "id": 1995323580,
        "node_id": "I_kwDOIWuq58527ji8",
        "number": 8932,
        "title": "[Bug]: Upgrading TensorFlow to version 2.15, encountering an issue...",
        "user": {
            "login": "qxpf666",
            "id": 166635,
            "node_id": "MDQ6VXNlcjE2NjYzNQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/166635?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/qxpf666",
            "html_url": "https://github.com/qxpf666",
            "followers_url": "https://api.github.com/users/qxpf666/followers",
            "following_url": "https://api.github.com/users/qxpf666/following{/other_user}",
            "gists_url": "https://api.github.com/users/qxpf666/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/qxpf666/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/qxpf666/subscriptions",
            "organizations_url": "https://api.github.com/users/qxpf666/orgs",
            "repos_url": "https://api.github.com/users/qxpf666/repos",
            "events_url": "https://api.github.com/users/qxpf666/events{/privacy}",
            "received_events_url": "https://api.github.com/users/qxpf666/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-15T18:39:34Z",
        "updated_at": "2023-11-15T18:47:02Z",
        "closed_at": "2023-11-15T18:41:31Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\n2023-11-16 02:00:47.831031: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nWARNING:tensorflow:From D:\\djangosass\\sass_env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n### Version\n\n0.9.0\n\n### Steps to Reproduce\n\nIt seems like I am encountering an issue after upgrading TensorFlow to version 2.15. The warning suggests that custom operations for oneDNN are enabled, and it advises you to set the environment variable TF_ENABLE_ONEDNN_OPTS=0 to potentially address numerical differences.\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8932/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8932/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8931",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8931/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8931/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8931/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8931",
        "id": 1995268511,
        "node_id": "I_kwDOIWuq58527WGf",
        "number": 8931,
        "title": "[Feature Request]: GCP Matching Engine Support",
        "user": {
            "login": "tambulkar",
            "id": 28329861,
            "node_id": "MDQ6VXNlcjI4MzI5ODYx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28329861?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tambulkar",
            "html_url": "https://github.com/tambulkar",
            "followers_url": "https://api.github.com/users/tambulkar/followers",
            "following_url": "https://api.github.com/users/tambulkar/following{/other_user}",
            "gists_url": "https://api.github.com/users/tambulkar/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tambulkar/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tambulkar/subscriptions",
            "organizations_url": "https://api.github.com/users/tambulkar/orgs",
            "repos_url": "https://api.github.com/users/tambulkar/repos",
            "events_url": "https://api.github.com/users/tambulkar/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tambulkar/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-15T17:59:33Z",
        "updated_at": "2023-11-28T18:35:55Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nI would like to add  support for GCP Matching Engine as a vector store.\n\n### Reason\n\nI don't see GCP listed in https://github.com/run-llama/llama_index/tree/main/llama_index/vector_stores . We would have to write a custom vector store, but I feel like it could be include in the repo.\n\n### Value of Feature\n\nAllow Google Cloud Users to easily integrate with Llamaindex.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8931/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8931/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8929",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8929/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8929/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8929/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8929",
        "id": 1995239117,
        "node_id": "PR_kwDOIWuq585fizPO",
        "number": 8929,
        "title": "GPT4-V Experiments with pointed questions and COT techniques.",
        "user": {
            "login": "ravi03071991",
            "id": 12198101,
            "node_id": "MDQ6VXNlcjEyMTk4MTAx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12198101?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ravi03071991",
            "html_url": "https://github.com/ravi03071991",
            "followers_url": "https://api.github.com/users/ravi03071991/followers",
            "following_url": "https://api.github.com/users/ravi03071991/following{/other_user}",
            "gists_url": "https://api.github.com/users/ravi03071991/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ravi03071991/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ravi03071991/subscriptions",
            "organizations_url": "https://api.github.com/users/ravi03071991/orgs",
            "repos_url": "https://api.github.com/users/ravi03071991/repos",
            "events_url": "https://api.github.com/users/ravi03071991/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ravi03071991/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-15T17:37:50Z",
        "updated_at": "2023-11-15T18:00:28Z",
        "closed_at": "2023-11-15T17:38:25Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8929",
            "html_url": "https://github.com/run-llama/llama_index/pull/8929",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8929.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8929.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nPR to showcase the capabilities of GPT4-V with more pointed, granular questions and COT technique.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8929/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8929/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8928",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8928/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8928/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8928/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8928",
        "id": 1995131505,
        "node_id": "PR_kwDOIWuq585fiboS",
        "number": 8928,
        "title": "changelog tweak",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-15T16:32:52Z",
        "updated_at": "2023-11-15T16:33:11Z",
        "closed_at": "2023-11-15T16:33:10Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8928",
            "html_url": "https://github.com/run-llama/llama_index/pull/8928",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8928.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8928.patch",
            "merged_at": "2023-11-15T16:33:10Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8928/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8928/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8927",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8927/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8927/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8927/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8927",
        "id": 1995039312,
        "node_id": "I_kwDOIWuq58526eJQ",
        "number": 8927,
        "title": "[Bug]: .as_query_engine(streaming=True, similarity_top_k=1) returns \"Response\" type instead of \"StreamingResponse\"",
        "user": {
            "login": "Mokinz",
            "id": 62729600,
            "node_id": "MDQ6VXNlcjYyNzI5NjAw",
            "avatar_url": "https://avatars.githubusercontent.com/u/62729600?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Mokinz",
            "html_url": "https://github.com/Mokinz",
            "followers_url": "https://api.github.com/users/Mokinz/followers",
            "following_url": "https://api.github.com/users/Mokinz/following{/other_user}",
            "gists_url": "https://api.github.com/users/Mokinz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Mokinz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Mokinz/subscriptions",
            "organizations_url": "https://api.github.com/users/Mokinz/orgs",
            "repos_url": "https://api.github.com/users/Mokinz/repos",
            "events_url": "https://api.github.com/users/Mokinz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Mokinz/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2023-11-15T15:46:33Z",
        "updated_at": "2023-11-16T20:28:29Z",
        "closed_at": "2023-11-16T20:28:28Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\n.as_query_engine(streaming=True, similarity_top_k=1) returns \"Response\" type instead of \"StreamingResponse\"\n\n### Version\n\n0.8.69.post2\n\n### Steps to Reproduce\n\nCreate vector store index, then:\r\n\r\n```\r\nquery_engine = index.as_query_engine(streaming = True, similarity_top_k=1)\r\nstreming_response = query._engine.query(prompt)\r\n\r\nreturn response.response_gen\r\n```\n\n### Relevant Logs/Tracbacks\n\n```shell\nFile \"C:\\Projects\\chatbot\\llm.py\", line 107, in generate_llama_response\r\n    return response_iter.response_gen\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: 'Response' object has no attribute 'response_gen'\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8927/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8927/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8926",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8926/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8926/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8926/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8926",
        "id": 1995034154,
        "node_id": "PR_kwDOIWuq585fiGJ8",
        "number": 8926,
        "title": "added error msg when llm is not explicitly passed",
        "user": {
            "login": "girijesh97",
            "id": 19616698,
            "node_id": "MDQ6VXNlcjE5NjE2Njk4",
            "avatar_url": "https://avatars.githubusercontent.com/u/19616698?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/girijesh97",
            "html_url": "https://github.com/girijesh97",
            "followers_url": "https://api.github.com/users/girijesh97/followers",
            "following_url": "https://api.github.com/users/girijesh97/following{/other_user}",
            "gists_url": "https://api.github.com/users/girijesh97/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/girijesh97/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/girijesh97/subscriptions",
            "organizations_url": "https://api.github.com/users/girijesh97/orgs",
            "repos_url": "https://api.github.com/users/girijesh97/repos",
            "events_url": "https://api.github.com/users/girijesh97/events{/privacy}",
            "received_events_url": "https://api.github.com/users/girijesh97/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-15T15:43:45Z",
        "updated_at": "2023-11-16T16:38:30Z",
        "closed_at": "2023-11-16T16:38:30Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8926",
            "html_url": "https://github.com/run-llama/llama_index/pull/8926",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8926.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8926.patch",
            "merged_at": "2023-11-16T16:38:30Z"
        },
        "body": "# Description\r\n\r\nIf llm was not explicitly passed then code was trying to make request through Open Ai gpt3.5 turbo by default which may not be available and will throw an authentication error. It is hard to diagnose the problem without proper error message.\r\nWe didn't use the resolve_llm function from utils.py because it can roll down to mock LLM which won't help us in this question generation task.\r\n\r\nFixes # (issue)\r\nwe added None check, and if LLM is not explicitly passed then we will raise an error to pass the llm\r\n## Type of Change\r\nadded None check\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [x] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n\r\n# How Has This Been Tested?\r\nI tested by installing locally and by passing and not passing the llm object\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8926/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8926/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8925",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8925/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8925/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8925/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8925",
        "id": 1994646347,
        "node_id": "I_kwDOIWuq58524-NL",
        "number": 8925,
        "title": "Impossible to use any other LLM than openai for querying SQL",
        "user": {
            "login": "tekntrash",
            "id": 102220086,
            "node_id": "U_kgDOBhfBNg",
            "avatar_url": "https://avatars.githubusercontent.com/u/102220086?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tekntrash",
            "html_url": "https://github.com/tekntrash",
            "followers_url": "https://api.github.com/users/tekntrash/followers",
            "following_url": "https://api.github.com/users/tekntrash/following{/other_user}",
            "gists_url": "https://api.github.com/users/tekntrash/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tekntrash/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tekntrash/subscriptions",
            "organizations_url": "https://api.github.com/users/tekntrash/orgs",
            "repos_url": "https://api.github.com/users/tekntrash/repos",
            "events_url": "https://api.github.com/users/tekntrash/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tekntrash/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-11-15T12:03:27Z",
        "updated_at": "2023-11-15T16:10:32Z",
        "closed_at": "2023-11-15T16:10:31Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIt is impossible to use any other LLM than OpenAI for querying SQL\r\n\r\nEven though the manual says you can use ollama to load other LLMs such as Llama 2 and use the following commands to use it:\r\n_llm = Ollama(model=\"llama2\")\r\nservice_context = ServiceContext.from_defaults(llm=llm)_\r\n\r\nAs soon it finds and openai.api_key it just uses openAI and disregards the above. Also, it downloads the BAAI--bge-small-en model for no reason, as it cannot use it if we remove the open ai key, as it throws the message:\r\n_Exception ignored in: <function _LlamaContext.__del__ at 0x7f4d507ddaf0>\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/lib/python3.9/site-packages/llama_cpp/llama.py\", line 422, in __del__\r\nTypeError: 'NoneType' object is not callable\r\nException ignored in: <function _LlamaModel.__del__ at 0x7f4d507dbb80>\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/lib/python3.9/site-packages/llama_cpp/llama.py\", line 240, in __del__\r\nTypeError: 'NoneType' object is not callable\r\nException ignored in: <function _LlamaBatch.__del__ at 0x7f4d507dfe50>\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/lib/python3.9/site-packages/llama_cpp/llama.py\", line 670, in __del__\r\nTypeError: 'NoneType' object is not callable_\r\n\r\nIt looks like this is an incomplete product",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8925/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8925/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8924",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8924/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8924/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8924/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8924",
        "id": 1994611028,
        "node_id": "I_kwDOIWuq585241lU",
        "number": 8924,
        "title": "[Bug]: openai.log = \"debug\" doesn't work on latest",
        "user": {
            "login": "heldrida",
            "id": 236752,
            "node_id": "MDQ6VXNlcjIzNjc1Mg==",
            "avatar_url": "https://avatars.githubusercontent.com/u/236752?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/heldrida",
            "html_url": "https://github.com/heldrida",
            "followers_url": "https://api.github.com/users/heldrida/followers",
            "following_url": "https://api.github.com/users/heldrida/following{/other_user}",
            "gists_url": "https://api.github.com/users/heldrida/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/heldrida/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/heldrida/subscriptions",
            "organizations_url": "https://api.github.com/users/heldrida/orgs",
            "repos_url": "https://api.github.com/users/heldrida/repos",
            "events_url": "https://api.github.com/users/heldrida/events{/privacy}",
            "received_events_url": "https://api.github.com/users/heldrida/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-15T11:39:33Z",
        "updated_at": "2023-11-15T11:46:04Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nSetting openai log to \"debug\" doesn't work.\r\n\r\nFor example, here's a snippet of a use-case:\r\n\r\n```\r\n...\r\nfrom llama_index.llms import OpenAI\r\nimport openai\r\n\r\nopenai.log = \"debug\"\r\napp = FastAPI()\r\nloader = SitemapReader()\r\n\r\nllm = OpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\r\nservice_context = ServiceContext.from_defaults(llm=llm)\r\n```\r\n\r\nHere's a Discord discussion where another community member confirms it as nothing working https://discord.com/channels/1059199217496772688/1174306631136321576\n\n### Version\n\n0.8.68\n\n### Steps to Reproduce\n\nSetup:\r\n\r\n```\r\nimport openai\r\nopenai.log = \"debug\"\r\n```\r\n\r\nMake a request and TIAS\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8924/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8924/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8923",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8923/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8923/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8923/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8923",
        "id": 1994598398,
        "node_id": "I_kwDOIWuq58524yf-",
        "number": 8923,
        "title": "[Question]: How to combine multiple vector indexes into one vector index?",
        "user": {
            "login": "SCUT-ChenBD",
            "id": 49072146,
            "node_id": "MDQ6VXNlcjQ5MDcyMTQ2",
            "avatar_url": "https://avatars.githubusercontent.com/u/49072146?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SCUT-ChenBD",
            "html_url": "https://github.com/SCUT-ChenBD",
            "followers_url": "https://api.github.com/users/SCUT-ChenBD/followers",
            "following_url": "https://api.github.com/users/SCUT-ChenBD/following{/other_user}",
            "gists_url": "https://api.github.com/users/SCUT-ChenBD/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SCUT-ChenBD/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SCUT-ChenBD/subscriptions",
            "organizations_url": "https://api.github.com/users/SCUT-ChenBD/orgs",
            "repos_url": "https://api.github.com/users/SCUT-ChenBD/repos",
            "events_url": "https://api.github.com/users/SCUT-ChenBD/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SCUT-ChenBD/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-15T11:31:06Z",
        "updated_at": "2023-11-15T12:27:59Z",
        "closed_at": "2023-11-15T12:27:59Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHow to combine multiple vector indexes into one vector index?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8923/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8923/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8922",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8922/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8922/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8922/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8922",
        "id": 1994563504,
        "node_id": "I_kwDOIWuq58524p-w",
        "number": 8922,
        "title": "[Question]: About QueryFusionRetriever",
        "user": {
            "login": "SCUT-ChenBD",
            "id": 49072146,
            "node_id": "MDQ6VXNlcjQ5MDcyMTQ2",
            "avatar_url": "https://avatars.githubusercontent.com/u/49072146?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SCUT-ChenBD",
            "html_url": "https://github.com/SCUT-ChenBD",
            "followers_url": "https://api.github.com/users/SCUT-ChenBD/followers",
            "following_url": "https://api.github.com/users/SCUT-ChenBD/following{/other_user}",
            "gists_url": "https://api.github.com/users/SCUT-ChenBD/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SCUT-ChenBD/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SCUT-ChenBD/subscriptions",
            "organizations_url": "https://api.github.com/users/SCUT-ChenBD/orgs",
            "repos_url": "https://api.github.com/users/SCUT-ChenBD/repos",
            "events_url": "https://api.github.com/users/SCUT-ChenBD/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SCUT-ChenBD/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-15T11:08:32Z",
        "updated_at": "2023-11-20T04:22:14Z",
        "closed_at": "2023-11-15T11:33:28Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIf I have five vector indexes, how can I retrieve the topk relevant texts among these five? When retrieving using **QueryFusionRetriever**, the relevant content will be returned evenly from each vector index.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8922/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8922/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8921",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8921/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8921/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8921/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8921",
        "id": 1994001538,
        "node_id": "I_kwDOIWuq58522gyC",
        "number": 8921,
        "title": "[Bug]: why use async with fusion  + FastAPI is so slow",
        "user": {
            "login": "batman-do",
            "id": 77484083,
            "node_id": "MDQ6VXNlcjc3NDg0MDgz",
            "avatar_url": "https://avatars.githubusercontent.com/u/77484083?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/batman-do",
            "html_url": "https://github.com/batman-do",
            "followers_url": "https://api.github.com/users/batman-do/followers",
            "following_url": "https://api.github.com/users/batman-do/following{/other_user}",
            "gists_url": "https://api.github.com/users/batman-do/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/batman-do/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/batman-do/subscriptions",
            "organizations_url": "https://api.github.com/users/batman-do/orgs",
            "repos_url": "https://api.github.com/users/batman-do/repos",
            "events_url": "https://api.github.com/users/batman-do/events{/privacy}",
            "received_events_url": "https://api.github.com/users/batman-do/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-15T04:09:36Z",
        "updated_at": "2023-11-15T16:14:23Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nwhy  i use fusion + fastapi follow up by document in llama-index but response very slow\r\n\r\nmy code:\r\n`import asyncio\r\n        from enum import Enum\r\n        from pathlib import Path\r\n        from typing import Dict, List, Optional, Tuple\r\n        \r\n        from llama_index import (\r\n            SimpleDirectoryReader,\r\n            VectorStoreIndex,\r\n            download_loader,\r\n        )\r\n        from llama_index.async_utils import run_async_tasks\r\n        from llama_index.constants import DEFAULT_SIMILARITY_TOP_K\r\n        from llama_index.indices.query.schema import QueryBundle\r\n        from llama_index.llms.base import ChatMessage, MessageRole\r\n        from llama_index.llms.utils import LLMType, resolve_llm\r\n        from llama_index.prompts.base import ChatPromptTemplate\r\n        from llama_index.query_engine import RetrieverQueryEngine\r\n        from llama_index.retrievers import BaseRetriever\r\n        from llama_index.schema import NodeWithScore\r\n        \r\n        QUERY_GEN_PROMPT = (\r\n            \"You are a helpful assistant that generates multiple search queries based on a \"\r\n            \"single input query in Vietnamese of Owen men's fashion brand. Generate {num_queries} search queries, one on each line, \"\r\n            \"related to the following input query:\\n\"\r\n            \"Query: {query}\\n\"\r\n            \"Queries:\\n\"\r\n        )\r\n        \r\n        TEXT_QA_SYSTEM_PROMPT = ChatMessage(\r\n            content=(\r\n                \"You are an expert Q&A system of Owen men's fashion brand that is trusted around the world.\\n\"\r\n                \"Always answer the query in Vietnamese using the provided context information, \"\r\n                \"Your answer in Vietnamese must be full of information, not contradict the following context.\\n\"\r\n            ),\r\n            role=MessageRole.SYSTEM,\r\n        )\r\n        \r\n        TEXT_QA_PROMPT_TMPL_MSGS = [\r\n            TEXT_QA_SYSTEM_PROMPT,\r\n            ChatMessage(\r\n                content=(\r\n                    \"Context information is below.\\n\"\r\n                    \"---------------------\\n\"\r\n                    \"{context_str}\\n\"\r\n                    \"---------------------\\n\"\r\n                    \"Given the context information and not prior knowledge, \"\r\n                    \"answer the query.\\n\"\r\n                    \"Some rules to follow:\\n\"\r\n                    \"1. Never directly reference the given context in your answer.\\n\"\r\n                    \"2. Avoid statements like 'Based on the context, ...' or \"\r\n                    \"'The context information ...' or anything along \"\r\n                    \"those lines.\"\r\n                    \"3. Answer MUST in Vietnamese language.\"\r\n                    \"4. DON'T reference to SOURCE data (data/...).\"\r\n                    \"\\n\\n\"\r\n                    \"Question: {query_str}\\n\"\r\n                    \"Answer: \"\r\n                ),\r\n                role=MessageRole.USER,\r\n            ),\r\n        ]\r\n        \r\n        \r\n        def index_documents(service_contenxt, path):\r\n            UnstructuredReader = download_loader(\"UnstructuredReader\")\r\n            dir_reader = SimpleDirectoryReader(\r\n                Path(path),\r\n                file_extractor={\r\n                    \".txt\": UnstructuredReader(),\r\n                },\r\n            )\r\n            documents = dir_reader.load_data()\r\n            index = VectorStoreIndex.from_documents(\r\n                documents, service_contenxt=service_contenxt, show_progress=True\r\n            )\r\n            return index\r\n        \r\n        \r\n        class FUSION_MODES(str, Enum):\r\n            RECIPROCAL_RANK = \"reciprocal_rerank\"  \r\n            SIMPLE = \"simple\"  \r\n        \r\n        \r\n        class QueryFusionRetriever(BaseRetriever):\r\n            def __init__(\r\n                self,\r\n                retrievers: List[BaseRetriever],\r\n                llm: Optional[LLMType] = \"default\",\r\n                query_gen_prompt: Optional[str] = QUERY_GEN_PROMPT,\r\n                mode: FUSION_MODES = FUSION_MODES.RECIPROCAL_RANK,\r\n                similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\r\n                num_queries: int = 4,\r\n                use_async: bool = True,\r\n                verbose: bool = False,\r\n            ) -> None:\r\n                self.num_queries = num_queries\r\n                self.query_gen_prompt = query_gen_prompt\r\n                self.similarity_top_k = similarity_top_k\r\n                self.mode = mode\r\n                self.use_async = use_async\r\n                self.verbose = verbose\r\n                self._retrievers = retrievers\r\n                self._llm = resolve_llm(llm)\r\n        \r\n            def _get_queries(self, original_query: str) -> List[str]:\r\n                prompt_str = self.query_gen_prompt.format(\r\n                    num_queries=self.num_queries - 1,\r\n                    query=original_query,\r\n                )\r\n                response = self._llm.complete(prompt_str)\r\n        \r\n                # assume LLM proper put each query on a newline\r\n                queries = response.text.split(\"\\n\")\r\n                if self.verbose:\r\n                    queries_str = \"\\n\".join(queries)\r\n                    print(f\"Generated queries:\\n{queries_str}\")\r\n                return response.text.split(\"\\n\")\r\n        \r\n            def _reciprocal_rerank_fusion(\r\n                self, results: Dict[Tuple[str, int], List[NodeWithScore]]\r\n            ) -> List[NodeWithScore]:\r\n                \"\"\"Apply reciprocal rank fusion.\r\n        \r\n                The original paper uses k=60 for best results:\r\n                https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\r\n                \"\"\"\r\n                k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\r\n                fused_scores = {}\r\n                text_to_node = {}\r\n        \r\n                # compute reciprocal rank scores\r\n                for nodes_with_scores in results.values():\r\n                    for rank, node_with_score in enumerate(\r\n                        sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)\r\n                    ):\r\n                        text = node_with_score.node.get_content()\r\n                        text_to_node[text] = node_with_score\r\n                        if text not in fused_scores:\r\n                            fused_scores[text] = 0.0\r\n                        fused_scores[text] += 1.0 / (rank + k)\r\n        \r\n                # sort results\r\n                reranked_results = dict(\r\n                    sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\r\n                )\r\n        \r\n                # adjust node scores\r\n                reranked_nodes: List[NodeWithScore] = []\r\n                for text, score in reranked_results.items():\r\n                    reranked_nodes.append(text_to_node[text])\r\n                    reranked_nodes[-1].score = score\r\n        \r\n                return reranked_nodes\r\n        \r\n            def _simple_fusion(\r\n                self, results: Dict[Tuple[str, int], List[NodeWithScore]]\r\n            ) -> List[NodeWithScore]:\r\n                \"\"\"Apply simple fusion.\"\"\"\r\n                # Use a dict to de-duplicate nodes\r\n                all_nodes: Dict[str, NodeWithScore] = {}\r\n                for nodes_with_scores in results.values():\r\n                    for node_with_score in nodes_with_scores:\r\n                        text = node_with_score.node.get_content()\r\n                        all_nodes[text] = node_with_score\r\n        \r\n                return sorted(all_nodes.values(), key=lambda x: x.score or 0.0, reverse=True)\r\n        \r\n            def _run_nested_async_queries(\r\n                self, queries: List[str]\r\n            ) -> Dict[Tuple[str, int], List[NodeWithScore]]:\r\n                tasks, task_queries = [], []\r\n                for query in queries:\r\n                    for i, retriever in enumerate(self._retrievers):\r\n                        tasks.append(retriever.aretrieve(query))\r\n                        task_queries.append(query)\r\n        \r\n                task_results = run_async_tasks(tasks)\r\n        \r\n                results = {}\r\n                for i, (query, query_result) in enumerate(zip(task_queries, task_results)):\r\n                    results[(query, i)] = query_result\r\n        \r\n                return results\r\n        \r\n            async def _run_async_queries(\r\n                self, queries: List[str]\r\n            ) -> Dict[Tuple[str, int], List[NodeWithScore]]:\r\n                tasks, task_queries = [], []\r\n                for query in queries:\r\n                    for i, retriever in enumerate(self._retrievers):\r\n                        tasks.append(retriever.aretrieve(query))\r\n                        task_queries.append(query)\r\n        \r\n                task_results = await asyncio.gather(*tasks)\r\n        \r\n                results = {}\r\n                for i, (query, query_result) in enumerate(zip(task_queries, task_results)):\r\n                    results[(query, i)] = query_result\r\n        \r\n                return results\r\n        \r\n            def _run_sync_queries(\r\n                self, queries: List[str]\r\n            ) -> Dict[Tuple[str, int], List[NodeWithScore]]:\r\n                results = {}\r\n                for query in queries:\r\n                    for i, retriever in enumerate(self._retrievers):\r\n                        results[(query, i)] = retriever.retrieve(query)\r\n        \r\n                return results\r\n        \r\n            def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\r\n                if self.num_queries > 1:\r\n                    queries = self._get_queries(query_bundle.query_str)\r\n                else:\r\n                    queries = [query_bundle.query_str]\r\n        \r\n                if self.use_async:\r\n                    results = self._run_nested_async_queries(queries)\r\n                else:\r\n                    results = self._run_sync_queries(queries)\r\n        \r\n                if self.mode == FUSION_MODES.RECIPROCAL_RANK:\r\n                    return self._reciprocal_rerank_fusion(results)[: self.similarity_top_k]\r\n                elif self.mode == FUSION_MODES.SIMPLE:\r\n                    return self._simple_fusion(results)[: self.similarity_top_k]\r\n                else:\r\n                    raise ValueError(f\"Invalid fusion mode: {self.mode}\")\r\n        \r\n            async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\r\n                if self.num_queries > 1:\r\n                    queries = self._get_queries(query_bundle.query_str)\r\n                else:\r\n                    queries = [query_bundle.query_str]\r\n        \r\n                results = await self._run_async_queries(queries)\r\n        \r\n                if self.mode == FUSION_MODES.RECIPROCAL_RANK:\r\n                    return self._reciprocal_rerank_fusion(results)[: self.similarity_top_k]\r\n                elif self.mode == FUSION_MODES.SIMPLE:\r\n                    return self._simple_fusion(results)[: self.similarity_top_k]\r\n                else:\r\n                    raise ValueError(f\"Invalid fusion mode: {self.mode}\")\r\n        \r\n        \r\n        def answer(query, retriever, post_procesor):\r\n            prompt = ChatPromptTemplate(message_templates=TEXT_QA_PROMPT_TMPL_MSGS)\r\n            if post_procesor is not None:\r\n                query_engine = RetrieverQueryEngine.from_args(\r\n                    retriever=retriever,\r\n                    node_postprocessors=[post_procesor],\r\n                    text_qa_template=prompt,\r\n                )\r\n            else:\r\n                query_engine = RetrieverQueryEngine.from_args(\r\n                    retriever=retriever,\r\n                    text_qa_template=prompt,\r\n                )\r\n            response = query_engine.aquery(query)\r\n            return response\r\n\r\n`\r\n\r\napi.py\r\n `\r\n    import os\r\n    \r\n    import openai\r\n    import utils\r\n    import uvicorn\r\n    from fastapi import FastAPI, Response, Request\r\n    from llama_index import ServiceContext\r\n    from llama_index.embeddings import OpenAIEmbedding\r\n    from llama_index.llms import OpenAI\r\n    from llama_index.response.pprint_utils import pprint_response\r\n    from llama_index.retrievers import BM25Retriever\r\n    from pydantic import BaseModel\r\n    \r\n    \r\n    class QAInput(BaseModel):\r\n        question: str\r\n    \r\n    \r\n    app = FastAPI()\r\n    \r\n    \r\n    @app.middleware(\"http\")\r\n    async def cors_handler(request: Request, call_next):\r\n        response: Response = await call_next(request)\r\n        response.headers[\"Access-Control-Allow-Credentials\"] = \"true\"\r\n        response.headers[\"Access-Control-Allow-Origin\"] = \"*\"\r\n        response.headers[\"Access-Control-Allow-Methods\"] = \"*\"\r\n        response.headers[\"Access-Control-Allow-Headers\"] = \"*\"\r\n        return response\r\n    \r\n    \r\n    @app.post(\"/QA_system/generate_answer\")\r\n    async def answer(item: QAInput):\r\n        retriever = utils.QueryFusionRetriever(\r\n            retrievers=[vector_retriever, bm25_retriever],\r\n            llm=llm,\r\n            similarity_top_k=5,\r\n            verbose=True,\r\n            num_queries=3,\r\n            use_async=True,\r\n        )\r\n        response = await utils.answer(\r\n            query=item.question,\r\n            retriever=retriever,\r\n            post_procesor=None,\r\n        )\r\n        pprint_response(response, show_source=True)\r\n        ai_message = response.response\r\n        return {\"data\": ai_message}\r\n    \r\n    \r\n    if __name__ == \"__main__\":\r\n        import nest_asyncio\r\n    \r\n        nest_asyncio.apply()\r\n    \r\n        openai.api_key = os.environ[\"OPENAI_API_KEY\"]\r\n        llm = OpenAI(\r\n            model=\"gpt-4\",\r\n            max_tokens=1024,\r\n            temperature=0,\r\n            max_retries=1,\r\n            request_timeout=50,\r\n        )\r\n        embed_model = OpenAIEmbedding()\r\n    \r\n        # service context\r\n        service_contenxt = ServiceContext.from_defaults(\r\n            chunk_size=1024, embed_model=embed_model, llm_predictor=llm, chunk_overlap=128\r\n        )\r\n        index_documents = utils.index_documents(service_contenxt, \"data/\")\r\n    \r\n        vector_retriever = index_documents.as_retriever(similarity_top_k=5)\r\n    \r\n        bm25_retriever = BM25Retriever.from_defaults(\r\n            docstore=index_documents.docstore, similarity_top_k=5\r\n        )\r\n    \r\n        uvicorn.run(app, host=..., port=...)\r\n`\r\n\r\n### Version\r\n\r\n0.8.69.post2\r\n\r\n### Steps to Reproduce\r\n\r\n![image](https://github.com/run-llama/llama_index/assets/77484083/19b5762c-46ee-4617-9536-ad03ed1445bd)\r\n\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8921/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8921/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8920",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8920/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8920/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8920/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8920",
        "id": 1993802877,
        "node_id": "PR_kwDOIWuq585fd6zz",
        "number": 8920,
        "title": "Add support for custom httpx client configuration in AzureOpenAI",
        "user": {
            "login": "aaronjimv",
            "id": 67152883,
            "node_id": "MDQ6VXNlcjY3MTUyODgz",
            "avatar_url": "https://avatars.githubusercontent.com/u/67152883?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/aaronjimv",
            "html_url": "https://github.com/aaronjimv",
            "followers_url": "https://api.github.com/users/aaronjimv/followers",
            "following_url": "https://api.github.com/users/aaronjimv/following{/other_user}",
            "gists_url": "https://api.github.com/users/aaronjimv/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/aaronjimv/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/aaronjimv/subscriptions",
            "organizations_url": "https://api.github.com/users/aaronjimv/orgs",
            "repos_url": "https://api.github.com/users/aaronjimv/repos",
            "events_url": "https://api.github.com/users/aaronjimv/events{/privacy}",
            "received_events_url": "https://api.github.com/users/aaronjimv/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-15T00:21:54Z",
        "updated_at": "2023-11-16T15:51:07Z",
        "closed_at": "2023-11-15T16:21:08Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8920",
            "html_url": "https://github.com/run-llama/llama_index/pull/8920",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8920.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8920.patch",
            "merged_at": "2023-11-15T16:21:08Z"
        },
        "body": "# Description\r\n\r\nThis PR includes the changes requested in **[Feature Request]: Support for Custom httpx Client Configuration in AzureOpenAI\u00a0#8875**\r\n\r\nA new attribute, `http_client` , was added to `azure_openai.py` constructor . If a custom one is not provided, a default httpx client will be created.\r\n\r\nFixes #8875 \r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] My changes generate no new warnings\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8920/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8920/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8919",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8919/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8919/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8919/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8919",
        "id": 1993698926,
        "node_id": "I_kwDOIWuq58521W5u",
        "number": 8919,
        "title": "[Bug]: PandasQueryEngine with llama ccp bug unexpected indent (<unknown>, line 1)",
        "user": {
            "login": "giancarloerra",
            "id": 105157948,
            "node_id": "U_kgDOBkSVPA",
            "avatar_url": "https://avatars.githubusercontent.com/u/105157948?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/giancarloerra",
            "html_url": "https://github.com/giancarloerra",
            "followers_url": "https://api.github.com/users/giancarloerra/followers",
            "following_url": "https://api.github.com/users/giancarloerra/following{/other_user}",
            "gists_url": "https://api.github.com/users/giancarloerra/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/giancarloerra/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/giancarloerra/subscriptions",
            "organizations_url": "https://api.github.com/users/giancarloerra/orgs",
            "repos_url": "https://api.github.com/users/giancarloerra/repos",
            "events_url": "https://api.github.com/users/giancarloerra/events{/privacy}",
            "received_events_url": "https://api.github.com/users/giancarloerra/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-14T22:42:02Z",
        "updated_at": "2023-11-14T23:05:01Z",
        "closed_at": "2023-11-14T22:44:08Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nWhen using PandasQueryEngine with Llama CCP, whatever prompt I always get the same error.\r\nExample code below:\r\n`from llama_index.query_engine import PandasQueryEngine\r\nquery_engine = PandasQueryEngine(df=df, verbose=True, llm=llm, service_context=service_context)\r\nresponse = query_engine.query(\r\n    \"What is the size of this dataframe?\",\r\n)\r\ndisplay(response)`\r\n\r\nand error:\r\n\r\n`Llama.generate: prefix-match hit\r\n\r\n> Pandas Instructions:\r\n```\r\n  To convert the input query \"What is the size of this dataframe?\" into executable Python code using Pandas, we can use the following approach:\r\n1. Extract the relevant information from the `df` dataframe. In this case, we need to know the number of rows and columns in the dataframe.\r\n2. Use the `shape` attribute of the dataframe to get the number of rows and columns. The shape attribute is a tuple containing the number of rows and columns in the dataframe.\r\n3. Convert the number of rows and columns into a Python expression that can be evaluated using the `eval()` function. We can use the following formula: `len(df.index) * len(df.columns)`.\r\nHere's the code:\r\n```\r\n# Extract relevant information from df dataframe\r\nrows = len(df.index)\r\ncols = len(df.columns)\r\n\r\n# Convert to Python expression for evaluation using eval() function\r\nsize = rows * cols\r\n\r\n# Print the result as a solution to the query\r\nprint(\"The size of this dataframe is:\", size)\r\n```\r\nNow, let's break down each line of the code:\r\n1. `len(df.index)` and `len\r\n```\r\n> Pandas Output: There was an error running the output as Python code. Error message: unexpected indent (<unknown>, line 1)\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/llama_index/query_engine/pandas_query_engine.py\", line 60, in default_output_processor\r\n    tree = ast.parse(output)\r\n  File \"/usr/lib/python3.10/ast.py\", line 50, in parse\r\n    return compile(source, filename, mode, flags,\r\n  File \"<unknown>\", line 1\r\n    To convert the input query \"What is the size of this dataframe?\" into executable Python code using Pandas, we can use the following approach:\r\nIndentationError: unexpected indent\r\n\r\nResponse(response='There was an error running the output as Python code. Error message: unexpected indent (<unknown>, line 1)', source_nodes=[], metadata={'pandas_instruction_str': '  To convert the input query \"What is the size of this dataframe?\" into executable Python code using Pandas, we can use the following approach:\\n1. Extract the relevant information from the `df` dataframe. In this case, we need to know the number of rows and columns in the dataframe.\\n2. Use the `shape` attribute of the dataframe to get the number of rows and columns. The shape attribute is a tuple containing the number of rows and columns in the dataframe.\\n3. Convert the number of rows and columns into a Python expression that can be evaluated using the `eval()` function. We can use the following formula: `len(df.index) * len(df.columns)`.\\nHere\\'s the code:\\n```\\n# Extract relevant information from df dataframe\\nrows = len(df.index)\\ncols = len(df.columns)\\n\\n# Convert to Python expression for evaluation using eval() function\\nsize = rows * cols\\n\\n# Print the result as a solution to the query\\nprint(\"The size of this dataframe is:\", size)\\n```\\nNow, let\\'s break down each line of the code:\\n1. `len(df.index)` and `len'})`\r\n\r\nWhatever I ask, I always receive the same error:\r\n\r\n`There was an error running the output as Python code. Error message: unexpected indent (<unknown>, line 1)`\r\n\n\n### Version\n\n0.2.18\n\n### Steps to Reproduce\n\nUsing PandasQueryEngine on an dataframe created from a CSV.\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8919/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8919/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8918",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8918/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8918/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8918/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8918",
        "id": 1993254217,
        "node_id": "I_kwDOIWuq5852zqVJ",
        "number": 8918,
        "title": "[Bug]: AzureOpenAI doesn't pass the right key to base class",
        "user": {
            "login": "cipri-tom",
            "id": 2991890,
            "node_id": "MDQ6VXNlcjI5OTE4OTA=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2991890?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cipri-tom",
            "html_url": "https://github.com/cipri-tom",
            "followers_url": "https://api.github.com/users/cipri-tom/followers",
            "following_url": "https://api.github.com/users/cipri-tom/following{/other_user}",
            "gists_url": "https://api.github.com/users/cipri-tom/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cipri-tom/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cipri-tom/subscriptions",
            "organizations_url": "https://api.github.com/users/cipri-tom/orgs",
            "repos_url": "https://api.github.com/users/cipri-tom/repos",
            "events_url": "https://api.github.com/users/cipri-tom/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cipri-tom/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 13,
        "created_at": "2023-11-14T17:50:37Z",
        "updated_at": "2023-11-22T14:31:04Z",
        "closed_at": "2023-11-16T21:06:56Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nThe interface of `AzureOpenAI` class [has changed last week](https://github.com/run-llama/llama_index/commit/60ac17c677f51a5a8c630dd87c01ecc15cfe26e5#diff-f578cf6f0a325ac8e87a0335a7a34a5f68e9523091cb86a7c9240d78a57b1061), replacing the constructor arguments: `OPENAI_API_BASE` -> `AZURE_OPENAI_ENDPOINT` and `OPENAI_API_KEY` -> `AZURE_OPENAI_API_KEY`.\r\n\r\nWhen processing these new arguments, they are incorrectly passed to base class `OpenAI`:\r\n\r\nhttps://github.com/run-llama/llama_index/commit/60ac17c677f51a5a8c630dd87c01ecc15cfe26e5#diff-f578cf6f0a325ac8e87a0335a7a34a5f68e9523091cb86a7c9240d78a57b1061R98-R99\r\n\r\nAs a result, the base class is incorrectly initialized, having an empty `api_base`. And therefore all calls fail with `APIConnectionError: Connection error.`, more specifically `UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.` .\r\n\r\nSee below for traceback\r\n\n\n### Version\n\n0.8.68\n\n### Steps to Reproduce\n\n```python\r\n\r\nfrom llama_index.llms import AzureOpenAI\r\n\r\nllm = AzureOpenAI(\r\n            engine=os.environ[\"ENGINE\"],\r\n            api_key=os.environ[\"OPENAI_API_KEY\"],\r\n            azure_endpoint=os.environ.get(\"OPENAI_API_BASE\"),\r\n            api_type=os.environ.get(\"OPENAI_API_TYPE\"),\r\n            api_version=os.environ.get(\"OPENAI_API_VERSION\"),\r\n            temperature=0.0\r\n        )\r\nllm.chat(['doesnt matter'])\r\n```\n\n### Relevant Logs/Tracbacks\n\n```shell\n{\r\n\t\"name\": \"APIConnectionError\",\r\n\t\"message\": \"Connection error.\",\r\n\t\"stack\": \"---------------------------------------------------------------------------\r\nUnsupportedProtocol                       Traceback (most recent call last)\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:66, in map_httpcore_exceptions()\r\n     65 try:\r\n---> 66     yield\r\n     67 except Exception as exc:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:228, in HTTPTransport.handle_request(self, request)\r\n    227 with map_httpcore_exceptions():\r\n--> 228     resp = self._pool.handle_request(req)\r\n    230 assert isinstance(resp.stream, typing.Iterable)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:215, in ConnectionPool.handle_request(self, request)\r\n    214 if scheme == \\\"\\\":\r\n--> 215     raise UnsupportedProtocol(\r\n    216         \\\"Request URL is missing an 'http://' or 'https://' protocol.\\\"\r\n    217     )\r\n    218 if scheme not in (\\\"http\\\", \\\"https\\\", \\\"ws\\\", \\\"wss\\\"):\r\n\r\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nUnsupportedProtocol                       Traceback (most recent call last)\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:858, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    857 try:\r\n--> 858     response = self._client.send(request, auth=self.custom_auth, stream=stream)\r\n    859     log.debug(\r\n    860         'HTTP Request: %s %s \\\"%i %s\\\"', request.method, request.url, response.status_code, response.reason_phrase\r\n    861     )\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:901, in Client.send(self, request, stream, auth, follow_redirects)\r\n    899 auth = self._build_request_auth(request, auth)\r\n--> 901 response = self._send_handling_auth(\r\n    902     request,\r\n    903     auth=auth,\r\n    904     follow_redirects=follow_redirects,\r\n    905     history=[],\r\n    906 )\r\n    907 try:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:929, in Client._send_handling_auth(self, request, auth, follow_redirects, history)\r\n    928 while True:\r\n--> 929     response = self._send_handling_redirects(\r\n    930         request,\r\n    931         follow_redirects=follow_redirects,\r\n    932         history=history,\r\n    933     )\r\n    934     try:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:966, in Client._send_handling_redirects(self, request, follow_redirects, history)\r\n    964     hook(request)\r\n--> 966 response = self._send_single_request(request)\r\n    967 try:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:1002, in Client._send_single_request(self, request)\r\n   1001 with request_context(request=request):\r\n-> 1002     response = transport.handle_request(request)\r\n   1004 assert isinstance(response.stream, SyncByteStream)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:227, in HTTPTransport.handle_request(self, request)\r\n    215 req = httpcore.Request(\r\n    216     method=request.method,\r\n    217     url=httpcore.URL(\r\n   (...)\r\n    225     extensions=request.extensions,\r\n    226 )\r\n--> 227 with map_httpcore_exceptions():\r\n    228     resp = self._pool.handle_request(req)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\r\n    154 try:\r\n--> 155     self.gen.throw(typ, value, traceback)\r\n    156 except StopIteration as exc:\r\n    157     # Suppress StopIteration *unless* it's the same exception that\r\n    158     # was passed to throw().  This prevents a StopIteration\r\n    159     # raised inside the \\\"with\\\" statement from being suppressed.\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:83, in map_httpcore_exceptions()\r\n     82 message = str(exc)\r\n---> 83 raise mapped_exc(message) from exc\r\n\r\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nUnsupportedProtocol                       Traceback (most recent call last)\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:66, in map_httpcore_exceptions()\r\n     65 try:\r\n---> 66     yield\r\n     67 except Exception as exc:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:228, in HTTPTransport.handle_request(self, request)\r\n    227 with map_httpcore_exceptions():\r\n--> 228     resp = self._pool.handle_request(req)\r\n    230 assert isinstance(resp.stream, typing.Iterable)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:215, in ConnectionPool.handle_request(self, request)\r\n    214 if scheme == \\\"\\\":\r\n--> 215     raise UnsupportedProtocol(\r\n    216         \\\"Request URL is missing an 'http://' or 'https://' protocol.\\\"\r\n    217     )\r\n    218 if scheme not in (\\\"http\\\", \\\"https\\\", \\\"ws\\\", \\\"wss\\\"):\r\n\r\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nUnsupportedProtocol                       Traceback (most recent call last)\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:858, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    857 try:\r\n--> 858     response = self._client.send(request, auth=self.custom_auth, stream=stream)\r\n    859     log.debug(\r\n    860         'HTTP Request: %s %s \\\"%i %s\\\"', request.method, request.url, response.status_code, response.reason_phrase\r\n    861     )\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:901, in Client.send(self, request, stream, auth, follow_redirects)\r\n    899 auth = self._build_request_auth(request, auth)\r\n--> 901 response = self._send_handling_auth(\r\n    902     request,\r\n    903     auth=auth,\r\n    904     follow_redirects=follow_redirects,\r\n    905     history=[],\r\n    906 )\r\n    907 try:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:929, in Client._send_handling_auth(self, request, auth, follow_redirects, history)\r\n    928 while True:\r\n--> 929     response = self._send_handling_redirects(\r\n    930         request,\r\n    931         follow_redirects=follow_redirects,\r\n    932         history=history,\r\n    933     )\r\n    934     try:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:966, in Client._send_handling_redirects(self, request, follow_redirects, history)\r\n    964     hook(request)\r\n--> 966 response = self._send_single_request(request)\r\n    967 try:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:1002, in Client._send_single_request(self, request)\r\n   1001 with request_context(request=request):\r\n-> 1002     response = transport.handle_request(request)\r\n   1004 assert isinstance(response.stream, SyncByteStream)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:227, in HTTPTransport.handle_request(self, request)\r\n    215 req = httpcore.Request(\r\n    216     method=request.method,\r\n    217     url=httpcore.URL(\r\n   (...)\r\n    225     extensions=request.extensions,\r\n    226 )\r\n--> 227 with map_httpcore_exceptions():\r\n    228     resp = self._pool.handle_request(req)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\r\n    154 try:\r\n--> 155     self.gen.throw(typ, value, traceback)\r\n    156 except StopIteration as exc:\r\n    157     # Suppress StopIteration *unless* it's the same exception that\r\n    158     # was passed to throw().  This prevents a StopIteration\r\n    159     # raised inside the \\\"with\\\" statement from being suppressed.\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:83, in map_httpcore_exceptions()\r\n     82 message = str(exc)\r\n---> 83 raise mapped_exc(message) from exc\r\n\r\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nUnsupportedProtocol                       Traceback (most recent call last)\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:66, in map_httpcore_exceptions()\r\n     65 try:\r\n---> 66     yield\r\n     67 except Exception as exc:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:228, in HTTPTransport.handle_request(self, request)\r\n    227 with map_httpcore_exceptions():\r\n--> 228     resp = self._pool.handle_request(req)\r\n    230 assert isinstance(resp.stream, typing.Iterable)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:215, in ConnectionPool.handle_request(self, request)\r\n    214 if scheme == \\\"\\\":\r\n--> 215     raise UnsupportedProtocol(\r\n    216         \\\"Request URL is missing an 'http://' or 'https://' protocol.\\\"\r\n    217     )\r\n    218 if scheme not in (\\\"http\\\", \\\"https\\\", \\\"ws\\\", \\\"wss\\\"):\r\n\r\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nUnsupportedProtocol                       Traceback (most recent call last)\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:858, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    857 try:\r\n--> 858     response = self._client.send(request, auth=self.custom_auth, stream=stream)\r\n    859     log.debug(\r\n    860         'HTTP Request: %s %s \\\"%i %s\\\"', request.method, request.url, response.status_code, response.reason_phrase\r\n    861     )\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:901, in Client.send(self, request, stream, auth, follow_redirects)\r\n    899 auth = self._build_request_auth(request, auth)\r\n--> 901 response = self._send_handling_auth(\r\n    902     request,\r\n    903     auth=auth,\r\n    904     follow_redirects=follow_redirects,\r\n    905     history=[],\r\n    906 )\r\n    907 try:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:929, in Client._send_handling_auth(self, request, auth, follow_redirects, history)\r\n    928 while True:\r\n--> 929     response = self._send_handling_redirects(\r\n    930         request,\r\n    931         follow_redirects=follow_redirects,\r\n    932         history=history,\r\n    933     )\r\n    934     try:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:966, in Client._send_handling_redirects(self, request, follow_redirects, history)\r\n    964     hook(request)\r\n--> 966 response = self._send_single_request(request)\r\n    967 try:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_client.py:1002, in Client._send_single_request(self, request)\r\n   1001 with request_context(request=request):\r\n-> 1002     response = transport.handle_request(request)\r\n   1004 assert isinstance(response.stream, SyncByteStream)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:227, in HTTPTransport.handle_request(self, request)\r\n    215 req = httpcore.Request(\r\n    216     method=request.method,\r\n    217     url=httpcore.URL(\r\n   (...)\r\n    225     extensions=request.extensions,\r\n    226 )\r\n--> 227 with map_httpcore_exceptions():\r\n    228     resp = self._pool.handle_request(req)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/contextlib.py:155, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\r\n    154 try:\r\n--> 155     self.gen.throw(typ, value, traceback)\r\n    156 except StopIteration as exc:\r\n    157     # Suppress StopIteration *unless* it's the same exception that\r\n    158     # was passed to throw().  This prevents a StopIteration\r\n    159     # raised inside the \\\"with\\\" statement from being suppressed.\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/httpx/_transports/default.py:83, in map_httpcore_exceptions()\r\n     82 message = str(exc)\r\n---> 83 raise mapped_exc(message) from exc\r\n\r\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nAPIConnectionError                        Traceback (most recent call last)\r\n/Users/a770pl/Library/CloudStorage/OneDrive-AXA/Documents/code/llm-agent/src/claim_exploration.ipynb Cell 3 line 1\r\n----> <a href='vscode-notebook-cell:/Users/a770pl/Library/CloudStorage/OneDrive-AXA/Documents/code/llm-agent/src/claim_exploration.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a> executor.run(claim_id=41)\r\n\r\nFile ~/Library/CloudStorage/OneDrive-AXA/Documents/code/llm-agent/src/evaluation/agent_executor.py:151, in AgentExecutor.run(self, contract_id, claim_id, comments, **kwargs)\r\n    148 self.agent.set_log_file(logs_file)\r\n    150 ### Run the agent\r\n--> 151 response = self.agent.chat(message = claim, hint = hint)\r\n    152 agent_decision, agent_solution = self.parse_response(response)\r\n    153 if kwargs.get(\\\"majority_vote\\\", False):\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/llama_index/callbacks/utils.py:39, in trace_method.<locals>.decorator.<locals>.wrapper(self, *args, **kwargs)\r\n     37 callback_manager = cast(CallbackManager, callback_manager)\r\n     38 with callback_manager.as_trace(trace_id):\r\n---> 39     return func(self, *args, **kwargs)\r\n\r\nFile ~/Library/CloudStorage/OneDrive-AXA/Documents/code/llm-agent/src/agent/custom_agent.py:131, in InsuranceAgent.chat(self, message, chat_history, hint)\r\n    129 ### Initialize chat history so that the first step if forced (checking the policy)\r\n    130 if not chat_history and self.policy_reader:\r\n--> 131     chat_history, query = self.initialize_chat_history(message)\r\n    132     observation = \\\"Observation: \\\" + str(self.policy_reader(query))\r\n    133     chat_history.append(ChatMessage(content=observation, role=MessageRole.USER))\r\n\r\nFile ~/Library/CloudStorage/OneDrive-AXA/Documents/code/llm-agent/src/agent/custom_agent.py:193, in InsuranceAgent.initialize_chat_history(self, input)\r\n    182 '''Initialize chat history, forcing the first step to be a policy check.'''\r\n    184 llm = AzureOpenAI(\r\n    185     engine=os.environ[\\\"ENGINE\\\"],\r\n    186     api_key=os.environ[\\\"OPENAI_API_KEY\\\"],\r\n   (...)\r\n    190     temperature=0.0\r\n    191 )\r\n--> 193 paraphrased_input = self.paraphrase_input(input, llm)\r\n    194 thought_step = self.adapt_thought_pattern(input,llm)\r\n    195 agent_response = MESSAGE_FORMAT.format(thought=thought_step, action_input = \\\"{\\\\\\\"query\\\\\\\": \\\\\\\"\\\" + paraphrased_input + \\\"\\\\\\\"}\\\")\r\n\r\nFile ~/Library/CloudStorage/OneDrive-AXA/Documents/code/llm-agent/src/agent/custom_agent.py:220, in InsuranceAgent.paraphrase_input(self, input, llm)\r\n    215 '''Paraphrase the client claim into a query for the read_policy tool.'''\r\n    216 message_list = [\r\n    217     ChatMessage(role=MessageRole.SYSTEM, content=PARAPHRASER_SYSTEM_PROMPT),\r\n    218     ChatMessage(role=MessageRole.USER, content=PARAPHRASER_INPUT_PROMPT.format(examples = PARAPHRASER_EXAMPLES, question=input)),\r\n    219 ]\r\n--> 220 result = llm.chat(message_list)\r\n    221 return result.message.content.split(\\\"<\\\")[0].strip()\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/llama_index/llms/base.py:187, in llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat(_self, messages, **kwargs)\r\n    178 with wrapper_logic(_self) as callback_manager:\r\n    179     event_id = callback_manager.on_event_start(\r\n    180         CBEventType.LLM,\r\n    181         payload={\r\n   (...)\r\n    185         },\r\n    186     )\r\n--> 187     f_return_val = f(_self, messages, **kwargs)\r\n    189     if isinstance(f_return_val, Generator):\r\n    190         # intercept the generator and add a callback to the end\r\n    191         def wrapped_gen() -> ChatResponseGen:\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/llama_index/llms/openai.py:171, in OpenAI.chat(self, messages, **kwargs)\r\n    169 else:\r\n    170     chat_fn = completion_to_chat_decorator(self._complete)\r\n--> 171 return chat_fn(messages, **kwargs)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/llama_index/llms/openai.py:224, in OpenAI._chat(self, messages, **kwargs)\r\n    222 def _chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\r\n    223     message_dicts = to_openai_message_dicts(messages)\r\n--> 224     response = self._client.chat.completions.create(\r\n    225         messages=message_dicts,\r\n    226         stream=False,\r\n    227         **self._get_model_kwargs(**kwargs),\r\n    228     )\r\n    229     openai_message = response.choices[0].message\r\n    230     message = from_openai_message(openai_message)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_utils/_utils.py:299, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)\r\n    297             msg = f\\\"Missing required argument: {quote(missing[0])}\\\"\r\n    298     raise TypeError(msg)\r\n--> 299 return func(*args, **kwargs)\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/resources/chat/completions.py:594, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\r\n    548 @required_args([\\\"messages\\\", \\\"model\\\"], [\\\"messages\\\", \\\"model\\\", \\\"stream\\\"])\r\n    549 def create(\r\n    550     self,\r\n   (...)\r\n    592     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\r\n    593 ) -> ChatCompletion | Stream[ChatCompletionChunk]:\r\n--> 594     return self._post(\r\n    595         \\\"/chat/completions\\\",\r\n    596         body=maybe_transform(\r\n    597             {\r\n    598                 \\\"messages\\\": messages,\r\n    599                 \\\"model\\\": model,\r\n    600                 \\\"frequency_penalty\\\": frequency_penalty,\r\n    601                 \\\"function_call\\\": function_call,\r\n    602                 \\\"functions\\\": functions,\r\n    603                 \\\"logit_bias\\\": logit_bias,\r\n    604                 \\\"max_tokens\\\": max_tokens,\r\n    605                 \\\"n\\\": n,\r\n    606                 \\\"presence_penalty\\\": presence_penalty,\r\n    607                 \\\"response_format\\\": response_format,\r\n    608                 \\\"seed\\\": seed,\r\n    609                 \\\"stop\\\": stop,\r\n    610                 \\\"stream\\\": stream,\r\n    611                 \\\"temperature\\\": temperature,\r\n    612                 \\\"tool_choice\\\": tool_choice,\r\n    613                 \\\"tools\\\": tools,\r\n    614                 \\\"top_p\\\": top_p,\r\n    615                 \\\"user\\\": user,\r\n    616             },\r\n    617             completion_create_params.CompletionCreateParams,\r\n    618         ),\r\n    619         options=make_request_options(\r\n    620             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\r\n    621         ),\r\n    622         cast_to=ChatCompletion,\r\n    623         stream=stream or False,\r\n    624         stream_cls=Stream[ChatCompletionChunk],\r\n    625     )\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:1055, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\r\n   1041 def post(\r\n   1042     self,\r\n   1043     path: str,\r\n   (...)\r\n   1050     stream_cls: type[_StreamT] | None = None,\r\n   1051 ) -> ResponseT | _StreamT:\r\n   1052     opts = FinalRequestOptions.construct(\r\n   1053         method=\\\"post\\\", url=path, json_data=body, files=to_httpx_files(files), **options\r\n   1054     )\r\n-> 1055     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:834, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    825 def request(\r\n    826     self,\r\n    827     cast_to: Type[ResponseT],\r\n   (...)\r\n    832     stream_cls: type[_StreamT] | None = None,\r\n    833 ) -> ResponseT | _StreamT:\r\n--> 834     return self._request(\r\n    835         cast_to=cast_to,\r\n    836         options=options,\r\n    837         stream=stream,\r\n    838         stream_cls=stream_cls,\r\n    839         remaining_retries=remaining_retries,\r\n    840     )\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:890, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    888 except Exception as err:\r\n    889     if retries > 0:\r\n--> 890         return self._retry_request(\r\n    891             options,\r\n    892             cast_to,\r\n    893             retries,\r\n    894             stream=stream,\r\n    895             stream_cls=stream_cls,\r\n    896         )\r\n    897     raise APIConnectionError(request=request) from err\r\n    899 return self._process_response(\r\n    900     cast_to=cast_to,\r\n    901     options=options,\r\n   (...)\r\n    904     stream_cls=stream_cls,\r\n    905 )\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:925, in SyncAPIClient._retry_request(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\r\n    921 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\r\n    922 # different thread if necessary.\r\n    923 time.sleep(timeout)\r\n--> 925 return self._request(\r\n    926     options=options,\r\n    927     cast_to=cast_to,\r\n    928     remaining_retries=remaining,\r\n    929     stream=stream,\r\n    930     stream_cls=stream_cls,\r\n    931 )\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:890, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    888 except Exception as err:\r\n    889     if retries > 0:\r\n--> 890         return self._retry_request(\r\n    891             options,\r\n    892             cast_to,\r\n    893             retries,\r\n    894             stream=stream,\r\n    895             stream_cls=stream_cls,\r\n    896         )\r\n    897     raise APIConnectionError(request=request) from err\r\n    899 return self._process_response(\r\n    900     cast_to=cast_to,\r\n    901     options=options,\r\n   (...)\r\n    904     stream_cls=stream_cls,\r\n    905 )\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:925, in SyncAPIClient._retry_request(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\r\n    921 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\r\n    922 # different thread if necessary.\r\n    923 time.sleep(timeout)\r\n--> 925 return self._request(\r\n    926     options=options,\r\n    927     cast_to=cast_to,\r\n    928     remaining_retries=remaining,\r\n    929     stream=stream,\r\n    930     stream_cls=stream_cls,\r\n    931 )\r\n\r\nFile ~/mambaforge/envs/llm-agent/lib/python3.11/site-packages/openai/_base_client.py:897, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\r\n    889     if retries > 0:\r\n    890         return self._retry_request(\r\n    891             options,\r\n    892             cast_to,\r\n   (...)\r\n    895             stream_cls=stream_cls,\r\n    896         )\r\n--> 897     raise APIConnectionError(request=request) from err\r\n    899 return self._process_response(\r\n    900     cast_to=cast_to,\r\n    901     options=options,\r\n   (...)\r\n    904     stream_cls=stream_cls,\r\n    905 )\r\n\r\nAPIConnectionError: Connection error.\"\r\n}\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8918/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8918/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8917",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8917/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8917/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8917/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8917",
        "id": 1993114967,
        "node_id": "PR_kwDOIWuq585fbj0X",
        "number": 8917,
        "title": "Update base.py",
        "user": {
            "login": "dennyglee",
            "id": 1446829,
            "node_id": "MDQ6VXNlcjE0NDY4Mjk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1446829?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dennyglee",
            "html_url": "https://github.com/dennyglee",
            "followers_url": "https://api.github.com/users/dennyglee/followers",
            "following_url": "https://api.github.com/users/dennyglee/following{/other_user}",
            "gists_url": "https://api.github.com/users/dennyglee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dennyglee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dennyglee/subscriptions",
            "organizations_url": "https://api.github.com/users/dennyglee/orgs",
            "repos_url": "https://api.github.com/users/dennyglee/repos",
            "events_url": "https://api.github.com/users/dennyglee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dennyglee/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-14T16:33:58Z",
        "updated_at": "2023-11-14T18:00:37Z",
        "closed_at": "2023-11-14T18:00:37Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8917",
            "html_url": "https://github.com/run-llama/llama_index/pull/8917",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8917.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8917.patch",
            "merged_at": "2023-11-14T18:00:37Z"
        },
        "body": "Update the initial comment as it was referring to the keyword table index description.\r\n\r\n# Description\r\n\r\nMinor update to the initial comment of the knowledge graph index `base.py` as it had the description of the keyword table index.\r\n\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [X] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n> Update commments section\r\n\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [X] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8917/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8917/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8916",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8916/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8916/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8916/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8916",
        "id": 1993109228,
        "node_id": "PR_kwDOIWuq585fbikZ",
        "number": 8916,
        "title": "Define Vertex examples as a Sequence",
        "user": {
            "login": "wpiekutowski",
            "id": 612,
            "node_id": "MDQ6VXNlcjYxMg==",
            "avatar_url": "https://avatars.githubusercontent.com/u/612?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wpiekutowski",
            "html_url": "https://github.com/wpiekutowski",
            "followers_url": "https://api.github.com/users/wpiekutowski/followers",
            "following_url": "https://api.github.com/users/wpiekutowski/following{/other_user}",
            "gists_url": "https://api.github.com/users/wpiekutowski/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wpiekutowski/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wpiekutowski/subscriptions",
            "organizations_url": "https://api.github.com/users/wpiekutowski/orgs",
            "repos_url": "https://api.github.com/users/wpiekutowski/repos",
            "events_url": "https://api.github.com/users/wpiekutowski/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wpiekutowski/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-14T16:30:49Z",
        "updated_at": "2023-11-14T22:33:08Z",
        "closed_at": "2023-11-14T22:33:08Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8916",
            "html_url": "https://github.com/run-llama/llama_index/pull/8916",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8916.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8916.patch",
            "merged_at": "2023-11-14T22:33:08Z"
        },
        "body": "# Description\r\n\r\nPrevious `examples` definition was implying it's an optional `ChatMessage` but in fact it should be an optional Sequence of ChatMessages. [llama_index/llms/vertex_utils.py#L164](https://github.com/run-llama/llama_index/blob/main/llama_index/llms/vertex_utils.py#L164) clearly shows this should be kind of a Sequence, because `len()` is used and later `for` to iterate over the examples. \r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\nI've used Vertex integration in my project and using `examples` was yielding errors when a list was provided.\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] My changes generate no new warnings\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8916/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8916/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8915",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8915/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8915/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8915/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8915",
        "id": 1993024796,
        "node_id": "PR_kwDOIWuq585fbQLE",
        "number": 8915,
        "title": "[version] bump to v0.8.69.post2",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-14T15:49:35Z",
        "updated_at": "2023-11-14T16:11:48Z",
        "closed_at": "2023-11-14T16:11:47Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8915",
            "html_url": "https://github.com/run-llama/llama_index/pull/8915",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8915.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8915.patch",
            "merged_at": "2023-11-14T16:11:47Z"
        },
        "body": "# Description\r\n\r\nThis is hopefully the last \ud83d\ude4f\ud83c\udffb change related to pickling/deepcopy\r\n\r\nFixes https://github.com/run-llama/llama_index/issues/8907\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8915/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8915/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8914",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8914/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8914/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8914/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8914",
        "id": 1992966542,
        "node_id": "I_kwDOIWuq5852ykGO",
        "number": 8914,
        "title": "[Bug]: .to_df() method from df_rows_program attribute 'append' not found is it '_append'?",
        "user": {
            "login": "gich2009",
            "id": 83756959,
            "node_id": "MDQ6VXNlcjgzNzU2OTU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/83756959?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gich2009",
            "html_url": "https://github.com/gich2009",
            "followers_url": "https://api.github.com/users/gich2009/followers",
            "following_url": "https://api.github.com/users/gich2009/following{/other_user}",
            "gists_url": "https://api.github.com/users/gich2009/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gich2009/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gich2009/subscriptions",
            "organizations_url": "https://api.github.com/users/gich2009/orgs",
            "repos_url": "https://api.github.com/users/gich2009/repos",
            "events_url": "https://api.github.com/users/gich2009/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gich2009/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-14T15:21:09Z",
        "updated_at": "2023-11-19T22:00:46Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nStructured data extraction with DFRowsProgram not working.\r\n\r\nWhen I remove the .to_df() statement, the program runs as expected without converting it to a dataframe\r\n\r\n### Version\r\n\r\n0.8.69.post1\r\n\r\n### Steps to Reproduce\r\n\r\nfrom llama_index.program import (\r\n    OpenAIPydanticProgram,\r\n    DFFullProgram,\r\n    DFRowsProgram,\r\n)\r\nimport pandas as pd\r\nimport os\r\nfrom dotenv import load_dotenv\r\nimport openai\r\n\r\n\r\nif __name__=='__main__':\r\n    import sys\r\n    Current_dir = os.getcwd()\r\n    Base_dir = os.path.dirname(Current_dir)\r\n    sys.path.append(Base_dir) \r\n    print(sys.path)\r\n\r\n\r\nload_dotenv()\r\n\r\nDEBUG = os.getenv('DEBUG')\r\nSECRET_KEY = os.getenv('SECRET_KEY')\r\n\r\nos.environ[\"OPENAI_API_KEY\"] = SECRET_KEY\r\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\r\n\r\n# initialize empty df\r\ndf = pd.DataFrame(\r\n    {\r\n        \"Name\": pd.Series(dtype=\"str\"),\r\n        \"Age\": pd.Series(dtype=\"int\"),\r\n        \"City\": pd.Series(dtype=\"str\"),\r\n        \"Favorite Sport\": pd.Series(dtype=\"str\"),\r\n    }\r\n)\r\n\r\n# initialize program, using existing df as schema\r\ndf_rows_program = DFRowsProgram.from_defaults(\r\n    pydantic_program_cls=OpenAIPydanticProgram, df=df\r\n)\r\n# parse text, using existing df as schema\r\nresult_obj = df_rows_program(\r\n    input_str=\"\"\"My name is John and I am 25 years old. I live in \r\n        New York and I like to play basketball. His name is \r\n        Mike and he is 30 years old. He lives in San Francisco \r\n        and he likes to play baseball. Sarah is 20 years old \r\n        and she lives in Los Angeles. She likes to play tennis.\r\n        Her name is Mary and she is 35 years old. \r\n        She lives in Chicago.\"\"\"\r\n)\r\nresult_obj.to_df(existing_df=df)   //This is the statement that throws the error, when removed, the program runs as expected\r\n\r\n\r\nprint(result_obj.__repr__())\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"\", line 52, in <module>\r\n    result_obj.to_df(existing_df=df)\r\n  File \"\", line 65, in to_df\r\n    return existing_df.append(new_df, ignore_index=True)\r\n  File \"\", line 6204, in __getattr__\r\n    return object.__getattribute__(self, name)\r\nAttributeError: 'DataFrame' object has no attribute 'append'. Did you mean: '_append'?\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8914/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8914/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8913",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8913/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8913/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8913/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8913",
        "id": 1992598402,
        "node_id": "I_kwDOIWuq5852xKOC",
        "number": 8913,
        "title": "[Question]: FnRetrieverOpenAIAgent support non-OpenAI agent\uff1f",
        "user": {
            "login": "RangRun",
            "id": 22851612,
            "node_id": "MDQ6VXNlcjIyODUxNjEy",
            "avatar_url": "https://avatars.githubusercontent.com/u/22851612?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/RangRun",
            "html_url": "https://github.com/RangRun",
            "followers_url": "https://api.github.com/users/RangRun/followers",
            "following_url": "https://api.github.com/users/RangRun/following{/other_user}",
            "gists_url": "https://api.github.com/users/RangRun/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/RangRun/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/RangRun/subscriptions",
            "organizations_url": "https://api.github.com/users/RangRun/orgs",
            "repos_url": "https://api.github.com/users/RangRun/repos",
            "events_url": "https://api.github.com/users/RangRun/events{/privacy}",
            "received_events_url": "https://api.github.com/users/RangRun/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-14T11:58:02Z",
        "updated_at": "2023-11-16T09:45:49Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [x] I have searched both the documentation and discord for an answer.\n\n### Question\n\nFnRetrieverOpenAIAgent support non-OpenAI agent\uff1f\r\nI want to uses object retriever module to retrieve non-openAI  agent, what should I do?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8913/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8913/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8912",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8912/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8912/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8912/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8912",
        "id": 1992597547,
        "node_id": "I_kwDOIWuq5852xKAr",
        "number": 8912,
        "title": "[Bug]: Kaggle Llama-index installation error",
        "user": {
            "login": "ari1337an",
            "id": 36262263,
            "node_id": "MDQ6VXNlcjM2MjYyMjYz",
            "avatar_url": "https://avatars.githubusercontent.com/u/36262263?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ari1337an",
            "html_url": "https://github.com/ari1337an",
            "followers_url": "https://api.github.com/users/ari1337an/followers",
            "following_url": "https://api.github.com/users/ari1337an/following{/other_user}",
            "gists_url": "https://api.github.com/users/ari1337an/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ari1337an/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ari1337an/subscriptions",
            "organizations_url": "https://api.github.com/users/ari1337an/orgs",
            "repos_url": "https://api.github.com/users/ari1337an/repos",
            "events_url": "https://api.github.com/users/ari1337an/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ari1337an/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-14T11:57:26Z",
        "updated_at": "2023-11-15T07:04:19Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nAfter running the llama-index pip installation command with `!pip install -q llama-index` I got the dependency incompatible error. Can someone kindly tell me how can I fix it?\r\n\r\nI have tried to downgrade nltk and the downgrade was successful, however jupyterlab installation throws other incompatible errors with jupyter-server and jupyterlab~=3.4\n\n### Version\n\n0.8.69.post1\n\n### Steps to Reproduce\n\nI am sharing the notebook https://www.kaggle.com/code/ari1337an/llama-index-issue\r\n\r\nstep1. Create a kaggle notebook\r\nstep2. run the command `!pip install -q llama-index`\r\n\n\n### Relevant Logs/Tracbacks\n\n```shell\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\njupyterlab-lsp 5.0.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\njupyterlab-lsp 5.0.0 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\r\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8912/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8912/timeline",
        "performed_via_github_app": null,
        "state_reason": "reopened"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8911",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8911/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8911/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8911/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8911",
        "id": 1992597329,
        "node_id": "I_kwDOIWuq5852xJ9R",
        "number": 8911,
        "title": "[Question]: Context retriever and memory",
        "user": {
            "login": "grauvictor",
            "id": 149517415,
            "node_id": "U_kgDOCOl0Zw",
            "avatar_url": "https://avatars.githubusercontent.com/u/149517415?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/grauvictor",
            "html_url": "https://github.com/grauvictor",
            "followers_url": "https://api.github.com/users/grauvictor/followers",
            "following_url": "https://api.github.com/users/grauvictor/following{/other_user}",
            "gists_url": "https://api.github.com/users/grauvictor/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/grauvictor/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/grauvictor/subscriptions",
            "organizations_url": "https://api.github.com/users/grauvictor/orgs",
            "repos_url": "https://api.github.com/users/grauvictor/repos",
            "events_url": "https://api.github.com/users/grauvictor/events{/privacy}",
            "received_events_url": "https://api.github.com/users/grauvictor/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-11-14T11:57:17Z",
        "updated_at": "2023-11-15T08:54:09Z",
        "closed_at": "2023-11-15T08:54:09Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have a question about `ContextChatEngine` and more precisely the `chat` method. In this method the context nodes are retrieved with:\r\n```\r\n def chat(\r\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\r\n    ) -> AgentChatResponse:\r\n        if chat_history is not None:\r\n            self._memory.set(chat_history)\r\n        self._memory.put(ChatMessage(content=message, role=\"user\"))\r\n\r\n        context_str_template, nodes = self._generate_context(message)\r\n        ...\r\n```\r\n\r\nThe memory does not seem to be used to find the context. \r\n\r\nSo, for example, in conversation:\r\n\r\nhuman: \"I'd like to make a tiramisu\"\r\nbot: bot answer\r\nhuman: \"what's the recipe?\r\n\r\nCan you confirm that the context retrieved for the last input (\"What's the recipe\") does not take memory into account, and that the nodes retrieved from the knowledge base will therefore have no direct link with the first input (\"I'd like to make a tiramisu\")? \r\n\r\nIs this intended behaviour? \r\n\r\nThank you in advance for your reply\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8911/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8911/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8910",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8910/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8910/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8910/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/8910",
        "id": 1992360853,
        "node_id": "PR_kwDOIWuq585fY_Xb",
        "number": 8910,
        "title": "Add use_jsonb flag for metadata data type in PostgresVectorStore",
        "user": {
            "login": "spreeni",
            "id": 35889034,
            "node_id": "MDQ6VXNlcjM1ODg5MDM0",
            "avatar_url": "https://avatars.githubusercontent.com/u/35889034?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/spreeni",
            "html_url": "https://github.com/spreeni",
            "followers_url": "https://api.github.com/users/spreeni/followers",
            "following_url": "https://api.github.com/users/spreeni/following{/other_user}",
            "gists_url": "https://api.github.com/users/spreeni/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/spreeni/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/spreeni/subscriptions",
            "organizations_url": "https://api.github.com/users/spreeni/orgs",
            "repos_url": "https://api.github.com/users/spreeni/repos",
            "events_url": "https://api.github.com/users/spreeni/events{/privacy}",
            "received_events_url": "https://api.github.com/users/spreeni/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-14T09:37:41Z",
        "updated_at": "2023-11-15T18:34:10Z",
        "closed_at": "2023-11-14T22:33:22Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/8910",
            "html_url": "https://github.com/run-llama/llama_index/pull/8910",
            "diff_url": "https://github.com/run-llama/llama_index/pull/8910.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/8910.patch",
            "merged_at": "2023-11-14T22:33:22Z"
        },
        "body": "# Description\r\n\r\nI implemented a flag `use_jsonb` to use `jsonb` in PostgresVectorStore as the datatype for metadata instead of `json`. As this is by default disabled, backwards compatibility does not change.\r\n\r\nFixes #8905\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8910/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 1,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8910/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/8909",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/8909/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/8909/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/8909/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/8909",
        "id": 1992290588,
        "node_id": "I_kwDOIWuq5852v_Ec",
        "number": 8909,
        "title": "[Bug]: Cannot import Nodewithscore from llama_index.data_structs",
        "user": {
            "login": "sarathsurpur",
            "id": 42287472,
            "node_id": "MDQ6VXNlcjQyMjg3NDcy",
            "avatar_url": "https://avatars.githubusercontent.com/u/42287472?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sarathsurpur",
            "html_url": "https://github.com/sarathsurpur",
            "followers_url": "https://api.github.com/users/sarathsurpur/followers",
            "following_url": "https://api.github.com/users/sarathsurpur/following{/other_user}",
            "gists_url": "https://api.github.com/users/sarathsurpur/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sarathsurpur/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sarathsurpur/subscriptions",
            "organizations_url": "https://api.github.com/users/sarathsurpur/orgs",
            "repos_url": "https://api.github.com/users/sarathsurpur/repos",
            "events_url": "https://api.github.com/users/sarathsurpur/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sarathsurpur/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-14T09:03:01Z",
        "updated_at": "2023-11-14T15:51:09Z",
        "closed_at": "2023-11-14T15:51:09Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nwhen importing the below code getting the below error\r\n`from llama_index.data_structs import NodeWithScore`\r\n\r\nError: ImportError: cannot import name 'NodeWithScore' from 'llama_index.data_structs'\n\n### Version\n\nVersion: 0.8.64.post1\n\n### Steps to Reproduce\n\nImport the above statement in specified version\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/8909/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/8909/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    }
]