[
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9326",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9326/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9326/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9326/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9326",
        "id": 2026915097,
        "node_id": "PR_kwDOIWuq585hN5M5",
        "number": 9326,
        "title": "Add MM rst API doc",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710935,
                "node_id": "LA_kwDOIWuq588AAAABc3-fFw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:M",
                "name": "size:M",
                "color": "ebb800",
                "default": false,
                "description": "This PR changes 30-99 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-05T18:54:41Z",
        "updated_at": "2023-12-05T21:05:23Z",
        "closed_at": "2023-12-05T21:05:22Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9326",
            "html_url": "https://github.com/run-llama/llama_index/pull/9326",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9326.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9326.patch",
            "merged_at": "2023-12-05T21:05:22Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9326/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9326/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9324",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9324/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9324/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9324/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9324",
        "id": 2026901067,
        "node_id": "PR_kwDOIWuq585hN18T",
        "number": 9324,
        "title": "Add is_chat_model to huggingface runtime",
        "user": {
            "login": "geodavic",
            "id": 41129492,
            "node_id": "MDQ6VXNlcjQxMTI5NDky",
            "avatar_url": "https://avatars.githubusercontent.com/u/41129492?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/geodavic",
            "html_url": "https://github.com/geodavic",
            "followers_url": "https://api.github.com/users/geodavic/followers",
            "following_url": "https://api.github.com/users/geodavic/following{/other_user}",
            "gists_url": "https://api.github.com/users/geodavic/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/geodavic/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/geodavic/subscriptions",
            "organizations_url": "https://api.github.com/users/geodavic/orgs",
            "repos_url": "https://api.github.com/users/geodavic/repos",
            "events_url": "https://api.github.com/users/geodavic/events{/privacy}",
            "received_events_url": "https://api.github.com/users/geodavic/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-05T18:49:34Z",
        "updated_at": "2023-12-06T15:57:57Z",
        "closed_at": "2023-12-06T15:57:57Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9324",
            "html_url": "https://github.com/run-llama/llama_index/pull/9324",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9324.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9324.patch",
            "merged_at": "2023-12-06T15:57:57Z"
        },
        "body": "# Description\r\n\r\nThe HuggingFaceLLM runtime does not support `is_chat_model`, meaning you can't use a HuggingFace-based chat model (e.g. Mistral Instruct) locally for RAG. This PR adds the appropriate kwarg to support this.\r\n\r\n\r\n## Type of Change\r\n\r\nAdded the `is_chat_model` to the init parameters of `HuggingFaceLLM` and fixed a bug in `_tokenizer_messages_to_prompt`.\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\nI initialized a simple RAG pipeline using a HuggingFaceLLM, passing `is_chat_model` and I received expected results.\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] My changes generate no new warnings\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9324/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9324/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9323",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9323/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9323/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9323/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9323",
        "id": 2026885446,
        "node_id": "PR_kwDOIWuq585hNygH",
        "number": 9323,
        "title": "Limit GPT4V models to supported ones",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-05T18:39:34Z",
        "updated_at": "2023-12-05T18:56:44Z",
        "closed_at": "2023-12-05T18:56:43Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9323",
            "html_url": "https://github.com/run-llama/llama_index/pull/9323",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9323.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9323.patch",
            "merged_at": "2023-12-05T18:56:43Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9323/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9323/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9322",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9322/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9322/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9322/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9322",
        "id": 2026783187,
        "node_id": "PR_kwDOIWuq585hNcEb",
        "number": 9322,
        "title": "Pass kwargs to MM Query Engine for retriever",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-05T17:35:11Z",
        "updated_at": "2023-12-05T17:48:12Z",
        "closed_at": "2023-12-05T17:48:11Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9322",
            "html_url": "https://github.com/run-llama/llama_index/pull/9322",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9322.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9322.patch",
            "merged_at": "2023-12-05T17:48:11Z"
        },
        "body": "# Description\r\n\r\nPass `kwargs` to `as_query_engine(self, **kwargs: Any)`\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9322/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9322/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9321",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9321/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9321/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9321/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9321",
        "id": 2026694895,
        "node_id": "PR_kwDOIWuq585hNILP",
        "number": 9321,
        "title": "Change MM Docs and examples",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710935,
                "node_id": "LA_kwDOIWuq588AAAABc3-fFw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:M",
                "name": "size:M",
                "color": "ebb800",
                "default": false,
                "description": "This PR changes 30-99 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-05T16:58:12Z",
        "updated_at": "2023-12-05T17:15:01Z",
        "closed_at": "2023-12-05T17:15:00Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9321",
            "html_url": "https://github.com/run-llama/llama_index/pull/9321",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9321.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9321.patch",
            "merged_at": "2023-12-05T17:15:00Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9321/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9321/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9320",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9320/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9320/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9320/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9320",
        "id": 2026579122,
        "node_id": "PR_kwDOIWuq585hMuXZ",
        "number": 9320,
        "title": "Refactor Image Store into StorageContext",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-05T16:14:40Z",
        "updated_at": "2023-12-05T16:44:55Z",
        "closed_at": "2023-12-05T16:44:54Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9320",
            "html_url": "https://github.com/run-llama/llama_index/pull/9320",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9320.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9320.patch",
            "merged_at": "2023-12-05T16:44:54Z"
        },
        "body": "# Description\r\n\r\nSupport this way for image_store\r\n```\r\ntext_store = QdrantVectorStore(\r\n    client=client, collection_name=\"text_collection\"\r\n)\r\nimage_store = QdrantVectorStore(\r\n    client=client, collection_name=\"image_collection\"\r\n)\r\nstorage_context = StorageContext.from_defaults(vector_store=text_store, image_store=image_store)\r\n\r\n# Create the MultiModal index\r\nindex = MultiModalVectorStoreIndex.from_documents(\r\n    img_documents,\r\n    storage_context=storage_context,\r\n    # image_vector_store=image_store,\r\n)\r\n```\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9320/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9320/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9319",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9319/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9319/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9319/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9319",
        "id": 2026553802,
        "node_id": "PR_kwDOIWuq585hMop8",
        "number": 9319,
        "title": "bug fix for using mmr",
        "user": {
            "login": "ofermend",
            "id": 1823547,
            "node_id": "MDQ6VXNlcjE4MjM1NDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1823547?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ofermend",
            "html_url": "https://github.com/ofermend",
            "followers_url": "https://api.github.com/users/ofermend/followers",
            "following_url": "https://api.github.com/users/ofermend/following{/other_user}",
            "gists_url": "https://api.github.com/users/ofermend/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ofermend/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ofermend/subscriptions",
            "organizations_url": "https://api.github.com/users/ofermend/orgs",
            "repos_url": "https://api.github.com/users/ofermend/repos",
            "events_url": "https://api.github.com/users/ofermend/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ofermend/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-05T16:04:48Z",
        "updated_at": "2023-12-05T16:09:43Z",
        "closed_at": "2023-12-05T16:09:43Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9319",
            "html_url": "https://github.com/run-llama/llama_index/pull/9319",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9319.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9319.patch",
            "merged_at": "2023-12-05T16:09:43Z"
        },
        "body": "# Description\r\n\r\nFix bug in using Vectara MMR\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [X] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [X] Re-run unit/integration tests\r\n- [X] Tested notebook with demo code\r\n- [X] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [X] I have performed a self-review of my own code\r\n- [X] I have made corresponding changes to the documentation\r\n- [X] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9319/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9319/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9318",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9318/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9318/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9318/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9318",
        "id": 2026525841,
        "node_id": "PR_kwDOIWuq585hMidU",
        "number": 9318,
        "title": "missing files",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-05T15:51:36Z",
        "updated_at": "2023-12-05T15:51:55Z",
        "closed_at": "2023-12-05T15:51:54Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9318",
            "html_url": "https://github.com/run-llama/llama_index/pull/9318",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9318.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9318.patch",
            "merged_at": "2023-12-05T15:51:54Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9318/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9318/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9317",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9317/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9317/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9317/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9317",
        "id": 2026498546,
        "node_id": "PR_kwDOIWuq585hMcgT",
        "number": 9317,
        "title": "[version] bump to v0.9.12",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-05T15:38:13Z",
        "updated_at": "2023-12-05T15:50:50Z",
        "closed_at": "2023-12-05T15:50:49Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9317",
            "html_url": "https://github.com/run-llama/llama_index/pull/9317",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9317.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9317.patch",
            "merged_at": "2023-12-05T15:50:49Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9317/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9317/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9316",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9316/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9316/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9316/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9316",
        "id": 2026472192,
        "node_id": "PR_kwDOIWuq585hMWvV",
        "number": 9316,
        "title": "bug fix for using mmr",
        "user": {
            "login": "ofermend",
            "id": 1823547,
            "node_id": "MDQ6VXNlcjE4MjM1NDc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1823547?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ofermend",
            "html_url": "https://github.com/ofermend",
            "followers_url": "https://api.github.com/users/ofermend/followers",
            "following_url": "https://api.github.com/users/ofermend/following{/other_user}",
            "gists_url": "https://api.github.com/users/ofermend/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ofermend/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ofermend/subscriptions",
            "organizations_url": "https://api.github.com/users/ofermend/orgs",
            "repos_url": "https://api.github.com/users/ofermend/repos",
            "events_url": "https://api.github.com/users/ofermend/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ofermend/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-05T15:25:38Z",
        "updated_at": "2023-12-05T15:34:45Z",
        "closed_at": "2023-12-05T15:34:45Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9316",
            "html_url": "https://github.com/run-llama/llama_index/pull/9316",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9316.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9316.patch",
            "merged_at": "2023-12-05T15:34:45Z"
        },
        "body": "# Description\r\n\r\nFixes an issue when passing parameters to MMR\r\n\r\n## Type of Change\r\n\r\n- [X] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [X] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [X] I have performed a self-review of my own code\r\n- [X] New and existing unit tests pass locally with my changes\r\n- [X] I ran `make format; make lint` to appease the lint gods\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9316/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9316/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9315",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9315/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9315/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9315/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9315",
        "id": 2026307088,
        "node_id": "I_kwDOIWuq5854xv4Q",
        "number": 9315,
        "title": "[Bug]: BaseOpenAIAgent.chat() got an unexpected keyword argument 'function_call'",
        "user": {
            "login": "Mracobes9",
            "id": 24497107,
            "node_id": "MDQ6VXNlcjI0NDk3MTA3",
            "avatar_url": "https://avatars.githubusercontent.com/u/24497107?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Mracobes9",
            "html_url": "https://github.com/Mracobes9",
            "followers_url": "https://api.github.com/users/Mracobes9/followers",
            "following_url": "https://api.github.com/users/Mracobes9/following{/other_user}",
            "gists_url": "https://api.github.com/users/Mracobes9/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Mracobes9/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Mracobes9/subscriptions",
            "organizations_url": "https://api.github.com/users/Mracobes9/orgs",
            "repos_url": "https://api.github.com/users/Mracobes9/repos",
            "events_url": "https://api.github.com/users/Mracobes9/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Mracobes9/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-05T14:15:27Z",
        "updated_at": "2023-12-05T14:39:21Z",
        "closed_at": "2023-12-05T14:39:21Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI tried to send message to OpeanAI chatgpt. But got error.\r\nMy source code\r\n```\r\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, SQLDatabase\r\nfrom llama_index.chat_engine.types import ChatMode\r\nfrom llama_index.llms import OpenAI, ChatMessage, MessageRole\r\nfrom llama_index.readers.database import DatabaseReader\r\n\r\nAPI_KEY = \"secret\"\r\n\r\n\r\ndef setup():\r\n    system_message = \"You are icecream shop assistant. I must help to choose icecream for buy\"\r\n    service_context = ServiceContext.from_defaults(\r\n        llm=OpenAI(\r\n            model=\"gpt-3.5-turbo\",\r\n            api_key=API_KEY,\r\n        )\r\n    )\r\n    sql_database = SQLDatabase.from_uri(\"sqlite:///data.db\")\r\n    data = DatabaseReader(\r\n        sql_database=sql_database\r\n    ).load_data(\"SELECT * FROM icecreams\")\r\n    index = VectorStoreIndex.from_documents(data, service_context=service_context)\r\n    return index.as_chat_engine(chat_mode=ChatMode.OPENAI, verbose=True, system_prompt=system_message)\r\n\r\n\r\ndef start_conversation(chat_engine):\r\n    messages = []\r\n    while True:\r\n        user_input = input(\"User: \")\r\n        messages.append(ChatMessage(content=user_input))\r\n        if user_input == \"\":\r\n            break\r\n\r\n        response = chat_engine.chat(user_input, chat_history=messages, function_call=\"query_engine_tool\")\r\n        answer = ChatMessage(\r\n                content=response.response,\r\n                role=MessageRole.ASSISTANT\r\n            )\r\n        messages.append(answer)\r\n        print(answer)\r\n\r\n\r\nif __name__ == '__main__':\r\n    engine = setup()\r\n    start_conversation(engine)\r\n```\n\n### Version\n\n0.9.11.post, 0.9.11\n\n### Steps to Reproduce\n\n1) Insert API_KEY to script variable\r\n2) launch python script\r\n3) Try to insert message to terminal\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9315/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9315/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9314",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9314/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9314/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9314/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9314",
        "id": 2025944512,
        "node_id": "I_kwDOIWuq5854wXXA",
        "number": 9314,
        "title": "[Bug]: in pii.py file in postprocessing folder",
        "user": {
            "login": "amirgholipour",
            "id": 28088865,
            "node_id": "MDQ6VXNlcjI4MDg4ODY1",
            "avatar_url": "https://avatars.githubusercontent.com/u/28088865?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/amirgholipour",
            "html_url": "https://github.com/amirgholipour",
            "followers_url": "https://api.github.com/users/amirgholipour/followers",
            "following_url": "https://api.github.com/users/amirgholipour/following{/other_user}",
            "gists_url": "https://api.github.com/users/amirgholipour/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/amirgholipour/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/amirgholipour/subscriptions",
            "organizations_url": "https://api.github.com/users/amirgholipour/orgs",
            "repos_url": "https://api.github.com/users/amirgholipour/repos",
            "events_url": "https://api.github.com/users/amirgholipour/events{/privacy}",
            "received_events_url": "https://api.github.com/users/amirgholipour/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-12-05T11:15:22Z",
        "updated_at": "2023-12-05T22:39:24Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nwhen I was testing this code for PII detection:\r\nhttps://docs.llamaindex.ai/en/stable/examples/node_postprocessor/PII.html\r\nI encounter an issue in the line  74 of this code\r\n\r\nhttps://github.com/run-llama/llama_index/blob/main/llama_index/postprocessor/pii.py\r\n\r\nhere is the error:\r\n\r\n File \"/Users/skasmani/Downloads/IBM/ibm-repo/RAG-Fusion-WatsonX/genai_venv/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 534, in _run_script\r\n    exec(code, module.__dict__)\r\n  File \"/Users/skasmani/Downloads/IBM/ibm-repo/RAG-Fusion-WatsonX/Llama_index_multitasks_app_watsonx_GA.py\", line 399, in <module>\r\n    masked_text = PII_Detector(llm,node )\r\n  File \"/Users/skasmani/Downloads/IBM/ibm-repo/RAG-Fusion-WatsonX/Llama_index_multitasks_app_watsonx_GA.py\", line 277, in PII_Detector\r\n    new_nodes = processor.postprocess_nodes([NodeWithScore(node=node)])\r\n  File \"/Users/skasmani/Downloads/IBM/ibm-repo/RAG-Fusion-WatsonX/genai_venv/lib/python3.10/site-packages/llama_index/postprocessor/types.py\", line 48, in postprocess_nodes\r\n    return self._postprocess_nodes(nodes, query_bundle)\r\n  File \"/Users/skasmani/Downloads/IBM/ibm-repo/RAG-Fusion-WatsonX/genai_venv/lib/python3.10/site-packages/llama_index/postprocessor/pii.py\", line 91, in _postprocess_nodes\r\n    new_text, mapping_info = self.mask_pii(\r\n  File \"/Users/skasmani/Downloads/IBM/ibm-repo/RAG-Fusion-WatsonX/genai_venv/lib/python3.10/site-packages/llama_index/postprocessor/pii.py\", line 78, in mask_pii\r\n    json_dict = json.loads(json_str_output)\r\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py\", line 346, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py\", line 340, in decode\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 3 column 1 (char 112)\n\n### Version\n\n0.9.11.post1\n\n### Steps to Reproduce\n\nI just used the IBM watsonx model (llama 2) as the model and has used the code which was in this link:\r\nhttps://docs.llamaindex.ai/en/stable/examples/node_postprocessor/PII.html\r\n\r\n\r\nI was able to solve it via adding line:\r\njson_str_output = json_str_output.split('\\n')[0]\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9314/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9314/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9313",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9313/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9313/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9313/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9313",
        "id": 2025930006,
        "node_id": "I_kwDOIWuq5854wT0W",
        "number": 9313,
        "title": "[Question]: Does the Cohere llama-index interface support RAG",
        "user": {
            "login": "gich2009",
            "id": 83756959,
            "node_id": "MDQ6VXNlcjgzNzU2OTU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/83756959?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/gich2009",
            "html_url": "https://github.com/gich2009",
            "followers_url": "https://api.github.com/users/gich2009/followers",
            "following_url": "https://api.github.com/users/gich2009/following{/other_user}",
            "gists_url": "https://api.github.com/users/gich2009/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/gich2009/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/gich2009/subscriptions",
            "organizations_url": "https://api.github.com/users/gich2009/orgs",
            "repos_url": "https://api.github.com/users/gich2009/repos",
            "events_url": "https://api.github.com/users/gich2009/events{/privacy}",
            "received_events_url": "https://api.github.com/users/gich2009/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-12-05T11:07:37Z",
        "updated_at": "2023-12-08T21:51:31Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI have tried to use Cohere as my llm for a RAG service context but the cohere llm doesn't seem to answer based on any retrieved information. It answers based on prior knowledge. What could be the issue?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9313/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9313/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9312",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9312/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9312/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9312/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9312",
        "id": 2025741446,
        "node_id": "I_kwDOIWuq5854vlyG",
        "number": 9312,
        "title": "[Question]: langchain integration demo ",
        "user": {
            "login": "cyliu0204",
            "id": 11987690,
            "node_id": "MDQ6VXNlcjExOTg3Njkw",
            "avatar_url": "https://avatars.githubusercontent.com/u/11987690?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cyliu0204",
            "html_url": "https://github.com/cyliu0204",
            "followers_url": "https://api.github.com/users/cyliu0204/followers",
            "following_url": "https://api.github.com/users/cyliu0204/following{/other_user}",
            "gists_url": "https://api.github.com/users/cyliu0204/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cyliu0204/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cyliu0204/subscriptions",
            "organizations_url": "https://api.github.com/users/cyliu0204/orgs",
            "repos_url": "https://api.github.com/users/cyliu0204/repos",
            "events_url": "https://api.github.com/users/cyliu0204/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cyliu0204/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-12-05T09:25:15Z",
        "updated_at": "2023-12-07T06:39:42Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nhttps://github.com/run-llama/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb \r\n\r\nwhen I run the second demo, I always get the result like follows:\r\n\r\n> \r\n> Entering new AgentExecutor chain...\r\nINFO:httpx:HTTP Request: POST https://openai-oneapi.atompai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\r\nHTTP Request: POST https://openai-oneapi.atompai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\r\nThought: Do I need to use a tool? No\r\nAI: Hi Bob! It's nice to meet you. How can I assist you today?\r\n\r\n> Finished chain.\r\nHi Bob! It's nice to meet you. How can I assist you today?\r\nINFO:httpx:HTTP Request: POST https://openai-oneapi.atompai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\r\nHTTP Request: POST https://openai-oneapi.atompai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\r\n\r\n\r\n> Entering new AgentExecutor chain...\r\nINFO:httpx:HTTP Request: POST https://openai-oneapi.atompai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\r\nHTTP Request: POST https://openai-oneapi.atompai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\r\nThought: Do I need to use a tool? Yes\r\nAction: Named Entity Recognition (NER)\r\nAction Input: \"what's my bob's age?\"\r\nObservation: Named Entity Recognition (NER) is not a valid tool, try one of [].\r\nThought:INFO:httpx:HTTP Request: POST https://openai-oneapi.atompai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\r\nHTTP Request: POST https://openai-oneapi.atompai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\r\nDo I need to use a tool? No\r\nAI: I'm sorry, I don't have access to personal information such as Bob's age.\r\n\r\n> Finished chain.\r\nI'm sorry, I don't have access to personal information such as Bob's age.\r\n\r\n\r\nI debug it and get it running normally by comment the  return_source=True line,\uff1b\r\nand it was this line caused this issue:\r\n`source_messages = [\r\n                    m for id, m in self.id_to_message.items() if id in source_ids\r\n                ]`\r\n\r\n can anyone give any explaination about this ?  ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9312/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9312/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9311",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9311/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9311/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9311/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9311",
        "id": 2025713472,
        "node_id": "I_kwDOIWuq5854ve9A",
        "number": 9311,
        "title": "[Bug]: Response is Empty",
        "user": {
            "login": "shivanandmn",
            "id": 21271698,
            "node_id": "MDQ6VXNlcjIxMjcxNjk4",
            "avatar_url": "https://avatars.githubusercontent.com/u/21271698?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shivanandmn",
            "html_url": "https://github.com/shivanandmn",
            "followers_url": "https://api.github.com/users/shivanandmn/followers",
            "following_url": "https://api.github.com/users/shivanandmn/following{/other_user}",
            "gists_url": "https://api.github.com/users/shivanandmn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shivanandmn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shivanandmn/subscriptions",
            "organizations_url": "https://api.github.com/users/shivanandmn/orgs",
            "repos_url": "https://api.github.com/users/shivanandmn/repos",
            "events_url": "https://api.github.com/users/shivanandmn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shivanandmn/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-12-05T09:14:43Z",
        "updated_at": "2023-12-05T19:39:41Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nhttps://github.com/run-llama/llama_index/blob/b7188a3423e094b509cda5845cbc04e83f99f144/llama_index/llms/huggingface.py#L278 on this line you are trimming predicted tokens with `inputs[\"input_ids\"].size(1)`, but that is trimming all the predictions and hence I am getting empty response\n\n### Version\n\n0.8.68\n\n### Steps to Reproduce\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\r\nfrom llama_index.llms import HuggingFaceLLM\r\n\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\r\nhug_llm = HuggingFaceLLM(model=model, tokenizer=tokenizer)\r\n\r\n\r\nprompt = \\\r\n\"\"\"'Answer based on context:\\n\\nI learned some useful things at Interleaf, though they were mostly about what not to do. I learned that it\\'s better for technology companies to be run by product people than sales people (though sales is a real skill and people who are good at it are really good at it), that it leads to bugs when code is edited by too many people, that cheap office space is no bargain if it\\'s depressing, that planned meetings are inferior to corridor conversations, that big, bureaucratic customers are a dangerous source of money, and that there\\'s not much overlap between conventional office hours and the optimal time for hacking, or conventional offices and the optimal place for it.\\n\\nBut the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that the low end eats the high end: that it\\'s good to be the \"entry level\" option, even though that will be less prestigious, because if you\\'re not, someone else will be, and will squash you against the ceiling. Which in turn means that prestige is a danger sign.\\n\\nWhat happened at interleaf?'\r\n\"\"\"\r\nhug_llm.complete(prompt)\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9311/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9311/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9310",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9310/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9310/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9310/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9310",
        "id": 2025636885,
        "node_id": "PR_kwDOIWuq585hJePO",
        "number": 9310,
        "title": "Updated Clarifai functions with new clarifai-python SDK",
        "user": {
            "login": "mogith-pn",
            "id": 143642606,
            "node_id": "U_kgDOCI_P7g",
            "avatar_url": "https://avatars.githubusercontent.com/u/143642606?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mogith-pn",
            "html_url": "https://github.com/mogith-pn",
            "followers_url": "https://api.github.com/users/mogith-pn/followers",
            "following_url": "https://api.github.com/users/mogith-pn/following{/other_user}",
            "gists_url": "https://api.github.com/users/mogith-pn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mogith-pn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mogith-pn/subscriptions",
            "organizations_url": "https://api.github.com/users/mogith-pn/orgs",
            "repos_url": "https://api.github.com/users/mogith-pn/repos",
            "events_url": "https://api.github.com/users/mogith-pn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mogith-pn/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-12-05T08:48:07Z",
        "updated_at": "2023-12-11T20:03:44Z",
        "closed_at": "2023-12-11T20:03:44Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9310",
            "html_url": "https://github.com/run-llama/llama_index/pull/9310",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9310.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9310.patch",
            "merged_at": "2023-12-11T20:03:44Z"
        },
        "body": "* Updated functions\r\n\r\n* added inference params to model predict\r\n\r\n* modified embed size\r\n\r\n* modified pat args\r\n\r\n# Description\r\n\r\nThe aim of this change is to utilize clarifai python SDK functions in the class. \r\n1. Updated bulk predict with SDK funciton.\r\n2. updated embeddings and llm class to support CLARIFAI PAT as an arg.\r\n3. Updated notebooks with new examples.\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n\r\n- [x] Tested new functions in the notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review as well as peer-review of my code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9310/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9310/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9309",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9309/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9309/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9309/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9309",
        "id": 2025417638,
        "node_id": "I_kwDOIWuq5854uWum",
        "number": 9309,
        "title": "[Question]: Using Local LLM for Image to Image instead of openai",
        "user": {
            "login": "BakingBrains",
            "id": 51019420,
            "node_id": "MDQ6VXNlcjUxMDE5NDIw",
            "avatar_url": "https://avatars.githubusercontent.com/u/51019420?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BakingBrains",
            "html_url": "https://github.com/BakingBrains",
            "followers_url": "https://api.github.com/users/BakingBrains/followers",
            "following_url": "https://api.github.com/users/BakingBrains/following{/other_user}",
            "gists_url": "https://api.github.com/users/BakingBrains/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BakingBrains/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BakingBrains/subscriptions",
            "organizations_url": "https://api.github.com/users/BakingBrains/orgs",
            "repos_url": "https://api.github.com/users/BakingBrains/repos",
            "events_url": "https://api.github.com/users/BakingBrains/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BakingBrains/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-12-05T07:00:48Z",
        "updated_at": "2023-12-12T11:48:51Z",
        "closed_at": "2023-12-05T12:48:47Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI was trying out Image to Image Retrieval, can anyone suggest how can I use local LLM in place of openai for Image to Image Retrieval.\r\n\r\nRegards",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9309/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9309/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9308",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9308/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9308/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9308/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9308",
        "id": 2025408928,
        "node_id": "I_kwDOIWuq5854uUmg",
        "number": 9308,
        "title": "[Feature Request]: Can we have the feature to tackle acronyms",
        "user": {
            "login": "amitguptadumka",
            "id": 39477047,
            "node_id": "MDQ6VXNlcjM5NDc3MDQ3",
            "avatar_url": "https://avatars.githubusercontent.com/u/39477047?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/amitguptadumka",
            "html_url": "https://github.com/amitguptadumka",
            "followers_url": "https://api.github.com/users/amitguptadumka/followers",
            "following_url": "https://api.github.com/users/amitguptadumka/following{/other_user}",
            "gists_url": "https://api.github.com/users/amitguptadumka/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/amitguptadumka/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/amitguptadumka/subscriptions",
            "organizations_url": "https://api.github.com/users/amitguptadumka/orgs",
            "repos_url": "https://api.github.com/users/amitguptadumka/repos",
            "events_url": "https://api.github.com/users/amitguptadumka/events{/privacy}",
            "received_events_url": "https://api.github.com/users/amitguptadumka/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-05T06:53:44Z",
        "updated_at": "2023-12-05T07:01:39Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nOne problem i am facing during retrieval is that when the query has the acronym of a word it does not work as expected.\r\nFor example if my query is \"What is GBQ?\" and my document has no word like GBQ but it has 'Google Big Query' it fails to understand.\r\nExpectation is it should understand abbreviated words and fetch similar results for both query \"What is GBQ?\" or \"What is Google Big Query?\".\r\nSo say even if we have a mapping of acronyms-->Expansion i could not find any ways to achieve this? I have many such scenarios and this is causing lot of noise in retrieval.\r\nA feature to handle this would really be a big step.\n\n### Reason\n\nI have many such scenarios and this is causing lot of noise in retrieval hence decreasing the accuracy.\r\nA feature to handle this would really be a big step.\n\n### Value of Feature\n\nThis would be a great value add because in real world queries deal with lot of acronyms and our document might not be in that form. It will increase the accuracy and exclude the patchwork we need to apply for cleanup query which is an overhead and it comes with computational cost.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9308/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9308/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9307",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9307/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9307/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9307/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9307",
        "id": 2025338688,
        "node_id": "PR_kwDOIWuq585hIcUl",
        "number": 9307,
        "title": "add google drive ingestion example ",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-05T05:51:29Z",
        "updated_at": "2023-12-05T15:33:16Z",
        "closed_at": "2023-12-05T15:33:14Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9307",
            "html_url": "https://github.com/run-llama/llama_index/pull/9307",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9307.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9307.patch",
            "merged_at": "2023-12-05T15:33:14Z"
        },
        "body": "here we show how to build a \"live\" rag pipeline from google drive.\r\n\r\nwe first load/index data using ingestion pipeline.\r\n\r\nwe then edit google docs, save it, and then reload our pipeline to propagate incremental updates ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9307/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9307/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9306",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9306/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9306/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9306/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9306",
        "id": 2025217391,
        "node_id": "PR_kwDOIWuq585hICHf",
        "number": 9306,
        "title": "Solve Empty Text Vector Store with MM vector stores. Update MM docs",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-05T03:41:18Z",
        "updated_at": "2023-12-05T03:49:06Z",
        "closed_at": "2023-12-05T03:49:06Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9306",
            "html_url": "https://github.com/run-llama/llama_index/pull/9306",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9306.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9306.patch",
            "merged_at": "2023-12-05T03:49:05Z"
        },
        "body": "# Description\r\n\r\n* fix the MM vector store with only image store issue. Enable user only query image vector index without text vector index\r\n* update MM doc to reflect in the guideline. so user can only query image vector index\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9306/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9306/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9305",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9305/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9305/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9305/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9305",
        "id": 2025190196,
        "node_id": "I_kwDOIWuq5854tfM0",
        "number": 9305,
        "title": "[Question]: How to use cosine similarity mode in VectorIndexRetriever?",
        "user": {
            "login": "pavansandeep2910",
            "id": 55790895,
            "node_id": "MDQ6VXNlcjU1NzkwODk1",
            "avatar_url": "https://avatars.githubusercontent.com/u/55790895?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pavansandeep2910",
            "html_url": "https://github.com/pavansandeep2910",
            "followers_url": "https://api.github.com/users/pavansandeep2910/followers",
            "following_url": "https://api.github.com/users/pavansandeep2910/following{/other_user}",
            "gists_url": "https://api.github.com/users/pavansandeep2910/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pavansandeep2910/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pavansandeep2910/subscriptions",
            "organizations_url": "https://api.github.com/users/pavansandeep2910/orgs",
            "repos_url": "https://api.github.com/users/pavansandeep2910/repos",
            "events_url": "https://api.github.com/users/pavansandeep2910/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pavansandeep2910/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-05T03:12:27Z",
        "updated_at": "2023-12-05T03:59:59Z",
        "closed_at": "2023-12-05T03:59:58Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI created an index which is of instance VectorStoreIndex with embeddings using the OpenAIEmbedding model. I am using a retriever which is of instance VectorIndexRetriever to get the top k similar nodes(which compares the embeddings). what is the default mode the retriever uses to get similar nodes? How can I use the cosine similarity mode if it does not use it as the default mode?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9305/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9305/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9304",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9304/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9304/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9304/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9304",
        "id": 2025066639,
        "node_id": "PR_kwDOIWuq585hHimP",
        "number": 9304,
        "title": "support python 3.12",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-05T00:44:51Z",
        "updated_at": "2023-12-05T01:38:00Z",
        "closed_at": "2023-12-05T01:37:59Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9304",
            "html_url": "https://github.com/run-llama/llama_index/pull/9304",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9304.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9304.patch",
            "merged_at": "2023-12-05T01:37:59Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes https://github.com/run-llama/llama_index/issues/9292\r\n\r\nFixes https://github.com/run-llama/llama_index/issues/8153\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9304/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9304/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9303",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9303/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9303/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9303/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9303",
        "id": 2025013156,
        "node_id": "PR_kwDOIWuq585hHXFB",
        "number": 9303,
        "title": "Init Qdrant Metadata filter",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-04T23:48:59Z",
        "updated_at": "2023-12-05T01:58:31Z",
        "closed_at": "2023-12-05T01:58:30Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9303",
            "html_url": "https://github.com/run-llama/llama_index/pull/9303",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9303.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9303.patch",
            "merged_at": "2023-12-05T01:58:30Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9303/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9303/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9302",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9302/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9302/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9302/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9302",
        "id": 2024747888,
        "node_id": "PR_kwDOIWuq585hGi1n",
        "number": 9302,
        "title": "Init Weaviate Meta Data filter",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710935,
                "node_id": "LA_kwDOIWuq588AAAABc3-fFw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:M",
                "name": "size:M",
                "color": "ebb800",
                "default": false,
                "description": "This PR changes 30-99 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-04T21:10:41Z",
        "updated_at": "2023-12-04T21:58:07Z",
        "closed_at": "2023-12-04T21:58:06Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9302",
            "html_url": "https://github.com/run-llama/llama_index/pull/9302",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9302.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9302.patch",
            "merged_at": "2023-12-04T21:58:06Z"
        },
        "body": "# Description\r\n\r\nAdd Metadata filter for weaviate\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9302/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9302/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9301",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9301/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9301/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9301/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9301",
        "id": 2024677470,
        "node_id": "PR_kwDOIWuq585hGS5n",
        "number": 9301,
        "title": "add resuse_client option",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-04T20:32:54Z",
        "updated_at": "2023-12-04T22:30:42Z",
        "closed_at": "2023-12-04T22:30:40Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9301",
            "html_url": "https://github.com/run-llama/llama_index/pull/9301",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9301.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9301.patch",
            "merged_at": "2023-12-04T22:30:40Z"
        },
        "body": "# Description\r\n\r\nFrequent LLM/Embddings clients occur during async operation. Hypothesis is that this is due to sharing a single OpenAI client across all API calls.\r\n\r\nFix is to add an option that lets a new client be created for each API call\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9301/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9301/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9300",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9300/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9300/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9300/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9300",
        "id": 2024432959,
        "node_id": "PR_kwDOIWuq585hFdKa",
        "number": 9300,
        "title": "Allow `delete_kwargs` in vector store, use them in WeaviateVectorStore",
        "user": {
            "login": "elo-siema",
            "id": 17870661,
            "node_id": "MDQ6VXNlcjE3ODcwNjYx",
            "avatar_url": "https://avatars.githubusercontent.com/u/17870661?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/elo-siema",
            "html_url": "https://github.com/elo-siema",
            "followers_url": "https://api.github.com/users/elo-siema/followers",
            "following_url": "https://api.github.com/users/elo-siema/following{/other_user}",
            "gists_url": "https://api.github.com/users/elo-siema/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/elo-siema/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/elo-siema/subscriptions",
            "organizations_url": "https://api.github.com/users/elo-siema/orgs",
            "repos_url": "https://api.github.com/users/elo-siema/repos",
            "events_url": "https://api.github.com/users/elo-siema/events{/privacy}",
            "received_events_url": "https://api.github.com/users/elo-siema/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-04T18:06:12Z",
        "updated_at": "2023-12-04T22:32:42Z",
        "closed_at": "2023-12-04T22:32:42Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9300",
            "html_url": "https://github.com/run-llama/llama_index/pull/9300",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9300.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9300.patch",
            "merged_at": "2023-12-04T22:32:42Z"
        },
        "body": "# Description\r\n\r\nThis is something I needed to patch in order to implement a feature in an application I'm working on.\r\n\r\nUse case: Protect documents not owned by a user from deletion. Ownership info was stored in Weaviate metadata.\r\n\r\n\"filter\" param follows convention from the `query` method in `weaviate.py`.\r\n\r\nMypy complains:\r\n```\r\nllama_index/vector_stores/weaviate.py:214: error: List item 0 has incompatible type \"Dict[str, Sequence[str]]\"; expected \"str\"  [list-item]\r\n```\r\nI have silenced it as weaviate client lib says this:\r\n\r\n```python\r\n    def with_where(self, content: dict) -> \"GetBuilder\":\r\n        \"\"\"\r\n        Set `where` filter.\r\n\r\n        Parameters\r\n        ----------\r\n        content : dict\r\n            The content of the `where` filter to set. See examples below.\r\n\r\n        Examples\r\n        --------\r\n        The `content` prototype is like this:\r\n\r\n        >>> content = {\r\n        ...     'operator': '<operator>',\r\n        ...     'operands': [\r\n        ...         {\r\n        ...             'path': [path],\r\n        ...             'operator': '<operator>'\r\n        ...             '<valueType>': <value>\r\n        ...         },\r\n        ...         {\r\n        ...             'path': [<matchPath>],\r\n        ...             'operator': '<operator>',\r\n        ...             '<valueType>': <value>\r\n        ...         }\r\n        ...     ]\r\n        ... }\r\n\r\n```\r\n\r\nWhich matches what we have here.\r\n\r\nIf some testing / refinement is needed, please let me know.\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\nPasses tests in my app. Weaviate vector store in llamaindex doesn't seem to have extensive tests anyway at the moment?\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9300/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9300/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9299",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9299/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9299/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9299/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9299",
        "id": 2024384811,
        "node_id": "I_kwDOIWuq5854qakr",
        "number": 9299,
        "title": "[Bug]:  i got function _LlamaContext.__del__ at 0x7fdf0ef156c0 error ",
        "user": {
            "login": "papipsycho",
            "id": 20132010,
            "node_id": "MDQ6VXNlcjIwMTMyMDEw",
            "avatar_url": "https://avatars.githubusercontent.com/u/20132010?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/papipsycho",
            "html_url": "https://github.com/papipsycho",
            "followers_url": "https://api.github.com/users/papipsycho/followers",
            "following_url": "https://api.github.com/users/papipsycho/following{/other_user}",
            "gists_url": "https://api.github.com/users/papipsycho/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/papipsycho/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/papipsycho/subscriptions",
            "organizations_url": "https://api.github.com/users/papipsycho/orgs",
            "repos_url": "https://api.github.com/users/papipsycho/repos",
            "events_url": "https://api.github.com/users/papipsycho/events{/privacy}",
            "received_events_url": "https://api.github.com/users/papipsycho/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-04T17:37:18Z",
        "updated_at": "2023-12-04T17:52:24Z",
        "closed_at": "2023-12-04T17:52:23Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nwhen i run  this code : \r\n\r\n```\r\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\r\n\r\n\r\nfrom llama_index.llms import LlamaCPP\r\n\r\nllm = LlamaCPP(\r\n    model_path='mistral-7b-instruct-v0.1.Q4_K_M.gguf',\r\n    temperature=0.1,\r\n    max_new_tokens=256,\r\n    # llama2 has a context window of 4096 tokens,\r\n    # but we set it lower to allow for some wiggle room\r\n    context_window=3900,\r\n    generate_kwargs={},\r\n    # All to GPU\r\n    model_kwargs={\"n_gpu_layers\": -1},\r\n    # transform inputs into Llama2 format\r\n    messages_to_prompt=\"\",\r\n    completion_to_prompt=\"\",\r\n    verbose=True,\r\n)\r\n\r\nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\")\r\n\r\ndocuments = SimpleDirectoryReader(\"ingest\").load_data()\r\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\r\n```\r\n\r\n\r\ni got this error \r\nException ignored in: <function _LlamaContext.__del__ at 0x7fdf0ef156c0>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.11/site-packages/llama_cpp/llama.py\", line 422, in __del__\r\nTypeError: 'NoneType' object is not callable\r\nException ignored in: <function _LlamaModel.__del__ at 0x7fdf0ef14680>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.11/site-packages/llama_cpp/llama.py\", line 240, in __del__\r\nTypeError: 'NoneType' object is not callable\r\nException ignored in: <function _LlamaBatch.__del__ at 0x7fdf0ef16b60>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.11/site-packages/llama_cpp/llama.py\", line 670, in __del__\r\nTypeError: 'NoneType' object is not callable\r\n\r\ni used the version 0.2.20 of llama_cpp on python 3.11\n\n### Version\n\n0.9.11.post1\n\n### Steps to Reproduce\n\njust ran the code above\n\n### Relevant Logs/Tracbacks\n\n```shell\npython3.11 index.py\r\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\r\nllama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\r\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   15:              blk.1.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   33:              blk.3.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   43:            blk.4.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   51:              blk.5.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   69:              blk.7.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   79:            blk.8.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   87:              blk.9.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor   97:           blk.10.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  105:             blk.11.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  123:             blk.13.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  133:           blk.14.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  151:           blk.16.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  159:             blk.17.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  177:             blk.19.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  195:             blk.21.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  205:           blk.22.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  213:             blk.23.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  231:             blk.25.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  232:           blk.25.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  241:           blk.26.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  249:             blk.27.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  267:             blk.29.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\r\nllama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  285:             blk.31.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\r\nllama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\r\nllama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\r\nllama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\r\nllama_model_loader: - kv   0:                       general.architecture str              = llama\r\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\r\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\r\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\r\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\r\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\r\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\r\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\r\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\r\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\r\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\r\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\r\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\r\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\r\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\r\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\r\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\r\nllama_model_loader: - type  f32:   65 tensors\r\nllama_model_loader: - type q4_K:  193 tensors\r\nllama_model_loader: - type q6_K:   33 tensors\r\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\r\nllm_load_print_meta: format           = GGUF V2\r\nllm_load_print_meta: arch             = llama\r\nllm_load_print_meta: vocab type       = SPM\r\nllm_load_print_meta: n_vocab          = 32000\r\nllm_load_print_meta: n_merges         = 0\r\nllm_load_print_meta: n_ctx_train      = 32768\r\nllm_load_print_meta: n_embd           = 4096\r\nllm_load_print_meta: n_head           = 32\r\nllm_load_print_meta: n_head_kv        = 8\r\nllm_load_print_meta: n_layer          = 32\r\nllm_load_print_meta: n_rot            = 128\r\nllm_load_print_meta: n_gqa            = 4\r\nllm_load_print_meta: f_norm_eps       = 0.0e+00\r\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\r\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\r\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\r\nllm_load_print_meta: n_ff             = 14336\r\nllm_load_print_meta: rope scaling     = linear\r\nllm_load_print_meta: freq_base_train  = 10000.0\r\nllm_load_print_meta: freq_scale_train = 1\r\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\r\nllm_load_print_meta: rope_finetuned   = unknown\r\nllm_load_print_meta: model type       = 7B\r\nllm_load_print_meta: model ftype      = mostly Q4_K - Medium\r\nllm_load_print_meta: model params     = 7.24 B\r\nllm_load_print_meta: model size       = 4.07 GiB (4.83 BPW)\r\nllm_load_print_meta: general.name   = mistralai_mistral-7b-instruct-v0.1\r\nllm_load_print_meta: BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta: UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors: ggml ctx size =    0.11 MiB\r\nllm_load_tensors: mem required  = 4165.47 MiB\r\n...............................................................................................\r\nllama_new_context_with_model: n_ctx      = 3900\r\nllama_new_context_with_model: freq_base  = 10000.0\r\nllama_new_context_with_model: freq_scale = 1\r\nllama_new_context_with_model: kv self size  =  487.50 MiB\r\nllama_build_graph: non-view tensors processed: 740/740\r\nllama_new_context_with_model: compute buffer total size = 278.43 MiB\r\nAVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\r\nException ignored in: <function _LlamaContext.__del__ at 0x7fcd77a856c0>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.11/site-packages/llama_cpp/llama.py\", line 422, in __del__\r\nTypeError: 'NoneType' object is not callable\r\nException ignored in: <function _LlamaModel.__del__ at 0x7fcd77a84680>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.11/site-packages/llama_cpp/llama.py\", line 240, in __del__\r\nTypeError: 'NoneType' object is not callable\r\nException ignored in: <function _LlamaBatch.__del__ at 0x7fcd77a86b60>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.11/site-packages/llama_cpp/llama.py\", line 670, in __del__\r\nTypeError: 'NoneType' object is not callable\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9299/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9299/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9298",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9298/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9298/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9298/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9298",
        "id": 2024190865,
        "node_id": "PR_kwDOIWuq585hEoMb",
        "number": 9298,
        "title": "Handling the case where the response does not have a \"usage\" in the token counts function",
        "user": {
            "login": "vldvasi",
            "id": 46647774,
            "node_id": "MDQ6VXNlcjQ2NjQ3Nzc0",
            "avatar_url": "https://avatars.githubusercontent.com/u/46647774?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vldvasi",
            "html_url": "https://github.com/vldvasi",
            "followers_url": "https://api.github.com/users/vldvasi/followers",
            "following_url": "https://api.github.com/users/vldvasi/following{/other_user}",
            "gists_url": "https://api.github.com/users/vldvasi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vldvasi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vldvasi/subscriptions",
            "organizations_url": "https://api.github.com/users/vldvasi/orgs",
            "repos_url": "https://api.github.com/users/vldvasi/repos",
            "events_url": "https://api.github.com/users/vldvasi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vldvasi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-04T15:57:37Z",
        "updated_at": "2023-12-04T22:42:22Z",
        "closed_at": "2023-12-04T22:42:22Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9298",
            "html_url": "https://github.com/run-llama/llama_index/pull/9298",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9298.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9298.patch",
            "merged_at": "2023-12-04T22:42:22Z"
        },
        "body": "# Description\r\n\r\nTo code is throwing an exception for \"usage\" is None when used with AWS Bedrock APIs , QueryFusionRetriever and CondensePlusContextChatEngine . Adjusted the logic to handle cases where \"usage\" is not be present.\r\n\r\nFixes # (issue) https://github.com/run-llama/llama_index/issues/9276\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n- [x] I have performed a self-review of my own code\r\n- [x] My changes generate no new warnings\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9298/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9298/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9297",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9297/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9297/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9297/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9297",
        "id": 2024042340,
        "node_id": "I_kwDOIWuq5854pG9k",
        "number": 9297,
        "title": "[Bug]: Metadata not included in node text",
        "user": {
            "login": "GabrieleGioetto",
            "id": 35202396,
            "node_id": "MDQ6VXNlcjM1MjAyMzk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/35202396?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/GabrieleGioetto",
            "html_url": "https://github.com/GabrieleGioetto",
            "followers_url": "https://api.github.com/users/GabrieleGioetto/followers",
            "following_url": "https://api.github.com/users/GabrieleGioetto/following{/other_user}",
            "gists_url": "https://api.github.com/users/GabrieleGioetto/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/GabrieleGioetto/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/GabrieleGioetto/subscriptions",
            "organizations_url": "https://api.github.com/users/GabrieleGioetto/orgs",
            "repos_url": "https://api.github.com/users/GabrieleGioetto/repos",
            "events_url": "https://api.github.com/users/GabrieleGioetto/events{/privacy}",
            "received_events_url": "https://api.github.com/users/GabrieleGioetto/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-04T14:49:18Z",
        "updated_at": "2023-12-04T15:04:05Z",
        "closed_at": "2023-12-04T15:04:05Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI would like to split a document and include for each split at the top of the text the formatted `metadata` string.\r\nMy desired output would be\r\n```\r\n{metadata_key}:{value}\r\n\r\n{node_content}\r\n```\r\n\r\nWhen creating a `TextNode` from splitting a `Document`, the `metadata` of the `Document` are not inserted into the `TextNode` text, even if the `metadata` is present among the node properties.\n\n### Version\n\n0.9.8.post1\n\n### Steps to Reproduce\n\n```\r\nfrom llama_index import Document\r\nfrom llama_index.schema import TextNode\r\nfrom llama_index.text_splitter import SentenceSplitter\r\n\r\ntext_splitter = SentenceSplitter(\r\n    chunk_size=200, chunk_overlap=10, include_metadata=True\r\n)\r\n\r\ntext = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus massa quam, posuere vel rhoncus non, venenatis id quam. Pellentesque ut purus sit amet magna fringilla dictum at vitae orci. Etiam in commodo sapien, quis suscipit ex. Donec commodo est ut est viverra mollis. Nam ullamcorper nunc purus, vel rutrum est tincidunt non. Vivamus venenatis maximus dui a faucibus. Aliquam non auctor velit. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sit amet metus felis. Aliquam non lorem non magna semper gravida. Morbi feugiat lobortis nulla non rhoncus.\r\n\r\nProin in fermentum leo. Sed et mauris ac dui finibus dignissim. Donec bibendum nisi ligula, finibus fringilla diam laoreet et. Pellentesque dui dolor, accumsan id facilisis ut, facilisis vel tellus. Praesent quam dolor, convallis eu semper et, maximus in odio. Ut iaculis semper posuere. Fusce nec mi et nibh tincidunt efficitur ornare id arcu. Etiam ex tellus, dignissim ac ligula scelerisque, sodales lobortis nisi. Maecenas at maximus urna.\r\n\"\"\"\r\nurl = \"www.google.com\"\r\n\r\ndoc = Document(text=text, extra_info={\"url\": url})\r\n\r\ndocuments = [doc]\r\n\r\nnodes: list[TextNode] = text_splitter.get_nodes_from_documents(documents)\r\n\r\nprint(f\"{nodes[0].text=}\")\r\nprint(f\"{nodes[0].text_template=}\")\r\nprint(f\"{nodes[0].metadata_template=}\")\r\nprint(f\"{nodes[0].metadata=}\")\r\n\r\n\r\n```\n\n### Relevant Logs/Tracbacks\n\n```shell\nnodes[0].text='Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus massa quam, posuere vel rhoncus non, venenatis id quam. Pellentesque ut purus sit amet magna fringilla dictum at vitae orci. Etiam in commodo sapien, quis suscipit ex. Donec commodo est ut est viverra mollis. Nam ullamcorper nunc purus, vel rutrum est tincidunt non. Vivamus venenatis maximus dui a faucibus. Aliquam non auctor velit. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse sit amet metus felis. Aliquam non lorem non magna semper gravida. Morbi feugiat lobortis nulla non rhoncus.\\n\\nProin in fermentum leo. Sed et mauris ac dui finibus dignissim.'\r\nnodes[0].text_template='{metadata_str}\\n\\n{content}'\r\nnodes[0].metadata_template='{key}: {value}'\r\nnodes[0].metadata={'url': 'www.google.com'}\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9297/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9297/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9296",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9296/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9296/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9296/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9296",
        "id": 2023878053,
        "node_id": "PR_kwDOIWuq585hDjP4",
        "number": 9296,
        "title": "Improve `Vertex` hints",
        "user": {
            "login": "Danipulok",
            "id": 45077699,
            "node_id": "MDQ6VXNlcjQ1MDc3Njk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/45077699?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Danipulok",
            "html_url": "https://github.com/Danipulok",
            "followers_url": "https://api.github.com/users/Danipulok/followers",
            "following_url": "https://api.github.com/users/Danipulok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Danipulok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Danipulok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Danipulok/subscriptions",
            "organizations_url": "https://api.github.com/users/Danipulok/orgs",
            "repos_url": "https://api.github.com/users/Danipulok/repos",
            "events_url": "https://api.github.com/users/Danipulok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Danipulok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710935,
                "node_id": "LA_kwDOIWuq588AAAABc3-fFw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:M",
                "name": "size:M",
                "color": "ebb800",
                "default": false,
                "description": "This PR changes 30-99 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-04T13:35:47Z",
        "updated_at": "2023-12-05T08:26:47Z",
        "closed_at": "2023-12-04T22:49:19Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9296",
            "html_url": "https://github.com/run-llama/llama_index/pull/9296",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9296.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9296.patch",
            "merged_at": "2023-12-04T22:49:19Z"
        },
        "body": "# Description\r\n\r\nRefactor `Vertex` llm a little bit to have more correct typehints and naming the same as in client SDK.\r\n\r\n**Motivation:**\r\nI was testing Vertex from `llama_index` and then saw that there are wrong (non-similar with the original SDK) names and wrong typehints. That's what I changed. Also added auth example to the `.ipynb`, because only God knows how to do it correctly and with typehints.\r\n\r\n## Type of Change\r\n\r\nBreaking change in a way that argument name is changed: `credential` -> `credentials`\r\n\r\n- [x] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nLocal tests run\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9296/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9296/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9295",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9295/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9295/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9295/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9295",
        "id": 2023814666,
        "node_id": "I_kwDOIWuq5854oPYK",
        "number": 9295,
        "title": "[Question]:   Properly Loading Summary Index from Storage with Open Source LLM in Application Context",
        "user": {
            "login": "Leonschmitt",
            "id": 28598834,
            "node_id": "MDQ6VXNlcjI4NTk4ODM0",
            "avatar_url": "https://avatars.githubusercontent.com/u/28598834?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Leonschmitt",
            "html_url": "https://github.com/Leonschmitt",
            "followers_url": "https://api.github.com/users/Leonschmitt/followers",
            "following_url": "https://api.github.com/users/Leonschmitt/following{/other_user}",
            "gists_url": "https://api.github.com/users/Leonschmitt/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Leonschmitt/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Leonschmitt/subscriptions",
            "organizations_url": "https://api.github.com/users/Leonschmitt/orgs",
            "repos_url": "https://api.github.com/users/Leonschmitt/repos",
            "events_url": "https://api.github.com/users/Leonschmitt/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Leonschmitt/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-04T13:03:18Z",
        "updated_at": "2023-12-04T13:32:56Z",
        "closed_at": "2023-12-04T13:32:56Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi,\r\n\r\nI've been working on integrating the Llama Index library into my application, and I've run into a problem when trying to store and load a summary index. This process is crucial for my application, as the indexing and loading operations are performed earlier in the app flow.\r\n\r\nHere's how I've set up the process:\r\n```\r\n\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=llm,\r\n    embed_model=embed_model, \r\n    chunk_size=480\r\n)\r\n\r\nsummary_index = SummaryIndex.from_documents(docs, service_context=service_context)\r\n\r\n```\r\n\r\nStoring and Attempting to Read Back from Storage:\r\n```\r\nsummary_index.storage_context.persist(\"sum_index\")\r\n\r\n# Rebuild storage context\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"sum_index\")\r\n\r\n# Load index from the storage context\r\nnew_index = load_index_from_storage(storage_context)\r\n```\r\nHowever, I encountered the following error during the loading process:\r\n```\r\n[9](new_index = load_index_from_storage(storage_context, sum_id)\r\nllm = OpenAI()\r\n---> [29](python3.8/site-packages/llama_index/llms/utils.py:29)     validate_openai_api_key(llm.api_key)\r\n     [30]/python3.8/site-packages/llama_index/llms/utils.py:30) except ValueError as e:\r\n\r\n/site-packages/llama_index/llms/openai_utils.py:371), in validate_openai_api_key(api_key)\r\n    llama_index/llms/openai_utils.py:370) if not openai_api_key:\r\n-->/site-packages/llama_index/llms/openai_utils.py:371)     raise ValueError(MISSING_API_KEY_ERROR_MESSAGE)\r\n\r\nValueError: No API key found for OpenAI.\r\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\r\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\r\n```\r\n\r\nThis error, originating from the llama_index/llms/utils.py file, seems to indicate a missing OpenAI API key. I am unsure if this is due to an oversight in my setup or a potential bug in the library. Moreover, I'm uncertain if there's a different way to correctly set the LLM, especially in the context of my application where the loading and indexing are performed at an earlier stage.\r\n\r\nI also tried to set my storage context. \r\nCan someone assist me in understanding whether this is a bug, a setup error on my part, or if there is an alternative approach to setting the LLM in such a use case? Any insights or shared experiences related to handling such scenarios in an app context would be highly appreciated.\r\n\r\nThank you!\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9295/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9295/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9294",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9294/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9294/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9294/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9294",
        "id": 2023754056,
        "node_id": "I_kwDOIWuq5854oAlI",
        "number": 9294,
        "title": "[Bug]: Creating Index via Elastic Search",
        "user": {
            "login": "pranavbhat12",
            "id": 54463581,
            "node_id": "MDQ6VXNlcjU0NDYzNTgx",
            "avatar_url": "https://avatars.githubusercontent.com/u/54463581?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pranavbhat12",
            "html_url": "https://github.com/pranavbhat12",
            "followers_url": "https://api.github.com/users/pranavbhat12/followers",
            "following_url": "https://api.github.com/users/pranavbhat12/following{/other_user}",
            "gists_url": "https://api.github.com/users/pranavbhat12/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pranavbhat12/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pranavbhat12/subscriptions",
            "organizations_url": "https://api.github.com/users/pranavbhat12/orgs",
            "repos_url": "https://api.github.com/users/pranavbhat12/repos",
            "events_url": "https://api.github.com/users/pranavbhat12/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pranavbhat12/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-04T12:33:26Z",
        "updated_at": "2023-12-04T15:07:51Z",
        "closed_at": "2023-12-04T15:07:50Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nSince past few days we were trying Elasticsearchstore index to enhance the retrieval capabilities but suddenly today getting error when pushing the documents to the elastic search.Can anybody help me to sort this issue?\r\n\r\n### Version\r\n\r\n0.9.11\r\n\r\n### Steps to Reproduce\r\n\r\nfrom llama_index.llms import HuggingFaceLLM\r\nllm = HuggingFaceLLM(context_window=8000,\r\n                    max_new_tokens=1024,\r\n                    system_prompt=system_prompt,\r\n                    query_wrapper_prompt=query_wrapper_prompt,\r\n                    model=model,\r\n                    tokenizer=tokenizer,\r\n                    generate_kwargs={\r\n                      \"temperature\": 0.5,\r\n                      # \"return_full_text\":True,\r\n                      \"do_sample\": True,\r\n                      \"repetition_penalty\":1.1,\r\n                      \"top_p\":0.7,\r\n                      \"top_k\":50,\r\n                      # \"return_dict_in_generate\":True,\r\n                    },\r\n                    tokenizer_outputs_to_remove=[\"token_type_ids\"]\r\n                 #   generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\r\n                #    tokenizer_kwargs={\"max_length\": 4096},\r\n                    )\r\nfrom langchain.embeddings import HuggingFaceEmbeddings\r\nembed_model_id = 'jinaai/jina-embeddings-v2-base-en'\r\nmodel_kwargs = {\"device\": \"cpu\"}\r\nembeddings = HuggingFaceEmbeddings(model_name=embed_model_id,model_kwargs=model_kwargs)\r\n\r\nfrom langchain.document_loaders import TextLoader\r\nfrom langchain.text_splitter import CharacterTextSplitter\r\nfrom langchain.document_loaders import PyPDFLoader,DirectoryLoader\r\nimport os\r\ndir_loader = DirectoryLoader(\"docs/\",\r\n                                 glob=\"*.pdf\",\r\n                                 loader_kwargs=\r\n                                 {\r\n                                     \"mode\":\"paged\"\r\n                                 }\r\n                                 )\r\ndocs = dir_loader.load()\r\n\r\nfrom llama_index import Document as llama_doc\r\nllama_doc_list=[]\r\nfor document in docs:\r\n    doc=llama_doc(\r\n        text=document.page_content,\r\n        metadata=document.metadata\r\n    )\r\n    llama_doc_list.append(doc)\r\n\r\nfrom llama_index.vector_stores import ElasticsearchStore\r\nvector_store = ElasticsearchStore(\r\n     es_cloud_id=\"\",\r\n    index_name=\"rag\",\r\n    es_user=\"elastic\",\r\n    es_password=\"\"\r\n  \r\n)\r\nstorage_context = StorageContext.from_defaults(vector_store)\r\nservice_context = ServiceContext.from_defaults(\r\n   \r\n    llm=llm,\r\n    embed_model=embeddings\r\n)\r\n\r\nset_global_service_context(service_context)\r\nindex = VectorStoreIndex.from_documents(\r\n    llama_doc_list, \r\n    storage_context=storage_context,\r\n)\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n```shell\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[25], line 1\r\n----> 1 index = VectorStoreIndex.from_documents(\r\n      2     llama_doc_list, \r\n      3     storage_context=storage_context,\r\n      4     # service_context=service_context\r\n      5 )\r\n      6 # index.storage_context.persist(\"jina_elasticindex_8k_IL\")\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/indices/base.py:97, in BaseIndex.from_documents(cls, documents, storage_context, service_context, show_progress, **kwargs)\r\n     95 with service_context.callback_manager.as_trace(\"index_construction\"):\r\n     96     for doc in documents:\r\n---> 97         docstore.set_document_hash(doc.get_doc_id(), doc.hash)\r\n     99     nodes = run_transformations(\r\n    100         documents,  # type: ignore\r\n    101         service_context.transformations,\r\n    102         show_progress=show_progress,\r\n    103         **kwargs,\r\n    104     )\r\n    106     return cls(\r\n    107         nodes=nodes,\r\n    108         storage_context=storage_context,\r\n   (...)\r\n    111         **kwargs,\r\n    112     )\r\n\r\nAttributeError: 'ElasticsearchStore' object has no attribute 'set_document_hash'\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9294/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9294/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9293",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9293/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9293/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9293/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9293",
        "id": 2023733033,
        "node_id": "I_kwDOIWuq5854n7cp",
        "number": 9293,
        "title": "[Feature Request]: Similarity between document names and query (user question) ",
        "user": {
            "login": "danilyef",
            "id": 12939044,
            "node_id": "MDQ6VXNlcjEyOTM5MDQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/12939044?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/danilyef",
            "html_url": "https://github.com/danilyef",
            "followers_url": "https://api.github.com/users/danilyef/followers",
            "following_url": "https://api.github.com/users/danilyef/following{/other_user}",
            "gists_url": "https://api.github.com/users/danilyef/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/danilyef/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/danilyef/subscriptions",
            "organizations_url": "https://api.github.com/users/danilyef/orgs",
            "repos_url": "https://api.github.com/users/danilyef/repos",
            "events_url": "https://api.github.com/users/danilyef/events{/privacy}",
            "received_events_url": "https://api.github.com/users/danilyef/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-04T12:21:41Z",
        "updated_at": "2023-12-04T15:17:15Z",
        "closed_at": "2023-12-04T15:17:15Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nI would like to suggest the following idea (I didn't find implementation in Llamaindex):\r\n\r\nInstead of doing a  search in the all documents (for example 100 pdf/json files), it would be great if llama_index had an option to measure similarity (cosine, for example) between query (user question) and filenames, and then performs RAG on the top k (parameter defined by user) similar documents.\r\n\r\nAlgorithm will be the following:\r\n1. Loading all files by using SimpleDirectoryReader\r\n2. Creating Service context and VectorIndex (and defining parameter \"query_filename_similarity = True\")\r\n3. User asks question.\r\n4. Algorithm measures similarity between question and filenames and retrieves top_k similar to the question files ( by using, for example, `SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')`\r\n5. RAG is performed only on files from the step 4 (either separately or joined into one big document.\r\n\r\n\n\n### Reason\n\nBy having hundreds files with similar content and detailed information (like PDF files about different Internet tarifs: their prices, speed etc.) RAG ( even advanced techniques like Auto-merging Retrieval and Sentence-Window Retrieval) often retrieves wrong information from the documents.\r\n\r\nIn may case doing  approach mentioned above improved quality of the answers by a lot. I didn't find anything similar and if something like that doesn't exist in llama_index, I would suggest to implement it.\n\n### Value of Feature\n\nIn quick and dirty code it looks like this:\r\n\r\n```\r\nfrom llama_index import SimpleDirectoryReader\r\n\r\ndocuments = SimpleDirectoryReader(\"./source_data\").load_data()\r\n\r\nfrom llama_index.node_parser import SentenceWindowNodeParser\r\n\r\n# create the sentence window node parser w/ default settings\r\nnode_parser = SentenceWindowNodeParser.from_defaults(\r\n    window_size=3,\r\n    window_metadata_key=\"window\",\r\n    original_text_metadata_key=\"original_text\",\r\n)\r\n\r\nfrom llama_index import LLMPredictor\r\nfrom llama_index.llms import LangChainLLM\r\nfrom llama_index.prompts import PromptTemplate\r\nfrom langchain.llms import HuggingFaceTextGenInference\r\n\r\n\r\nquery_wrapper_prompt = PromptTemplate(\r\n    \"[INST]<<SYS>>\\n\" + system_prompt + \"<</SYS>>\\n\\n{query_str}[/INST]\"\r\n)\r\n\r\n\r\n\r\n# Change default model\r\nllm = LLMPredictor(\r\n    llm=HuggingFaceTextGenInference(\r\n        inference_server_url=\"http://172.17.0.1:8080\",\r\n        max_new_tokens=1200,\r\n        top_k=10,\r\n        top_p=0.95,\r\n        temperature=0.01,\r\n        repetition_penalty=1.0,\r\n        streaming=True,\r\n        server_kwargs={},\r\n    ),\r\n    system_prompt=system_prompt,\r\n    query_wrapper_prompt=query_wrapper_prompt \r\n)\r\n\r\n# Embeddings:\r\nfrom langchain.embeddings import HuggingFaceEmbeddings\r\n\r\nmodel_name = \"intfloat/multilingual-e5-large\"\r\nmodel_kwargs = {'device': 'cpu'} \r\nencode_kwargs = {'normalize_embeddings': True}\r\n\r\nembeddings = HuggingFaceEmbeddings(model_name = model_name,model_kwargs = model_kwargs,encode_kwargs = encode_kwargs)\r\n\r\n### Question/filenames cosine similarity:\r\n\r\nfrom sentence_transformers import SentenceTransformer\r\nimport numpy as np\r\n\r\nmodel_query_filename = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',device = 'cpu')\r\n\r\ndef filename_question_similarity(model,query,filenames):\r\n    \r\n    sentences = [query] + filenames\r\n    embeddings = model.encode(sentences)\r\n    \r\n    #query_embedding = np.expand_dims(embeddings[0],axis = 0)\r\n    query_embedding = embeddings[0]\r\n    file_names_embeddings = embeddings[1:]\r\n                                     \r\n    cosine_similarity = (query_embedding @ file_names_embeddings.T)/(np.linalg.norm(query_embedding,axis=0) * np.linalg.norm(file_names_embeddings.T,axis=0))\r\n    return cosine_similarity[0],np.argmax(cosine_similarity[0])\r\n\r\n\r\n#all file_names:\r\nfilenames = []\r\nfor i in range(len(documents)):\r\n    file_name = documents[i].metadata['file_name'].split('.')[0]\r\n    filenames.append(file_name)\r\nfilenames = list(set(filenames))\r\n\r\n# Question answering:\r\n\r\n# Test:\r\nfrom llama_index import ServiceContext\r\nfrom llama_index import VectorStoreIndex\r\nfrom llama_index import Document\r\n\r\nstart_time = time.time()\r\nquestion = \"Tell me about X Tarif\"\r\nscore, closest_doc_id = filename_question_similarity(model_query_filename,question,filenames)\r\n\r\n# Service Context\r\nsentence_context = ServiceContext.from_defaults(\r\n    llm_predictor=llm,\r\n    embed_model=embeddings,\r\n    node_parser=node_parser,\r\n)\r\n\r\n\r\n\r\n# Documents with closest similarity between filename and query\r\ndocs = [doc for doc in documents if doc.metadata['file_name'].split('.')[0] == filenames[closest_doc_id]]\r\ndocument = Document(text=\"\\n\\n\".join([doc.text for doc in docs]))\r\n\r\n# VectorStore index\r\nsentence_index = VectorStoreIndex.from_documents(\r\n    [document], service_context=sentence_context\r\n)\r\n\r\n\r\n# Building postprocessor\r\n\r\nfrom llama_index.indices.postprocessor import MetadataReplacementPostProcessor\r\npostproc = MetadataReplacementPostProcessor(\r\n    target_metadata_key=\"window\"\r\n)\r\n\r\n# Adding reranker:\r\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\r\nrerank = SentenceTransformerRerank(top_n=2, model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\r\n#rerank = SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-base\")\r\n#rerank = SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-large\") \r\n\r\n# Query engine:\r\nsentence_window_engine = sentence_index.as_query_engine(\r\n    similarity_top_k=3, node_postprocessors=[postproc, rerank]\r\n)\r\n\r\n\r\n#Answer\r\nwindow_response = sentence_window_engine.query(\r\n    question\r\n)\r\nend_time = time.time()\r\nelapsed_time = end_time - start_time\r\n\r\nprint(window_response.response)\r\nprint(f\"Time elapsed: {elapsed_time} seconds\")\r\n\r\n\r\n# Question answering:\r\n\r\n# Test:\r\nfrom llama_index import ServiceContext\r\nfrom llama_index import VectorStoreIndex\r\nfrom llama_index import Document\r\n\r\nstart_time = time.time()\r\nquestion = \"Tell me about Y Tarif\"\r\nscore, closest_doc_id = filename_question_similarity(model_query_filename,question,filenames)\r\n\r\n# Service Context\r\nsentence_context = ServiceContext.from_defaults(\r\n    llm_predictor=llm,\r\n    embed_model=embeddings,\r\n    node_parser=node_parser,\r\n)\r\n\r\n\r\n\r\n# Documents with closest similarity between filename and query\r\ndocs = [doc for doc in documents if doc.metadata['file_name'].split('.')[0] == filenames[closest_doc_id]]\r\ndocument = Document(text=\"\\n\\n\".join([doc.text for doc in docs]))\r\n\r\n# VectorStore index\r\nsentence_index = VectorStoreIndex.from_documents(\r\n    [document], service_context=sentence_context\r\n)\r\n\r\n\r\n# Building postprocessor\r\n\r\nfrom llama_index.indices.postprocessor import MetadataReplacementPostProcessor\r\npostproc = MetadataReplacementPostProcessor(\r\n    target_metadata_key=\"window\"\r\n)\r\n\r\n# Adding reranker:\r\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\r\nrerank = SentenceTransformerRerank(top_n=2, model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\r\n#rerank = SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-base\")\r\n#rerank = SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-large\") \r\n\r\n# Query engine:\r\nsentence_window_engine = sentence_index.as_query_engine(\r\n    similarity_top_k=3, node_postprocessors=[postproc, rerank]\r\n)\r\n\r\n\r\n#Answer\r\nwindow_response = sentence_window_engine.query(\r\n    question\r\n)\r\nend_time = time.time()\r\nelapsed_time = end_time - start_time\r\n\r\nprint(window_response.response)\r\nprint(f\"Time elapsed: {elapsed_time} seconds\")\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9293/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9293/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9292",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9292/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9292/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9292/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9292",
        "id": 2023546137,
        "node_id": "I_kwDOIWuq5854nN0Z",
        "number": 9292,
        "title": "[Bug]: No module named 'openai.openai_object'",
        "user": {
            "login": "hynky1999",
            "id": 39408646,
            "node_id": "MDQ6VXNlcjM5NDA4NjQ2",
            "avatar_url": "https://avatars.githubusercontent.com/u/39408646?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hynky1999",
            "html_url": "https://github.com/hynky1999",
            "followers_url": "https://api.github.com/users/hynky1999/followers",
            "following_url": "https://api.github.com/users/hynky1999/following{/other_user}",
            "gists_url": "https://api.github.com/users/hynky1999/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hynky1999/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hynky1999/subscriptions",
            "organizations_url": "https://api.github.com/users/hynky1999/orgs",
            "repos_url": "https://api.github.com/users/hynky1999/repos",
            "events_url": "https://api.github.com/users/hynky1999/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hynky1999/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-12-04T10:42:53Z",
        "updated_at": "2023-12-05T23:00:53Z",
        "closed_at": "2023-12-05T01:38:00Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nAfter clean installing the llama-index, I am getting following error:\r\n`No module named 'openai.openai_object'`\r\n\r\nwhen running almost anything from llama-index e.g:\r\n```\r\nfrom llama_index import download_loader\r\nimport os\r\n\r\nAirtableReader = download_loader('AirtableReader')\r\n\r\nreader = AirtableReader(AIRTABLE_API_KEY)\r\n```\r\n\r\nI believe this is caused by the breaking changes in openai>1.x.x of `openai` package.\n\n### Version\n\n0.8.42\n\n### Steps to Reproduce\n\nClean install `llama-index` on python3.12\r\n\r\nRun following code:\r\n```\r\nfrom llama_index import download_loader\r\nimport os\r\n\r\nAirtableReader = download_loader('AirtableReader')\r\n\r\nreader = AirtableReader(\"TEST\")\r\n```\n\n### Relevant Logs/Tracbacks\n\n```shell\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n/Users/hynekkdylicek/airtable/fun.ipynb Cell 5 line 1\r\n----> 1 from llama_index import download_loader\r\n      2 import os\r\n      4 AirtableReader = download_loader('AirtableReader')\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/__init__.py:21\r\n     18 from llama_index.embeddings.openai import OpenAIEmbedding\r\n     20 # structured\r\n---> 21 from llama_index.indices.common.struct_store.base import SQLDocumentContextBuilder\r\n     23 # for composability\r\n     24 from llama_index.indices.composability.graph import ComposableGraph\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/indices/__init__.py:4\r\n      1 \"\"\"LlamaIndex data structures.\"\"\"\r\n      3 # indices\r\n----> 4 from llama_index.indices.document_summary.base import DocumentSummaryIndex\r\n      5 from llama_index.indices.keyword_table.base import (\r\n      6     GPTKeywordTableIndex,\r\n      7     KeywordTableIndex,\r\n      8 )\r\n      9 from llama_index.indices.keyword_table.rake_base import (\r\n     10     GPTRAKEKeywordTableIndex,\r\n     11     RAKEKeywordTableIndex,\r\n     12 )\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/indices/document_summary/__init__.py:4\r\n      1 \"\"\"Document summary index.\"\"\"\r\n----> 4 from llama_index.indices.document_summary.base import (\r\n      5     DocumentSummaryIndex,\r\n      6     GPTDocumentSummaryIndex,\r\n      7 )\r\n      8 from llama_index.indices.document_summary.retrievers import (\r\n      9     DocumentSummaryIndexEmbeddingRetriever,\r\n     10     DocumentSummaryIndexLLMRetriever,\r\n     11     DocumentSummaryIndexRetriever,\r\n     12 )\r\n     14 __all__ = [\r\n     15     \"DocumentSummaryIndex\",\r\n     16     \"DocumentSummaryIndexLLMRetriever\",\r\n   (...)\r\n     20     \"DocumentSummaryIndexRetriever\",\r\n     21 ]\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/indices/document_summary/base.py:14\r\n     11 from typing import Any, Dict, Optional, Sequence, Union, cast\r\n     13 from llama_index.data_structs.document_summary import IndexDocumentSummary\r\n---> 14 from llama_index.indices.base import BaseIndex\r\n     15 from llama_index.indices.base_retriever import BaseRetriever\r\n     16 from llama_index.indices.service_context import ServiceContext\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/indices/base.py:6\r\n      3 from abc import ABC, abstractmethod\r\n      4 from typing import Any, Dict, Generic, List, Optional, Sequence, Type, TypeVar, cast\r\n----> 6 from llama_index.chat_engine.types import BaseChatEngine, ChatMode\r\n      7 from llama_index.data_structs.data_structs import IndexStruct\r\n      8 from llama_index.indices.base_retriever import BaseRetriever\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/chat_engine/__init__.py:1\r\n----> 1 from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine\r\n      2 from llama_index.chat_engine.context import ContextChatEngine\r\n      3 from llama_index.chat_engine.simple import SimpleChatEngine\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/chat_engine/condense_question.py:6\r\n      3 from typing import Any, List, Optional, Type\r\n      5 from llama_index.callbacks import CallbackManager, trace_method\r\n----> 6 from llama_index.chat_engine.types import (\r\n      7     AgentChatResponse,\r\n      8     BaseChatEngine,\r\n      9     StreamingAgentChatResponse,\r\n     10 )\r\n     11 from llama_index.chat_engine.utils import response_gen_from_query_engine\r\n     12 from llama_index.indices.query.base import BaseQueryEngine\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/chat_engine/types.py:14\r\n     12 from llama_index.response.schema import Response, StreamingResponse\r\n     13 from llama_index.schema import NodeWithScore\r\n---> 14 from llama_index.tools import ToolOutput\r\n     16 logger = logging.getLogger(__name__)\r\n     17 logger.setLevel(logging.WARNING)\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/tools/__init__.py:5\r\n      3 from llama_index.tools.function_tool import FunctionTool\r\n      4 from llama_index.tools.query_engine import QueryEngineTool\r\n----> 5 from llama_index.tools.query_plan import QueryPlanTool\r\n      6 from llama_index.tools.retriever_tool import RetrieverTool\r\n      7 from llama_index.tools.types import (\r\n      8     AsyncBaseTool,\r\n      9     BaseTool,\r\n   (...)\r\n     12     adapt_to_async_tool,\r\n     13 )\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/tools/query_plan.py:6\r\n      3 from typing import Any, Dict, List, Optional\r\n      5 from llama_index.bridge.pydantic import BaseModel, Field\r\n----> 6 from llama_index.response_synthesizers import BaseSynthesizer, get_response_synthesizer\r\n      7 from llama_index.schema import NodeWithScore, TextNode\r\n      8 from llama_index.tools.types import BaseTool, ToolMetadata, ToolOutput\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/response_synthesizers/__init__.py:3\r\n      1 \"\"\"Init file.\"\"\"\r\n----> 3 from llama_index.response_synthesizers.accumulate import Accumulate\r\n      4 from llama_index.response_synthesizers.base import BaseSynthesizer\r\n      5 from llama_index.response_synthesizers.compact_and_refine import CompactAndRefine\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/response_synthesizers/accumulate.py:5\r\n      2 from typing import Any, List, Optional, Sequence\r\n      4 from llama_index.async_utils import run_async_tasks\r\n----> 5 from llama_index.indices.service_context import ServiceContext\r\n      6 from llama_index.prompts import BasePromptTemplate\r\n      7 from llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/indices/service_context.py:10\r\n      8 from llama_index.embeddings.base import BaseEmbedding\r\n      9 from llama_index.embeddings.utils import EmbedType, resolve_embed_model\r\n---> 10 from llama_index.indices.prompt_helper import PromptHelper\r\n     11 from llama_index.llm_predictor import LLMPredictor\r\n     12 from llama_index.llm_predictor.base import BaseLLMPredictor, LLMMetadata\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/indices/prompt_helper.py:16\r\n     14 from llama_index.bridge.pydantic import Field, PrivateAttr\r\n     15 from llama_index.constants import DEFAULT_CONTEXT_WINDOW, DEFAULT_NUM_OUTPUTS\r\n---> 16 from llama_index.llm_predictor.base import LLMMetadata\r\n     17 from llama_index.llms.openai_utils import is_chat_model\r\n     18 from llama_index.prompts import BasePromptTemplate\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/llm_predictor/__init__.py:3\r\n      1 \"\"\"Init params.\"\"\"\r\n----> 3 from llama_index.llm_predictor.base import LLMPredictor\r\n      5 # NOTE: this results in a circular import\r\n      6 # from llama_index.llm_predictor.mock import MockLLMPredictor\r\n      7 from llama_index.llm_predictor.structured import StructuredLLMPredictor\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/llm_predictor/base.py:18\r\n     11 from llama_index.llm_predictor.utils import (\r\n     12     astream_chat_response_to_tokens,\r\n     13     astream_completion_response_to_tokens,\r\n     14     stream_chat_response_to_tokens,\r\n     15     stream_completion_response_to_tokens,\r\n     16 )\r\n     17 from llama_index.llms.base import LLM, ChatMessage, LLMMetadata, MessageRole\r\n---> 18 from llama_index.llms.utils import LLMType, resolve_llm\r\n     19 from llama_index.prompts.base import BasePromptTemplate, PromptTemplate\r\n     20 from llama_index.schema import BaseComponent\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/llms/__init__.py:23\r\n     21 from llama_index.llms.konko import Konko\r\n     22 from llama_index.llms.langchain import LangChainLLM\r\n---> 23 from llama_index.llms.litellm import LiteLLM\r\n     24 from llama_index.llms.llama_cpp import LlamaCPP\r\n     25 from llama_index.llms.localai import LocalAI\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/llms/litellm.py:28\r\n      5 from llama_index.llms.base import (\r\n      6     LLM,\r\n      7     ChatMessage,\r\n   (...)\r\n     16     llm_completion_callback,\r\n     17 )\r\n     18 from llama_index.llms.generic_utils import (\r\n     19     achat_to_completion_decorator,\r\n     20     acompletion_to_chat_decorator,\r\n   (...)\r\n     26     stream_completion_to_chat_decorator,\r\n     27 )\r\n---> 28 from llama_index.llms.litellm_utils import (\r\n     29     acompletion_with_retry,\r\n     30     completion_with_retry,\r\n     31     from_openai_message_dict,\r\n     32     is_chat_model,\r\n     33     is_function_calling_model,\r\n     34     openai_modelname_to_contextsize,\r\n     35     to_openai_message_dicts,\r\n     36     validate_litellm_api_key,\r\n     37 )\r\n     40 class LiteLLM(LLM):\r\n     41     model: str = Field(\r\n     42         description=\"The LiteLLM model to use.\"\r\n     43     )  # For complete list of providers https://docs.litellm.ai/docs/providers\r\n\r\nFile ~/.pyenv/versions/3.12.0/envs/airtable3.12/lib/python3.12/site-packages/llama_index/llms/litellm_utils.py:4\r\n      1 import logging\r\n      2 from typing import Any, Callable, Dict, List, Optional, Sequence, Type\r\n----> 4 from openai.openai_object import OpenAIObject\r\n      5 from tenacity import (\r\n      6     before_sleep_log,\r\n      7     retry,\r\n   (...)\r\n     10     wait_exponential,\r\n     11 )\r\n     13 from llama_index.bridge.pydantic import BaseModel\r\n\r\nModuleNotFoundError: No module named 'openai.openai_object'\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9292/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9292/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9291",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9291/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9291/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9291/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9291",
        "id": 2023332845,
        "node_id": "PR_kwDOIWuq585hBquW",
        "number": 9291,
        "title": "[version] bump to v0.9.11.post1",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-04T08:51:19Z",
        "updated_at": "2023-12-04T08:52:18Z",
        "closed_at": "2023-12-04T08:52:17Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9291",
            "html_url": "https://github.com/run-llama/llama_index/pull/9291",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9291.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9291.patch",
            "merged_at": "2023-12-04T08:52:17Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9291/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9291/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9290",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9290/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9290/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9290/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9290",
        "id": 2023153954,
        "node_id": "PR_kwDOIWuq585hBDmJ",
        "number": 9290,
        "title": "Fix dataset url",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-04T06:52:35Z",
        "updated_at": "2023-12-04T06:59:23Z",
        "closed_at": "2023-12-04T06:59:22Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9290",
            "html_url": "https://github.com/run-llama/llama_index/pull/9290",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9290.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9290.patch",
            "merged_at": "2023-12-04T06:59:22Z"
        },
        "body": "# Description\r\n\r\nFix dataset url\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9290/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9290/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9289",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9289/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9289/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9289/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9289",
        "id": 2023103236,
        "node_id": "PR_kwDOIWuq585hA4aU",
        "number": 9289,
        "title": "Remove duplicate duplicate arg in CLI",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-04T06:10:29Z",
        "updated_at": "2023-12-04T06:14:33Z",
        "closed_at": "2023-12-04T06:14:32Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9289",
            "html_url": "https://github.com/run-llama/llama_index/pull/9289",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9289.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9289.patch",
            "merged_at": "2023-12-04T06:14:32Z"
        },
        "body": "# Description\r\n\r\nRemove duplicate duplicate arg in CLI",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9289/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9289/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9288",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9288/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9288/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9288/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9288",
        "id": 2022950042,
        "node_id": "I_kwDOIWuq5854k8Sa",
        "number": 9288,
        "title": "[Question]: I want the user to use his/her own api key, where should I put the api key",
        "user": {
            "login": "dinhan92",
            "id": 86275789,
            "node_id": "MDQ6VXNlcjg2Mjc1Nzg5",
            "avatar_url": "https://avatars.githubusercontent.com/u/86275789?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dinhan92",
            "html_url": "https://github.com/dinhan92",
            "followers_url": "https://api.github.com/users/dinhan92/followers",
            "following_url": "https://api.github.com/users/dinhan92/following{/other_user}",
            "gists_url": "https://api.github.com/users/dinhan92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dinhan92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dinhan92/subscriptions",
            "organizations_url": "https://api.github.com/users/dinhan92/orgs",
            "repos_url": "https://api.github.com/users/dinhan92/repos",
            "events_url": "https://api.github.com/users/dinhan92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dinhan92/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-12-04T03:34:35Z",
        "updated_at": "2023-12-04T05:31:40Z",
        "closed_at": "2023-12-04T03:43:54Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nCurrently, fixed api key:\r\n\r\n``` python\r\napp = Quart(__name__)\r\nos.environ[\"OPENAI_API_KEY\"] = ''\r\nopenai.api_key = ''\r\n\r\nmax_input_size = 4096\r\nnum_outputs = 512\r\nmax_chunk_overlap = 0.9\r\n# chunk_size_limit = 256\r\n\r\nprompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap)\r\n\r\nllm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\r\n\r\nservice_context = ServiceContext.from_defaults(\r\n    llm = llm,\r\n    # embed_model = embed_model, \r\n    prompt_helper = prompt_helper\r\n)\r\n\r\ndef construct_index(directory_path):\r\n    # Load data from directory\r\n    documents = SimpleDirectoryReader(directory_path).load_data()\r\n    documents_mongo_plants = SimpleMongoReader(uri=\"mongodb://localhost:27017\").load_data(db_name=\"AgricultureAppDb\", collection_name=\"plants\", field_names=[\"Name\", \"Description\", \"Content\"])\r\n    documents_mongo_plant_procedures = SimpleMongoReader(uri=\"mongodb://localhost:27017\").load_data(db_name=\"AgricultureAppDb\", collection_name=\"plantGrowthProcedures\", field_names=[\"Name\", \"Tag\", \"Description\", \"Content\"])\r\n    report_indices = {}\r\n    report_indices[name_const_pdf] = VectorStoreIndex.from_documents(documents, service_context=service_context)\r\n    report_indices[name_const_plant] = VectorStoreIndex.from_documents(documents_mongo_plants, service_context=service_context)\r\n    report_indices[name_const_plant_procedure] = VectorStoreIndex.from_documents(documents_mongo_plant_procedures, service_context=service_context)\r\n\r\n    for name_pdf in name_pdfs:\r\n        name_pdf_get = name_pdf.Name\r\n        report_indices[name_pdf_get].storage_context.persist(persist_dir=f'./storage_{name_pdf_get}')\r\n\r\n    return report_indices\r\n\r\nquery_engine_tools_global = []\r\n\r\ndef construct_tools():\r\n    report_indices = {}\r\n    query_engine = {}\r\n    query_engine_tools = []\r\n    # rebuild storage context\r\n    for name_pdf in name_pdfs:\r\n        name_pdf_get = name_pdf.Name\r\n        storage_context = StorageContext.from_defaults(persist_dir=f\"./storage_{name_pdf_get}\")\r\n        report_indices[name_pdf_get] = load_index_from_storage(storage_context = storage_context,\r\n                                    # embed_model=embed_model, \r\n                                    service_context = service_context)\r\n        query_engine[name_pdf_get] = report_indices[name_pdf_get].as_query_engine()\r\n        query_engine_tools.append(QueryEngineTool(query_engine=query_engine[name_pdf_get], metadata = ToolMetadata(name=name_pdf_get, description=name_pdf.Description)))\r\n    \r\n    return query_engine_tools\r\n\r\nquery_engine_tools_global = construct_tools()\r\n\r\n@app.route('/api/chatbot', methods=['GET', 'POST'])\r\nasync def chatbot(**kwargs) -> str:\r\n    try:\r\n        data = await request.data\r\n        input_text = data.decode(\"utf-8\")\r\n\r\n        agent = OpenAIAgent.from_tools(\r\n            tools=query_engine_tools_global,\r\n            llm=llm,\r\n            chat_history=[\r\n                ChatMessage(role=MessageRole.USER, content=\"Ch\u00e0o BSNH AI, ch\u00fang ta s\u1ebd b\u00e0n v\u1ec1 n\u00f4ng nghi\u1ec7p ng\u00e0y h\u00f4m nay.\"),\r\n                \r\n            ],\r\n            prefix_messages=[\r\n                ChatMessage(role=MessageRole.SYSTEM, content=\"B\u1ea1n l\u00e0 m\u1ed9t tr\u1ee3 l\u00fd \u1ea3o c\u1ee7a TNH99, c\u00f3 t\u00ean l\u00e0 BSNH AI, \u0111\u01b0\u1ee3c ph\u00e1t tri\u1ec3n b\u1edfi TNH99, ch\u1ee7 \u0111\u1ec1 ch\u00ednh c\u1ee7a b\u1ea1n l\u00e0 n\u00f4ng nghi\u1ec7p. N\u1ebfu b\u1ea1n kh\u00f4ng tr\u1ea3 l\u1eddi \u0111\u01b0\u1ee3c h\u00e3y b\u1ea3o ng\u01b0\u1eddi d\u00f9ng li\u00ean h\u1ec7 v\u1edbi TNH99. Vui l\u00f2ng tr\u1ea3 l\u1eddi b\u1eb1ng ti\u1ebfng Vi\u1ec7t.\"),\r\n                ChatMessage(role=MessageRole.USER, content=\"Ch\u00e0o BSNH AI, ch\u00fang ta s\u1ebd b\u00e0n v\u1ec1 n\u00f4ng nghi\u1ec7p ng\u00e0y h\u00f4m nay.\"),\r\n                \r\n            ],\r\n            verbose=True\r\n        )\r\n\r\n        agent_stream = await agent.achat(input_text)\r\n\r\n        savedAnswer = dumps(ChatHistory(Question=input_text, Answer=agent_stream.response, IsDone=True, IdUser=\"\").__dict__)\r\n\r\n        # q.put(savedAnswer)\r\n        return app.response_class(savedAnswer)\r\n    except Exception as e:\r\n        print(e)\r\n        exception = \"Xin l\u1ed7i, BSNH AI \u0111ang b\u1eadn.\"\r\n        return app.response_class(exception)\r\n```\r\n\r\nif I put the api key inside the function chatbot then I would meet error No API Key provided when start the app",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9288/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9288/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9287",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9287/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9287/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9287/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9287",
        "id": 2022919031,
        "node_id": "I_kwDOIWuq5854k0t3",
        "number": 9287,
        "title": "[Bug]: cannot import name 'SimpleMongoReader' from 'llama_index'",
        "user": {
            "login": "dinhan92",
            "id": 86275789,
            "node_id": "MDQ6VXNlcjg2Mjc1Nzg5",
            "avatar_url": "https://avatars.githubusercontent.com/u/86275789?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dinhan92",
            "html_url": "https://github.com/dinhan92",
            "followers_url": "https://api.github.com/users/dinhan92/followers",
            "following_url": "https://api.github.com/users/dinhan92/following{/other_user}",
            "gists_url": "https://api.github.com/users/dinhan92/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dinhan92/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dinhan92/subscriptions",
            "organizations_url": "https://api.github.com/users/dinhan92/orgs",
            "repos_url": "https://api.github.com/users/dinhan92/repos",
            "events_url": "https://api.github.com/users/dinhan92/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dinhan92/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-04T02:58:19Z",
        "updated_at": "2023-12-04T03:00:31Z",
        "closed_at": "2023-12-04T03:00:31Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nCan not import SimpleMongoReader\n\n### Version\n\n0.9.10\n\n### Steps to Reproduce\n\ninstall the newest 0.9.10 of llama index\r\n\r\nfrom llama_index import SimpleDirectoryReader, ServiceContext, VectorStoreIndex, PromptHelper, SimpleMongoReader, StorageContext, load_index_from_storage\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9287/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9287/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9286",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9286/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9286/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9286/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9286",
        "id": 2022870812,
        "node_id": "I_kwDOIWuq5854ko8c",
        "number": 9286,
        "title": "[Bug]: Chroma.py breaks when using InstructorEmbedding",
        "user": {
            "login": "mphipps2",
            "id": 5166558,
            "node_id": "MDQ6VXNlcjUxNjY1NTg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5166558?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mphipps2",
            "html_url": "https://github.com/mphipps2",
            "followers_url": "https://api.github.com/users/mphipps2/followers",
            "following_url": "https://api.github.com/users/mphipps2/following{/other_user}",
            "gists_url": "https://api.github.com/users/mphipps2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mphipps2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mphipps2/subscriptions",
            "organizations_url": "https://api.github.com/users/mphipps2/orgs",
            "repos_url": "https://api.github.com/users/mphipps2/repos",
            "events_url": "https://api.github.com/users/mphipps2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mphipps2/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-12-04T02:02:39Z",
        "updated_at": "2023-12-04T02:13:28Z",
        "closed_at": "2023-12-04T02:13:28Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nI'm able to use faiss locally with either Instructor (InstructorEmbedding) or HuggingFace Embeddings (HuggingFaceEmbedding) models, and I'm able to use chroma locally with the HF models, but when I try to use it with the Instruct models I get value errors when chromadb does type checks on the embeddings in this function (chromadb/api/types.py):\r\n\r\ndef validate_embeddings(embeddings: Embeddings) -> Embeddings:\r\n    \"\"\"Validates embeddings to ensure it is a list of list of ints, or floats\"\"\"\r\n    if not isinstance(embeddings, list):\r\n        raise ValueError(f\"Expected embeddings to be a list, got {embeddings}\")\r\n    if len(embeddings) == 0:\r\n        raise ValueError(\r\n            f\"Expected embeddings to be a list with at least one item, got {embeddings}\"\r\n        )\r\n    if not all([isinstance(e, list) for e in embeddings]):\r\n        print('type: ', type(embeddings), ' type embeddings[0]: ' , type(embeddings[0]))\r\n        raise ValueError(\r\n            f\"Expected each embedding in the embeddings to be a list, got {embeddings}\"\r\n        )\r\n    for embedding in embeddings:\r\n        if not all([isinstance(value, (int, float)) for value in embedding]):\r\n            raise ValueError(\r\n                f\"Expected each value in the embedding to be a int or float, got {embeddings}\"\r\n            )\r\n    return embeddings\r\n    \r\n\r\nThis is triggered when adding both the context and query embeddings to chromadb in lines 243 and 287 from this file:\r\nhttps://github.com/run-llama/llama_index/blob/main/llama_index/vector_stores/chroma.py. In the context embedding case this is happening because chromadb wants the embeddings to be a list of lists, not a list of numpy arrays. Then in the query case, the query embeddings are being sent to chromadb as a list of np.float32. Chroma wants it as a list of list of native floats. \r\n\r\nTo fix this problem I think you can do this (tested successfully with Chroma using either Instruct embeddings or HF):\r\n\r\nin llama_index/vector_stores/chroma.py:\r\n\r\n1) import numpy as np\r\n\r\n2)     def add(self, nodes: List[BaseNode], **add_kwargs: Any) -> List[str]:\r\n        \"\"\"Add nodes to index.\r\n\r\n        Args:\r\n            nodes: List[BaseNode]: list of nodes with embeddings\r\n\r\n        \"\"\"\r\n        if not self._collection:\r\n            raise ValueError(\"Collection not initialized\")\r\n\r\n        max_chunk_size = MAX_CHUNK_SIZE\r\n        node_chunks = chunk_list(nodes, max_chunk_size)\r\n\r\n        all_ids = []\r\n        for node_chunk in node_chunks:\r\n            embeddings = []\r\n            metadatas = []\r\n            ids = []\r\n            documents = []\r\n            for node in node_chunk:\r\n                embedding = node.get_embedding()\r\n                # Convert the embedding to a list if it's a NumPy array\r\n                if isinstance(embedding, np.ndarray):\r\n                    embedding = embedding.tolist()\r\n                embeddings.append(embedding)\r\n                \r\n3)       def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:\r\n        \"\"\"Query index for top k most similar nodes.\r\n\r\n        Args:\r\n            query_embedding (List[float]): query embedding\r\n            similarity_top_k (int): top k most similar nodes\r\n\r\n        \"\"\"\r\n        if query.filters is not None:\r\n            if \"where\" in kwargs:\r\n                raise ValueError(\r\n                    \"Cannot specify metadata filters via both query and kwargs. \"\r\n                    \"Use kwargs only for chroma specific items that are \"\r\n                    \"not supported via the generic query interface.\"\r\n                )\r\n            where = _to_chroma_filter(query.filters)\r\n        else:\r\n            where = kwargs.pop(\"where\", {})\r\n\r\n        # Convert query_embedding to the correct format\r\n        query_embedding = query.query_embedding\r\n        if isinstance(query_embedding, np.ndarray):\r\n            query_embedding = query_embedding.tolist()\r\n        if isinstance(query_embedding, list) and not isinstance(query_embedding[0], list):\r\n            query_embedding = [query_embedding]\r\n\r\n        # Convert each numpy.float32 element to a native Python float\r\n        query_embedding = [[float(value) for value in emb] for emb in query_embedding]\r\n\r\n        results = self._collection.query(\r\n            query_embeddings=query_embedding,\r\n            n_results=query.similarity_top_k,\r\n            where=where,\r\n            **kwargs,\r\n        )\r\n\r\n\r\n\r\n### Version\r\n\r\nv0.9.11\r\n\r\n### Steps to Reproduce\r\n\r\nSet your embeddings model as \"local:hkunlp/instructor-xl\" and initialize your vector store as ChromaVectorStore:\r\n        chroma_client = chromadb.EphemeralClient()\r\n        chroma_collection = chroma_client.create_collection(\"quickstart\")\r\n        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\n        \r\nWill result in ValueErrors from chromadb\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9286/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9286/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9285",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9285/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9285/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9285/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9285",
        "id": 2022752309,
        "node_id": "PR_kwDOIWuq585g_sXK",
        "number": 9285,
        "title": "Implement FlagEmbeddingReranker for search re-ranking",
        "user": {
            "login": "ColeMurray",
            "id": 2492022,
            "node_id": "MDQ6VXNlcjI0OTIwMjI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2492022?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ColeMurray",
            "html_url": "https://github.com/ColeMurray",
            "followers_url": "https://api.github.com/users/ColeMurray/followers",
            "following_url": "https://api.github.com/users/ColeMurray/following{/other_user}",
            "gists_url": "https://api.github.com/users/ColeMurray/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ColeMurray/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ColeMurray/subscriptions",
            "organizations_url": "https://api.github.com/users/ColeMurray/orgs",
            "repos_url": "https://api.github.com/users/ColeMurray/repos",
            "events_url": "https://api.github.com/users/ColeMurray/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ColeMurray/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-12-03T22:54:32Z",
        "updated_at": "2023-12-09T00:24:02Z",
        "closed_at": "2023-12-07T15:51:44Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9285",
            "html_url": "https://github.com/run-llama/llama_index/pull/9285",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9285.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9285.patch",
            "merged_at": "2023-12-07T15:51:44Z"
        },
        "body": "\r\n\r\n# Description\r\n\r\nAdded FlagEmbeddingReranker for use with BAAI/bge-reranker as a post nodeprocessor\r\n\r\nFlagEmbedding is required to use the module.\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9285/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9285/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9284",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9284/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9284/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9284/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9284",
        "id": 2022717690,
        "node_id": "PR_kwDOIWuq585g_lba",
        "number": 9284,
        "title": "labelled + labeled ",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-03T21:12:46Z",
        "updated_at": "2023-12-03T21:56:53Z",
        "closed_at": "2023-12-03T21:56:52Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9284",
            "html_url": "https://github.com/run-llama/llama_index/pull/9284",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9284.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9284.patch",
            "merged_at": "2023-12-03T21:56:52Z"
        },
        "body": "labelled and labeled are british/american spellings, added proxy to class names lol \r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9284/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9284/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9283",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9283/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9283/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9283/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9283",
        "id": 2022700500,
        "node_id": "PR_kwDOIWuq585g_iAu",
        "number": 9283,
        "title": "[version] bump to v0.9.11",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-03T20:24:25Z",
        "updated_at": "2023-12-03T20:28:45Z",
        "closed_at": "2023-12-03T20:28:44Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9283",
            "html_url": "https://github.com/run-llama/llama_index/pull/9283",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9283.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9283.patch",
            "merged_at": "2023-12-03T20:28:44Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9283/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9283/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9282",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9282/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9282/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9282/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9282",
        "id": 2022696428,
        "node_id": "I_kwDOIWuq5854j-Xs",
        "number": 9282,
        "title": "[Question]: What is the default \"local\" embedding model?",
        "user": {
            "login": "stephanedebove",
            "id": 6358105,
            "node_id": "MDQ6VXNlcjYzNTgxMDU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6358105?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stephanedebove",
            "html_url": "https://github.com/stephanedebove",
            "followers_url": "https://api.github.com/users/stephanedebove/followers",
            "following_url": "https://api.github.com/users/stephanedebove/following{/other_user}",
            "gists_url": "https://api.github.com/users/stephanedebove/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stephanedebove/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stephanedebove/subscriptions",
            "organizations_url": "https://api.github.com/users/stephanedebove/orgs",
            "repos_url": "https://api.github.com/users/stephanedebove/repos",
            "events_url": "https://api.github.com/users/stephanedebove/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stephanedebove/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-03T20:12:30Z",
        "updated_at": "2023-12-04T13:07:22Z",
        "closed_at": "2023-12-04T13:07:21Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIn the doc here: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html\r\n\r\nwe can read:\r\n\r\n> To save costs, you may want to use a local model.\r\n> \r\n> from llama_index import ServiceContext\r\n> \r\n> service_context = ServiceContext.from_defaults(embed_model=\"local\")\r\n>\r\n> This will use a well-performing and fast default from Hugging Face.\r\n\r\nBut what is the fast default model loaded from HuggingFace exactly?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9282/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9282/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9281",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9281/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9281/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9281/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9281",
        "id": 2022686046,
        "node_id": "PR_kwDOIWuq585g_fIc",
        "number": 9281,
        "title": "AutoRetrievel Filter using OpenAI agent and multi-filters",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-03T19:43:36Z",
        "updated_at": "2023-12-04T04:09:29Z",
        "closed_at": "2023-12-04T04:09:28Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9281",
            "html_url": "https://github.com/run-llama/llama_index/pull/9281",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9281.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9281.patch",
            "merged_at": "2023-12-04T04:09:28Z"
        },
        "body": "# Description\r\nEnable Metadata fitler for Auto Retriever\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9281/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9281/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9280",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9280/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9280/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9280/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9280",
        "id": 2022666960,
        "node_id": "I_kwDOIWuq5854j3LQ",
        "number": 9280,
        "title": "[Question]: Support for multi-gpu embedding",
        "user": {
            "login": "mphipps2",
            "id": 5166558,
            "node_id": "MDQ6VXNlcjUxNjY1NTg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5166558?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mphipps2",
            "html_url": "https://github.com/mphipps2",
            "followers_url": "https://api.github.com/users/mphipps2/followers",
            "following_url": "https://api.github.com/users/mphipps2/following{/other_user}",
            "gists_url": "https://api.github.com/users/mphipps2/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mphipps2/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mphipps2/subscriptions",
            "organizations_url": "https://api.github.com/users/mphipps2/orgs",
            "repos_url": "https://api.github.com/users/mphipps2/repos",
            "events_url": "https://api.github.com/users/mphipps2/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mphipps2/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-03T18:54:35Z",
        "updated_at": "2023-12-04T01:44:02Z",
        "closed_at": "2023-12-04T01:44:02Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nIs there any support for embedding model parallelism across multiple gpus? From what I can tell, both the InstructorEmbedding and HuggingFaceEmbedding models are currently defaulting to cuda:0 (with device not being passed to either during the instantiation in resolve_embed_model -- an easy update). But going a step further, I'm interested in how we could configure model parallelism here. \r\n\r\nThanks",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9280/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9280/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9278",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9278/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9278/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9278/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9278",
        "id": 2022561328,
        "node_id": "PR_kwDOIWuq585g_Fb3",
        "number": 9278,
        "title": "Close doc issues #7874 and #8476",
        "user": {
            "login": "aaronjimv",
            "id": 67152883,
            "node_id": "MDQ6VXNlcjY3MTUyODgz",
            "avatar_url": "https://avatars.githubusercontent.com/u/67152883?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/aaronjimv",
            "html_url": "https://github.com/aaronjimv",
            "followers_url": "https://api.github.com/users/aaronjimv/followers",
            "following_url": "https://api.github.com/users/aaronjimv/following{/other_user}",
            "gists_url": "https://api.github.com/users/aaronjimv/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/aaronjimv/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/aaronjimv/subscriptions",
            "organizations_url": "https://api.github.com/users/aaronjimv/orgs",
            "repos_url": "https://api.github.com/users/aaronjimv/repos",
            "events_url": "https://api.github.com/users/aaronjimv/events{/privacy}",
            "received_events_url": "https://api.github.com/users/aaronjimv/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-03T14:54:59Z",
        "updated_at": "2023-12-04T14:33:54Z",
        "closed_at": "2023-12-03T15:10:03Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9278",
            "html_url": "https://github.com/run-llama/llama_index/pull/9278",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9278.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9278.patch",
            "merged_at": "2023-12-03T15:10:03Z"
        },
        "body": "# Description\r\n\r\nThis PR closes two documentation issues that have already been resolved but remain open.\r\n\r\nFixes #7874 \r\nFixes #8476 \r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] My changes generate no new warnings\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9278/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9278/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9277",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9277/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9277/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9277/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9277",
        "id": 2022524450,
        "node_id": "I_kwDOIWuq5854jUYi",
        "number": 9277,
        "title": "[Question]: PromptTemplate seems to mess up the answer",
        "user": {
            "login": "stephanedebove",
            "id": 6358105,
            "node_id": "MDQ6VXNlcjYzNTgxMDU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6358105?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stephanedebove",
            "html_url": "https://github.com/stephanedebove",
            "followers_url": "https://api.github.com/users/stephanedebove/followers",
            "following_url": "https://api.github.com/users/stephanedebove/following{/other_user}",
            "gists_url": "https://api.github.com/users/stephanedebove/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stephanedebove/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stephanedebove/subscriptions",
            "organizations_url": "https://api.github.com/users/stephanedebove/orgs",
            "repos_url": "https://api.github.com/users/stephanedebove/repos",
            "events_url": "https://api.github.com/users/stephanedebove/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stephanedebove/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-12-03T13:40:26Z",
        "updated_at": "2023-12-04T15:19:47Z",
        "closed_at": "2023-12-04T15:19:47Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nFollowing the documentation, I have been building a small app creating an index from a docx file (containing a FAQ) and then simply answering the question from the user. It works pretty well, except when I start to customize the prompt template. Then, instead of just giving me my answer, it gives me my answer as well as a couple of other questions and answers it found in my file.\r\n\r\nSo this is working well:\r\n\r\n```\r\nimport logging\r\nimport sys\r\nfrom llama_index.llms import LlamaCPP\r\nfrom llama_index.prompts import PromptTemplate\r\nimport os.path\r\nfrom llama_index import (\r\n    VectorStoreIndex,\r\n    SimpleDirectoryReader,\r\n    StorageContext,\r\n    load_index_from_storage,\r\n    ServiceContext,\r\n    PromptHelper,\r\n)\r\n\r\nllm = LlamaCPP(\r\n    model_path=\"./llms/zephyr-7b-alpha.Q5_K_M.gguf\",\r\n    temperature=0.0,\r\n    max_new_tokens=1024,\r\n    context_window=3900, \r\n    model_kwargs={\"n_gpu_layers\": 4}, \r\n)\r\n\r\nfrom llama_index.embeddings import HuggingFaceEmbedding\r\nembed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\")\r\n\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=llm, \r\n    embed_model=\"local:intfloat/multilingual-e5-large\", \r\n    chunk_size=512, \r\n    chunk_overlap=20\r\n)\r\n\r\nif not os.path.exists(\"./storage\"):\r\n    documents = SimpleDirectoryReader(\"data\").load_data()\r\n    index = VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=True)\r\n    index.storage_context.persist()\r\nelse:\r\n    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\r\n    index = load_index_from_storage(storage_context, service_context=service_context)\r\n\r\nquery_engine = index.as_query_engine( service_context=service_context, similarity_top_k=2,response_mode=\"compact\")\r\nresponse = query_engine.query(\"What is the meaning of life ?\")\r\nprint(response)  \r\n```\r\n\r\nbut if I add\r\n\r\n```\r\ntemplate = (\r\n    \"Using this context : \\n\"\r\n    \"---------------------\\n\"\r\n    \"{context_str}\"\r\n    \"\\n---------------------\\n\"\r\n    \"Answer this question : {query_str}\\n\"\r\n    \"If the answer is not in the context, say 'I don\u2019t know, sorry'. \u00bb \"\r\n)\r\nqa_template = PromptTemplate(template)\r\n```\r\n\r\nand change the query_engine to:\r\n\r\n`query_engine = index.as_query_engine( service_context=service_context, similarity_top_k=2,response_mode=\"compact\",text_qa_template=qa_template)\r\n`\r\n\r\nit just starts giving me complicated and non-relevant outputs. Any idea why?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9277/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9277/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9276",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9276/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9276/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9276/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9276",
        "id": 2022463727,
        "node_id": "I_kwDOIWuq5854jFjv",
        "number": 9276,
        "title": "[Bug]: TypeError: 'NoneType' in get_llm_token_counts when using CondensePlusContextChatEngine and Bedrock",
        "user": {
            "login": "vldvasi",
            "id": 46647774,
            "node_id": "MDQ6VXNlcjQ2NjQ3Nzc0",
            "avatar_url": "https://avatars.githubusercontent.com/u/46647774?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vldvasi",
            "html_url": "https://github.com/vldvasi",
            "followers_url": "https://api.github.com/users/vldvasi/followers",
            "following_url": "https://api.github.com/users/vldvasi/following{/other_user}",
            "gists_url": "https://api.github.com/users/vldvasi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vldvasi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vldvasi/subscriptions",
            "organizations_url": "https://api.github.com/users/vldvasi/orgs",
            "repos_url": "https://api.github.com/users/vldvasi/repos",
            "events_url": "https://api.github.com/users/vldvasi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vldvasi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-12-03T10:46:48Z",
        "updated_at": "2023-12-05T01:40:38Z",
        "closed_at": "2023-12-05T01:40:38Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nWhen using LLamaIndex's QueryFusionRetriever and CondensePlusContextChatEngine with the Bedrock APIs (Claude-v2 and Titan-embed-text-v1) a NoneType exception is triggered in the function get_llm_token_counts() of token_counting.py.  \r\n\r\n```\r\n    usage = response.raw[\"usage\"]  # type: ignore\r\n            ~~~~~~~~~~~~^^^^^^^^^\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\nPossibly handling the case when response.raw is None would fix the issue.\n\n### Version\n\n0.9.8\n\n### Steps to Reproduce\n\nThe following snippet of code results in the mentioned error:\r\n\r\n```\r\nllm = Bedrock(\r\n    client=BEDROCK_CLIENT,\r\n    model_id=\"anthropic.claude-v2\",\r\n    region_name=\"us-east-1\",\r\n    # verbose=True,\r\n)\r\n\r\nembed_model = LangchainEmbedding(\r\n    BedrockEmbeddings(\r\n        model_id=\"amazon.titan-embed-text-v1\",\r\n        region_name=\"us-east-1\",\r\n        client=BEDROCK_CLIENT,\r\n    )\r\n)\r\n\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=llm,\r\n    embed_model=embed_model,\r\n    callback_manager=callback_manager,\r\n    chunk_size=llm_model_chunk_size,\r\n    context_window=llm_model_context_window,\r\n)\r\n\r\nnest_asyncio.apply()\r\n\r\nretriever = QueryFusionRetriever(\r\n        [index.as_retriever() for index in indexes],\r\n        llm=llm,\r\n        similarity_top_k=int(similarity_top_k),\r\n        num_queries=1, \r\n        use_async=True,\r\n        verbose=True\r\n    )   \r\n\r\ncustom_chat_history = get_chat_history(conversation_id, db)\r\nchat_engine = CondensePlusContextChatEngine.from_defaults(\r\n    retriever=retriever,\r\n    service_context=service_context,\r\n    context_prompt=qa_prompt_templ_text,\r\n    condense_prompt=custom_prompt,\r\n    chat_history=custom_chat_history,\r\n    verbose=True\r\n)\r\n\r\nllm_response = chat_engine.chat(question)\r\nanswer = llm_response.response\r\n```\r\n\r\n\n\n### Relevant Logs/Tracbacks\n\n```shell\n2023-12-03 11:58:02,595 - ERROR - Error in chat_query_engine_multi_index_ep: 'NoneType' object is not subscriptable\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\a912553\\InsightEngine\\routes\\chat_engine_routes.py\", line 810, in chat_query_engine_multi_index_ep\r\n    response = chat_query_engine_multi_index(db, query_input)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\a912553\\InsightEngine\\routes\\chat_engine_routes.py\", line 718, in chat_query_engine_multi_index\r\n    llm_response = chat_engine.chat(question)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\a912553\\InsightEngine\\.venv\\Lib\\site-packages\\llama_index\\callbacks\\utils.py\", line 39, in wrapper\r\n    return func(self, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\a912553\\InsightEngine\\.venv\\Lib\\site-packages\\llama_index\\chat_engine\\condense_plus_context.py\", line 289, in chat\r\n    chat_response = self._llm.chat(chat_messages)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\a912553\\InsightEngine\\.venv\\Lib\\site-packages\\llama_index\\llms\\base.py\", line 208, in wrapped_llm_chat\r\n    callback_manager.on_event_end(\r\n  File \"C:\\Users\\a912553\\InsightEngine\\.venv\\Lib\\site-packages\\llama_index\\callbacks\\base.py\", line 116, in on_event_end\r\n    handler.on_event_end(event_type, payload, event_id=event_id, **kwargs)\r\n  File \"C:\\Users\\a912553\\InsightEngine\\.venv\\Lib\\site-packages\\llama_index\\callbacks\\token_counting.py\", line 154, in on_event_end\r\n    get_llm_token_counts(\r\n  File \"C:\\Users\\a912553\\InsightEngine\\.venv\\Lib\\site-packages\\llama_index\\callbacks\\token_counting.py\", line 49, in get_llm_token_counts\r\n    usage = response.raw[\"usage\"]  # type: ignore\r\n            ~~~~~~~~~~~~^^^^^^^^^\r\nTypeError: 'NoneType' object is not subscriptable\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9276/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9276/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9275",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9275/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9275/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9275/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9275",
        "id": 2022450597,
        "node_id": "PR_kwDOIWuq585g-uyS",
        "number": 9275,
        "title": "Add `anthropic.claude-v2:1` model tokens",
        "user": {
            "login": "Danipulok",
            "id": 45077699,
            "node_id": "MDQ6VXNlcjQ1MDc3Njk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/45077699?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Danipulok",
            "html_url": "https://github.com/Danipulok",
            "followers_url": "https://api.github.com/users/Danipulok/followers",
            "following_url": "https://api.github.com/users/Danipulok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Danipulok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Danipulok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Danipulok/subscriptions",
            "organizations_url": "https://api.github.com/users/Danipulok/orgs",
            "repos_url": "https://api.github.com/users/Danipulok/repos",
            "events_url": "https://api.github.com/users/Danipulok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Danipulok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-03T10:04:52Z",
        "updated_at": "2023-12-04T07:35:02Z",
        "closed_at": "2023-12-03T20:24:47Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9275",
            "html_url": "https://github.com/run-llama/llama_index/pull/9275",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9275.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9275.patch",
            "merged_at": "2023-12-03T20:24:47Z"
        },
        "body": "# Description\r\n\r\nAdd `anthropic.claude-v2:1` model tokens.\r\n\r\n## Type of Change\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\nRequests made manually.\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9275/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9275/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9274",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9274/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9274/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9274/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9274",
        "id": 2022416756,
        "node_id": "I_kwDOIWuq5854i6F0",
        "number": 9274,
        "title": "[Question]: RAG apps in other languages than english",
        "user": {
            "login": "stephanedebove",
            "id": 6358105,
            "node_id": "MDQ6VXNlcjYzNTgxMDU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6358105?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stephanedebove",
            "html_url": "https://github.com/stephanedebove",
            "followers_url": "https://api.github.com/users/stephanedebove/followers",
            "following_url": "https://api.github.com/users/stephanedebove/following{/other_user}",
            "gists_url": "https://api.github.com/users/stephanedebove/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stephanedebove/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stephanedebove/subscriptions",
            "organizations_url": "https://api.github.com/users/stephanedebove/orgs",
            "repos_url": "https://api.github.com/users/stephanedebove/repos",
            "events_url": "https://api.github.com/users/stephanedebove/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stephanedebove/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-03T08:12:15Z",
        "updated_at": "2023-12-03T08:22:47Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi everyone,\r\n\r\nthis is not a question related to Llama-index specifically, but I think you\u2019re the most qualified to answer with your experience in RAG apps.\r\n\r\nAlthough I have now read a lot of documentation and seen a lot of videos about building RAG apps, I haven\u2019t seen much about building RAG apps in other languages than english (specifically french in my case).\r\n\r\nI have seen that some embedding models are \"multilingual\", and the best one on the HuggingFace leaderboard is currently \"multilingual E5 large\". Does anyone have experience about using it? Does it make a big difference compared to non-multilingual models?\r\n\r\nAnd do you know if some LLMs used for response synthesis are better than others for dealing with non-english languages? This is never mentioned in their doc, but if we have multilingual models for embeddings, I don\u2019t see why we wouldn\u2019t have multilingual models for response synthesis too.\r\n\r\nDo you know any good resource talking about this subject, or do you have experience in this?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9274/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9274/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9273",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9273/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9273/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9273/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9273",
        "id": 2022296854,
        "node_id": "PR_kwDOIWuq585g-OkG",
        "number": 9273,
        "title": "LlamaDatasets Submission Template Notebook",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710958,
                "node_id": "LA_kwDOIWuq588AAAABc3-fLg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XXL",
                "name": "size:XXL",
                "color": "ffb8b8",
                "default": false,
                "description": "This PR changes 1000+ lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-03T02:28:30Z",
        "updated_at": "2023-12-04T02:48:44Z",
        "closed_at": "2023-12-04T02:48:43Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9273",
            "html_url": "https://github.com/run-llama/llama_index/pull/9273",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9273.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9273.patch",
            "merged_at": "2023-12-04T02:48:43Z"
        },
        "body": "# Description\r\n\r\n- Adds template notebook (`ragdataset_submission_template.ipynb`) to guide the llama-dataset submission process\r\n- Makes changes to module guides within `Evaluation`:\r\n     - Adds separate page for \"Contributing A `LabelledRagDataset` that links to the template notebook.\r\n \r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] New feature/notebook (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] Added new notebook (that tests end-to-end)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9273/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9273/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9272",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9272/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9272/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9272/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9272",
        "id": 2022232714,
        "node_id": "I_kwDOIWuq5854iNKK",
        "number": 9272,
        "title": "[Question]: Use local embeddings",
        "user": {
            "login": "stephanedebove",
            "id": 6358105,
            "node_id": "MDQ6VXNlcjYzNTgxMDU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6358105?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stephanedebove",
            "html_url": "https://github.com/stephanedebove",
            "followers_url": "https://api.github.com/users/stephanedebove/followers",
            "following_url": "https://api.github.com/users/stephanedebove/following{/other_user}",
            "gists_url": "https://api.github.com/users/stephanedebove/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stephanedebove/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stephanedebove/subscriptions",
            "organizations_url": "https://api.github.com/users/stephanedebove/orgs",
            "repos_url": "https://api.github.com/users/stephanedebove/repos",
            "events_url": "https://api.github.com/users/stephanedebove/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stephanedebove/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 14,
        "created_at": "2023-12-02T22:22:06Z",
        "updated_at": "2023-12-08T09:39:24Z",
        "closed_at": "2023-12-07T15:32:16Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI would like to use local embeddings using the multilingual-e5-large model specifically:\r\n\r\n```\r\nfrom llama_index.embeddings import HuggingFaceEmbedding\r\nembed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\")\r\n```\r\n\r\n I have read the documentation and many examples, but I\u2019ve found at least three different ways to do so:\r\n\r\n`service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local\")`\r\nor\r\n`service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:intfloat/multilingual-e5-large\")`\r\nor\r\n`service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)`\r\n\r\nWhich is the correct one? Only the first one is not giving me back \"'NoneType' object is not callable\" errors, but I don\u2019t see how service_context will know where to look for my embedding model by only passing a \"local\" string.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9272/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9272/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9271",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9271/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9271/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9271/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9271",
        "id": 2022209894,
        "node_id": "PR_kwDOIWuq585g987Q",
        "number": 9271,
        "title": "Update CONTRIBUTING.md",
        "user": {
            "login": "ColeMurray",
            "id": 2492022,
            "node_id": "MDQ6VXNlcjI0OTIwMjI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2492022?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ColeMurray",
            "html_url": "https://github.com/ColeMurray",
            "followers_url": "https://api.github.com/users/ColeMurray/followers",
            "following_url": "https://api.github.com/users/ColeMurray/following{/other_user}",
            "gists_url": "https://api.github.com/users/ColeMurray/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ColeMurray/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ColeMurray/subscriptions",
            "organizations_url": "https://api.github.com/users/ColeMurray/orgs",
            "repos_url": "https://api.github.com/users/ColeMurray/repos",
            "events_url": "https://api.github.com/users/ColeMurray/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ColeMurray/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-02T21:25:50Z",
        "updated_at": "2023-12-03T06:52:53Z",
        "closed_at": "2023-12-03T04:32:27Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9271",
            "html_url": "https://github.com/run-llama/llama_index/pull/9271",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9271.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9271.patch",
            "merged_at": "2023-12-03T04:32:27Z"
        },
        "body": "# Description\r\n\r\nFix Node PostProcessor References Due to Package Refactoring\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\nDocumentation\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x ] I have performed a self-review of my own code\r\n- [ x] I have made corresponding changes to the documentation\r\n- [x ] My changes generate no new warnings\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9271/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9271/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9270",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9270/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9270/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9270/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9270",
        "id": 2022193054,
        "node_id": "I_kwDOIWuq5854iDee",
        "number": 9270,
        "title": "local OpenAILike llm, when extracting database schema with ObjectIndex it always resorts to using OpenAI API",
        "user": {
            "login": "dinonovak",
            "id": 18152490,
            "node_id": "MDQ6VXNlcjE4MTUyNDkw",
            "avatar_url": "https://avatars.githubusercontent.com/u/18152490?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dinonovak",
            "html_url": "https://github.com/dinonovak",
            "followers_url": "https://api.github.com/users/dinonovak/followers",
            "following_url": "https://api.github.com/users/dinonovak/following{/other_user}",
            "gists_url": "https://api.github.com/users/dinonovak/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dinonovak/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dinonovak/subscriptions",
            "organizations_url": "https://api.github.com/users/dinonovak/orgs",
            "repos_url": "https://api.github.com/users/dinonovak/repos",
            "events_url": "https://api.github.com/users/dinonovak/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dinonovak/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-02T20:29:56Z",
        "updated_at": "2023-12-02T20:39:03Z",
        "closed_at": "2023-12-02T20:39:03Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am initiating llm and servicecontect with OpenAILike model declaration\r\n\r\nllm = OpenAILike(\r\n    api_base=\"http://localhost:1234/v1\",\r\n    api_key=\"111\", \r\n    is_chat_model=True,\r\n    is_local=True, \r\n    context_window=50000, \r\n    model=\"TheBloke/Orca-2-13B-GGUF\", \r\n    is_function_calling_model=True  # provide this field\r\n)\r\nsvc = ServiceContext.from_defaults(embed_model=\"local\", llm=llm)\r\n\r\nwhen trying to get database definition to local vector store:\r\n\r\nfrom llama_index.indices.struct_store import SQLTableRetrieverQueryEngine\r\nfrom llama_index.objects import (\r\n    SQLTableNodeMapping,\r\n    ObjectIndex,\r\n    SQLTableSchema,\r\n)\r\nfrom llama_index import VectorStoreIndex\r\n\r\ntable_node_mapping = SQLTableNodeMapping(sql_database)\r\n\r\ntable_schema_objs = []\r\nfor table_name in all_table_names:\r\n    table_schema_objs.append(SQLTableSchema(table_name=table_name))\r\n\r\nobj_index = ObjectIndex.from_objects(\r\n    table_schema_objs,\r\n    table_node_mapping,\r\n    VectorStoreIndex,\r\n)\r\n\r\nWhen creating obj_index calls are still being made to openai, how can I redirect it to local llm, shouldn't it use service context and llm that is defined there ?\r\n\r\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 401 Unauthorized\"\r\nHTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 401 Unauthorized\"\r\nWARNING:llama_index.llms.openai_utils:Retrying llama_index.embeddings.openai.get_embeddings in 0.13011903158574234 seconds as it raised AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NONE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}.\r\nRetrying llama_index.embeddings.openai.get_embeddings in 0.13011903158574234 seconds as it raised AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: NONE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9270/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9270/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9269",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9269/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9269/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9269/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9269",
        "id": 2022141907,
        "node_id": "I_kwDOIWuq5854h2_T",
        "number": 9269,
        "title": "[Feature Request]: Adding Table(s) to Knowledge graph building using LLM's",
        "user": {
            "login": "Kirushikesh",
            "id": 49152921,
            "node_id": "MDQ6VXNlcjQ5MTUyOTIx",
            "avatar_url": "https://avatars.githubusercontent.com/u/49152921?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Kirushikesh",
            "html_url": "https://github.com/Kirushikesh",
            "followers_url": "https://api.github.com/users/Kirushikesh/followers",
            "following_url": "https://api.github.com/users/Kirushikesh/following{/other_user}",
            "gists_url": "https://api.github.com/users/Kirushikesh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Kirushikesh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Kirushikesh/subscriptions",
            "organizations_url": "https://api.github.com/users/Kirushikesh/orgs",
            "repos_url": "https://api.github.com/users/Kirushikesh/repos",
            "events_url": "https://api.github.com/users/Kirushikesh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Kirushikesh/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-02T17:47:01Z",
        "updated_at": "2023-12-02T17:49:39Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Feature Description\n\nBuilding the table(s) to knowledge graph with the help of LLM's. Happy to take lead and start working for the PR :)\n\n### Reason\n\nQuerying and indexing the existing table is always a pain point with the size of table grows. As KnowledgeGraphIndex in llama-index is adapted for converting documents into knowledge graph, a similar advantages build when trying to convert a table into knowledge graph. \n\n### Value of Feature\n\nWith Table entities converted to knowledge graphs the implicit relations between the entities can be explicitly modelled, even the relations between different tables can be brought to a single graph which is easy to query and index compared to the tables.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9269/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9269/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9268",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9268/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9268/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9268/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9268",
        "id": 2022133029,
        "node_id": "I_kwDOIWuq5854h00l",
        "number": 9268,
        "title": "[Question]: Can not load my index created locally",
        "user": {
            "login": "stephanedebove",
            "id": 6358105,
            "node_id": "MDQ6VXNlcjYzNTgxMDU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6358105?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stephanedebove",
            "html_url": "https://github.com/stephanedebove",
            "followers_url": "https://api.github.com/users/stephanedebove/followers",
            "following_url": "https://api.github.com/users/stephanedebove/following{/other_user}",
            "gists_url": "https://api.github.com/users/stephanedebove/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stephanedebove/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stephanedebove/subscriptions",
            "organizations_url": "https://api.github.com/users/stephanedebove/orgs",
            "repos_url": "https://api.github.com/users/stephanedebove/repos",
            "events_url": "https://api.github.com/users/stephanedebove/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stephanedebove/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-02T17:21:48Z",
        "updated_at": "2023-12-02T20:05:06Z",
        "closed_at": "2023-12-02T17:24:16Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi,\r\n\r\nthanks for the great framework, I\u2019m having a lot of fun with it. I\u2019m trying to setup a RAG app using only local LLMs. Specifically Zephyr 7b as LLM and Multilingual E5 large for embedding. I have also created a persistent chromadb index:\r\n\r\n```\r\n\r\nllm = LlamaCPP(\r\n    model_path=\"./llms/zephyr-7b-alpha.Q5_K_M.gguf\",\r\n    temperature=0.0,\r\n    max_new_tokens=1024,\r\n    context_window=3900,  \r\n    model_kwargs={\"n_gpu_layers\": 4}, \r\n)\r\n\r\nfrom llama_index.embeddings import HuggingFaceEmbedding\r\nembed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-large\")\r\n\r\nservice_context = ServiceContext.from_defaults(\r\n    llm=llm, \r\n    embed_model=\"local\", \r\n    chunk_size=512, \r\n    chunk_overlap=20\r\n)\r\n\r\nimport chromadb\r\nfrom llama_index.vector_stores import ChromaVectorStore\r\nchroma_client = chromadb.PersistentClient()\r\nchroma_collection = chroma_client.create_collection(\"collection\", get_or_create = True)\r\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\r\n\r\ndocuments = SimpleDirectoryReader(\"data\").load_data()\r\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context, storage_context=storage_context)\r\nindex.storage_context.persist()\r\n   \r\n```\r\nSo far so good. Now, my problem is that when I try to load this index when I re-run my app, I get an error coming from this line of code:\r\n\r\n```\r\nindex = load_index_from_storage(storage_context, service_context=service_context)\r\n```\r\n\r\n```\r\nFile \"\\miniconda3\\envs\\llamaindex\\lib\\site-packages\\llama_index\\storage\\storage_context.py\", line 209, in vector_store\r\n    return self.vector_stores[DEFAULT_VECTOR_STORE]\r\nKeyError: 'default'\r\n```\r\n\r\nWhat am I doing wrong?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9268/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9268/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9267",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9267/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9267/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9267/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9267",
        "id": 2022116459,
        "node_id": "PR_kwDOIWuq585g9qml",
        "number": 9267,
        "title": "Fix automatic QDrant client construction",
        "user": {
            "login": "Omegastick",
            "id": 17452111,
            "node_id": "MDQ6VXNlcjE3NDUyMTEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/17452111?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Omegastick",
            "html_url": "https://github.com/Omegastick",
            "followers_url": "https://api.github.com/users/Omegastick/followers",
            "following_url": "https://api.github.com/users/Omegastick/following{/other_user}",
            "gists_url": "https://api.github.com/users/Omegastick/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Omegastick/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Omegastick/subscriptions",
            "organizations_url": "https://api.github.com/users/Omegastick/orgs",
            "repos_url": "https://api.github.com/users/Omegastick/repos",
            "events_url": "https://api.github.com/users/Omegastick/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Omegastick/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-12-02T16:38:24Z",
        "updated_at": "2023-12-04T14:10:43Z",
        "closed_at": "2023-12-04T00:32:40Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9267",
            "html_url": "https://github.com/run-llama/llama_index/pull/9267",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9267.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9267.patch",
            "merged_at": "2023-12-04T00:32:40Z"
        },
        "body": "# Description\r\n\r\nI think there's a typo in the `QDrantVectorStore` class. Forgive me if I'm actually misunderstanding how this is supposed to work, but the comma turns the object into a tuple, which the code further down the line doesn't expect.\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [X] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [X] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [X] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [X] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [X] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9267/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9267/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9266",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9266/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9266/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9266/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9266",
        "id": 2022116105,
        "node_id": "PR_kwDOIWuq585g9qi1",
        "number": 9266,
        "title": "make reference_context optional",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710935,
                "node_id": "LA_kwDOIWuq588AAAABc3-fFw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:M",
                "name": "size:M",
                "color": "ebb800",
                "default": false,
                "description": "This PR changes 30-99 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-02T16:37:26Z",
        "updated_at": "2023-12-02T20:23:33Z",
        "closed_at": "2023-12-02T20:23:32Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9266",
            "html_url": "https://github.com/run-llama/llama_index/pull/9266",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9266.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9266.patch",
            "merged_at": "2023-12-02T20:23:32Z"
        },
        "body": "# Description\r\n\r\n- Makes `reference_contexts` field optional in a `LabelledRagDataset` \r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9266/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9266/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9265",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9265/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9265/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9265/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9265",
        "id": 2022112255,
        "node_id": "PR_kwDOIWuq585g9p2j",
        "number": 9265,
        "title": "Bedrock embedding query for cohere was not in the correct format",
        "user": {
            "login": "deuscapturus",
            "id": 5497572,
            "node_id": "MDQ6VXNlcjU0OTc1NzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5497572?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/deuscapturus",
            "html_url": "https://github.com/deuscapturus",
            "followers_url": "https://api.github.com/users/deuscapturus/followers",
            "following_url": "https://api.github.com/users/deuscapturus/following{/other_user}",
            "gists_url": "https://api.github.com/users/deuscapturus/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/deuscapturus/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/deuscapturus/subscriptions",
            "organizations_url": "https://api.github.com/users/deuscapturus/orgs",
            "repos_url": "https://api.github.com/users/deuscapturus/repos",
            "events_url": "https://api.github.com/users/deuscapturus/events{/privacy}",
            "received_events_url": "https://api.github.com/users/deuscapturus/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-12-02T16:26:47Z",
        "updated_at": "2023-12-04T22:50:20Z",
        "closed_at": "2023-12-04T22:50:20Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9265",
            "html_url": "https://github.com/run-llama/llama_index/pull/9265",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9265.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9265.patch",
            "merged_at": "2023-12-04T22:50:20Z"
        },
        "body": "# Description\r\n\r\nFix: Bedrock get embedding was not formatting the request body properly for cohere models.  Instead of passing the valid json text format to the boto client, it was passing a json.parse() or whatever string was passed to `get_text_embedding`.\r\n\r\nThis also adds a unimplemented feature to get embeddings for queries as well and fixes type signatures for internal methods.\r\n\r\nExample:\r\n```\r\nembedding = BedrockEmbedding(\r\n    client=bedrock_client,\r\n    model_name=Models.COHERE_EMBED_ENGLISH_V3,\r\n)\r\n\r\nembedding.get_text_embedding(\"some text\")\r\n```\r\n\r\nResults in error:\r\n```\r\nbotocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request, please reformat your input and try again.\r\n```\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9265/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9265/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9264",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9264/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9264/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9264/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9264",
        "id": 2022110272,
        "node_id": "PR_kwDOIWuq585g9pdk",
        "number": 9264,
        "title": "Changes for MM docs and Pydantic Examples",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-02T16:20:56Z",
        "updated_at": "2023-12-02T16:52:10Z",
        "closed_at": "2023-12-02T16:52:10Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9264",
            "html_url": "https://github.com/run-llama/llama_index/pull/9264",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9264.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9264.patch",
            "merged_at": "2023-12-02T16:52:10Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9264/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9264/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9263",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9263/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9263/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9263/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9263",
        "id": 2022089511,
        "node_id": "I_kwDOIWuq5854hqMn",
        "number": 9263,
        "title": "[Bug]: UTF-8 error when getting the query response as Pydantic objects",
        "user": {
            "login": "ozzy1973",
            "id": 74435969,
            "node_id": "MDQ6VXNlcjc0NDM1OTY5",
            "avatar_url": "https://avatars.githubusercontent.com/u/74435969?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ozzy1973",
            "html_url": "https://github.com/ozzy1973",
            "followers_url": "https://api.github.com/users/ozzy1973/followers",
            "following_url": "https://api.github.com/users/ozzy1973/following{/other_user}",
            "gists_url": "https://api.github.com/users/ozzy1973/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ozzy1973/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ozzy1973/subscriptions",
            "organizations_url": "https://api.github.com/users/ozzy1973/orgs",
            "repos_url": "https://api.github.com/users/ozzy1973/repos",
            "events_url": "https://api.github.com/users/ozzy1973/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ozzy1973/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-02T15:36:34Z",
        "updated_at": "2023-12-02T15:42:48Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nI want to get query response as a Pydantic Object.\r\n\r\nThe object definition:\r\n\r\n`from typing import List\r\nfrom pydantic import BaseModel\r\n\r\nclass Topic(BaseModel):\r\n    \"\"\"Represents a topic extracted from a document.\"\"\"\r\n    name: str\r\n    description: str\r\n\r\nclass DocumentSummary(BaseModel):\r\n    \"\"\"Summary of a document with extracted topics.\"\"\"\r\n    document_title: str  # You might want to include the document's title\r\n    topics: List[Topic]`\r\n    \r\nQuery based on the object does not return proper UTF-8 encoded data.\r\n\r\n`query_engine_1 = index.as_query_engine(output_cls=DocumentSummary, response_mode=\"tree_summarize\")\r\nquery_engine_1.update_prompts(\r\n    {\"response_synthesizer:summary_template\": summary_tmpl}\r\n)\r\n\r\npydantic_summary = query_engine_1.query (\"Historia ksia\u017cki\")\r\nprint(pydantic_summary)`\r\n\r\nReturns:\r\n`{\"document_title\":\"Rany Julek str 1-13.pdf\",\"topics\":[{\"name\":\"Alchemia\",\"description\":\"Opis eksperyment\u00c3\u00b3w i mieszania substancji w celu stworzenia nowych produkt\u00c3\u00b3w.\"}...` \r\n\r\nWhere \u00c3\u00b3w (example) is improper character.\r\n\r\nQuery based on the text returns proper UTF-8 data.\r\n\r\n`query_engine_2 = index.as_query_engine(response_mode=\"tree_summarize\")\r\nquery_engine_2.update_prompts(\r\n    {\"response_synthesizer:summary_template\": summary_tmpl}\r\n)\r\ntext_summary = query_engine_2.query(\"Historia ksia\u017cki\")\r\nprint(text_summary)`\r\n\r\nReturns properly formated text in UTF-8. \n\n### Version\n\n0.9.7\n\n### Steps to Reproduce\n\nDescribed above - try to get the same answer as Pydantic Object and text (I used Polish language, but any encoding using UTF-8 will show the difference). \n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9263/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9263/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9262",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9262/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9262/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9262/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9262",
        "id": 2021967639,
        "node_id": "PR_kwDOIWuq585g9M29",
        "number": 9262,
        "title": "Fix node parser modules bug",
        "user": {
            "login": "nilecui",
            "id": 2919876,
            "node_id": "MDQ6VXNlcjI5MTk4NzY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2919876?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nilecui",
            "html_url": "https://github.com/nilecui",
            "followers_url": "https://api.github.com/users/nilecui/followers",
            "following_url": "https://api.github.com/users/nilecui/following{/other_user}",
            "gists_url": "https://api.github.com/users/nilecui/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nilecui/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nilecui/subscriptions",
            "organizations_url": "https://api.github.com/users/nilecui/orgs",
            "repos_url": "https://api.github.com/users/nilecui/repos",
            "events_url": "https://api.github.com/users/nilecui/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nilecui/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-02T09:36:56Z",
        "updated_at": "2023-12-02T16:52:40Z",
        "closed_at": "2023-12-02T16:52:40Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9262",
            "html_url": "https://github.com/run-llama/llama_index/pull/9262",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9262.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9262.patch",
            "merged_at": "2023-12-02T16:52:40Z"
        },
        "body": "# Description\r\n\r\nFix node parser modules bug\r\n\r\nfrom pathlib import Path\r\nmd_docs = FlatReader().load_data(Path(\"./test.md\"))\r\nparser = SimpleFileNodeParser()\r\nmd_nodes =parser.get_nodes_from_documents(md_docs)\r\n\r\n```text\r\n--------------------------------------------------------------------------\r\nllamaIndex_wk/demo/venv/lib/python3.9/site-packages/llama_index/readers/file/flat_reader.py:31)     metadata = {**metadata, **extra_info}\r\n\r\nAttributeError: 'str' object has no attribute 'name'\r\n```\r\n\r\ndocs/module_guides/loading/node_parsers/modules.md\r\n\r\n```python\r\nfrom llama_index.node_parser.file import SimpleFileNodeParser\r\nfrom llama_index.readers.file.flat_reader import FlatReader\r\n+ from pathlib import Path\r\n\r\n- md_docs = FlatReader().load_data(\"./test.md\")\r\n+ md_docs = FlatReader().load_data(Path(\"./test.md\"))\r\n\r\nparser = SimpleFileNodeParser()\r\nmd_nodes = parser.get_nodes_from_documents(md_docs)\r\n```\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9262/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9262/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9261",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9261/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9261/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9261/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9261",
        "id": 2021945467,
        "node_id": "I_kwDOIWuq5854hHB7",
        "number": 9261,
        "title": "OpenAILike model and API Error",
        "user": {
            "login": "dinonovak",
            "id": 18152490,
            "node_id": "MDQ6VXNlcjE4MTUyNDkw",
            "avatar_url": "https://avatars.githubusercontent.com/u/18152490?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dinonovak",
            "html_url": "https://github.com/dinonovak",
            "followers_url": "https://api.github.com/users/dinonovak/followers",
            "following_url": "https://api.github.com/users/dinonovak/following{/other_user}",
            "gists_url": "https://api.github.com/users/dinonovak/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dinonovak/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dinonovak/subscriptions",
            "organizations_url": "https://api.github.com/users/dinonovak/orgs",
            "repos_url": "https://api.github.com/users/dinonovak/repos",
            "events_url": "https://api.github.com/users/dinonovak/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dinonovak/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-02T08:35:19Z",
        "updated_at": "2023-12-02T08:40:57Z",
        "closed_at": "2023-12-02T08:40:57Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\r\n\r\n- [X] I have searched both the documentation and discord for an answer.\r\n\r\n### Question\r\n\r\nI set up LM Studio with TheBloke/Orca-2-13B-GGUF and am running it as server.\r\nAPI endpoint is alive in browser\r\nI am running llama-index-0.9.10\r\n\r\nI am trying to define OpenAILike model with this code:\r\n`import tqdm as notebook_tqdm\r\nfrom llama_index import SQLDatabase,ServiceContext\r\nfrom llama_index.llms import OpenAILike\r\nfrom sqlalchemy import select, create_engine, MetaData, Table,inspect\r\nfrom llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine\r\nprint(\"Okay starting\")\r\n\r\nllm = OpenAILike(\r\n    api_base=\"http://localhost:1234/v1\", \r\n    is_chat_model=True, \r\n    context_window=1024, \r\n    model=\"TheBloke/Orca-2-13B-GGUF\", \r\n    is_function_calling_model=True  # provide this field\r\n)`\r\n\r\nwhen trying to access model with\r\n`response = llm.complete(\"What can you tell me about the Ancient Aliens TV Series?\")\r\nprint(response.text)`\r\n\r\nI am getting API error below, any idea why is it popping up?\r\n\r\n\r\nLocalProtocolError                        Traceback (most recent call last)\r\nFile [~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_exceptions.py:10](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_exceptions.py:10), in map_exceptions(map)\r\n      [9](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_exceptions.py:9) try:\r\n---> [10](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_exceptions.py:10)     yield\r\n     [11](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_exceptions.py:11) except Exception as exc:  # noqa: PIE786\r\n\r\nFile [~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:142](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:142), in HTTP11Connection._send_request_headers(self, request)\r\n    [141](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:141) with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):\r\n--> [142](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:142)     event = h11.Request(\r\n    [143](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:143)         method=request.method,\r\n    [144](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:144)         target=request.url.target,\r\n    [145](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:145)         headers=request.headers,\r\n    [146](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:146)     )\r\n    [147](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:147) self._send_event(event, timeout=timeout)\r\n\r\nFile [~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_events.py:96](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_events.py:96), in Request.__init__(self, method, headers, target, http_version, _parsed)\r\n     [94](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_events.py:94) else:\r\n     [95](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_events.py:95)     object.__setattr__(\r\n---> [96](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_events.py:96)         self, \"headers\", normalize_and_validate(headers, _parsed=_parsed)\r\n     [97](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_events.py:97)     )\r\n     [98](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_events.py:98) if not _parsed:\r\n\r\nFile [~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_headers.py:164](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_headers.py:164), in normalize_and_validate(headers, _parsed)\r\n    [163](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/h11/_headers.py:163)     validate(_field_name_re, name, \"Illegal header name {!r}\", name)\r\n...\r\n   (...)\r\n    [912](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/openai/_base_client.py:912)     stream_cls=stream_cls,\r\n    [913](https://file+.vscode-resource.vscode-cdn.net/Users/dino/Codings/python/LLM_test1/~/Codings/python/LLM_test1/.venv/lib/python3.9/site-packages/openai/_base_client.py:913) )\r\n\r\nAPIConnectionError: Connection error.\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9261/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9261/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9260",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9260/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9260/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9260/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9260",
        "id": 2021833947,
        "node_id": "PR_kwDOIWuq585g8w2a",
        "number": 9260,
        "title": "Pinecone Metadata filter",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-02T03:25:43Z",
        "updated_at": "2023-12-02T15:27:16Z",
        "closed_at": "2023-12-02T15:27:15Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9260",
            "html_url": "https://github.com/run-llama/llama_index/pull/9260",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9260.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9260.patch",
            "merged_at": "2023-12-02T15:27:15Z"
        },
        "body": "# Description\r\n\r\nMetadata filter with advanced syntax for Pinecone\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9260/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9260/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9259",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9259/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9259/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9259/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9259",
        "id": 2021827393,
        "node_id": "PR_kwDOIWuq585g8vkF",
        "number": 9259,
        "title": "Update anthropic_utils.py",
        "user": {
            "login": "eugenepyvovarov",
            "id": 39050,
            "node_id": "MDQ6VXNlcjM5MDUw",
            "avatar_url": "https://avatars.githubusercontent.com/u/39050?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/eugenepyvovarov",
            "html_url": "https://github.com/eugenepyvovarov",
            "followers_url": "https://api.github.com/users/eugenepyvovarov/followers",
            "following_url": "https://api.github.com/users/eugenepyvovarov/following{/other_user}",
            "gists_url": "https://api.github.com/users/eugenepyvovarov/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/eugenepyvovarov/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/eugenepyvovarov/subscriptions",
            "organizations_url": "https://api.github.com/users/eugenepyvovarov/orgs",
            "repos_url": "https://api.github.com/users/eugenepyvovarov/repos",
            "events_url": "https://api.github.com/users/eugenepyvovarov/events{/privacy}",
            "received_events_url": "https://api.github.com/users/eugenepyvovarov/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-02T03:02:52Z",
        "updated_at": "2023-12-02T03:09:06Z",
        "closed_at": "2023-12-02T03:09:06Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9259",
            "html_url": "https://github.com/run-llama/llama_index/pull/9259",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9259.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9259.patch",
            "merged_at": "2023-12-02T03:09:06Z"
        },
        "body": "add new Claude model to the list\r\n\r\n# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9259/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9259/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9258",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9258/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9258/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9258/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9258",
        "id": 2021745899,
        "node_id": "PR_kwDOIWuq585g8er7",
        "number": 9258,
        "title": "pass output parser through to response synthesizers",
        "user": {
            "login": "zsimjee",
            "id": 6458766,
            "node_id": "MDQ6VXNlcjY0NTg3NjY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6458766?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zsimjee",
            "html_url": "https://github.com/zsimjee",
            "followers_url": "https://api.github.com/users/zsimjee/followers",
            "following_url": "https://api.github.com/users/zsimjee/following{/other_user}",
            "gists_url": "https://api.github.com/users/zsimjee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zsimjee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zsimjee/subscriptions",
            "organizations_url": "https://api.github.com/users/zsimjee/orgs",
            "repos_url": "https://api.github.com/users/zsimjee/repos",
            "events_url": "https://api.github.com/users/zsimjee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zsimjee/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-01T23:54:32Z",
        "updated_at": "2023-12-11T20:02:50Z",
        "closed_at": "2023-12-11T20:02:50Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9258",
            "html_url": "https://github.com/run-llama/llama_index/pull/9258",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9258.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9258.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n- [x] Updated outputparser notebook to ensure it works\r\n- [x] Ran existing tests to make sure stuff works \r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9258/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9258/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9257",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9257/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9257/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9257/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9257",
        "id": 2021742968,
        "node_id": "PR_kwDOIWuq585g8eCQ",
        "number": 9257,
        "title": "Added support for vllm framework",
        "user": {
            "login": "yureikami",
            "id": 127286701,
            "node_id": "U_kgDOB5Y9rQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/127286701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yureikami",
            "html_url": "https://github.com/yureikami",
            "followers_url": "https://api.github.com/users/yureikami/followers",
            "following_url": "https://api.github.com/users/yureikami/following{/other_user}",
            "gists_url": "https://api.github.com/users/yureikami/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yureikami/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yureikami/subscriptions",
            "organizations_url": "https://api.github.com/users/yureikami/orgs",
            "repos_url": "https://api.github.com/users/yureikami/repos",
            "events_url": "https://api.github.com/users/yureikami/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yureikami/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710958,
                "node_id": "LA_kwDOIWuq588AAAABc3-fLg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XXL",
                "name": "size:XXL",
                "color": "ffb8b8",
                "default": false,
                "description": "This PR changes 1000+ lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-01T23:50:11Z",
        "updated_at": "2023-12-05T01:59:51Z",
        "closed_at": "2023-12-05T01:59:51Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9257",
            "html_url": "https://github.com/run-llama/llama_index/pull/9257",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9257.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9257.patch",
            "merged_at": "2023-12-05T01:59:51Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\nI have Included the support for VLLM framework and VLLM api server. VLLM is one the top leading inferencing servers at the moment and I think there is no dedicated support since the release of new openAI client which breaks the existing bootleg implementation. This request fixes that challenge by introducing dedicated support for VLLM.\r\nFixes # ([issue](https://github.com/run-llama/llama_index/issues/7818)) \r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nI have written a Pytest Script which checks for imports and running a minimalistic example of the Implementation.\r\n\r\n- [x] Added new unit/integration tests\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [x] My changes generate no new warnings\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9257/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9257/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9256",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9256/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9256/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9256/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9256",
        "id": 2021738567,
        "node_id": "PR_kwDOIWuq585g8dEA",
        "number": 9256,
        "title": "Added call to Llamahub proxy to track download numbers",
        "user": {
            "login": "pankhudib1234",
            "id": 11802037,
            "node_id": "MDQ6VXNlcjExODAyMDM3",
            "avatar_url": "https://avatars.githubusercontent.com/u/11802037?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pankhudib1234",
            "html_url": "https://github.com/pankhudib1234",
            "followers_url": "https://api.github.com/users/pankhudib1234/followers",
            "following_url": "https://api.github.com/users/pankhudib1234/following{/other_user}",
            "gists_url": "https://api.github.com/users/pankhudib1234/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pankhudib1234/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pankhudib1234/subscriptions",
            "organizations_url": "https://api.github.com/users/pankhudib1234/orgs",
            "repos_url": "https://api.github.com/users/pankhudib1234/repos",
            "events_url": "https://api.github.com/users/pankhudib1234/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pankhudib1234/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710935,
                "node_id": "LA_kwDOIWuq588AAAABc3-fFw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:M",
                "name": "size:M",
                "color": "ebb800",
                "default": false,
                "description": "This PR changes 30-99 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-01T23:44:33Z",
        "updated_at": "2023-12-02T01:51:35Z",
        "closed_at": "2023-12-02T01:51:34Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9256",
            "html_url": "https://github.com/run-llama/llama_index/pull/9256",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9256.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9256.patch",
            "merged_at": "2023-12-02T01:51:34Z"
        },
        "body": "# Description\r\nAdded tracking code in following functions\r\n* download_loader\r\n* download_tool\r\n* download_llama_pack\r\n* download_lllama_datasets\r\n\r\nFixes # (issue)\r\nNA \r\n\r\n## Type of Change\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\nUsing python shell. Verified that Llama proxy is hit successfully & download count is updated for each of the four plugin types. \r\n\r\n# Todo\r\nHow can we track the downloads when download-utils is not called, i.e., when users directly imports a package.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9256/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9256/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9255",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9255/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9255/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9255/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9255",
        "id": 2021685339,
        "node_id": "PR_kwDOIWuq585g8RN-",
        "number": 9255,
        "title": "convert numpy to list for InstructorEmbedding",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-01T22:43:57Z",
        "updated_at": "2023-12-01T22:51:25Z",
        "closed_at": "2023-12-01T22:51:24Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9255",
            "html_url": "https://github.com/run-llama/llama_index/pull/9255",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9255.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9255.patch",
            "merged_at": "2023-12-01T22:51:24Z"
        },
        "body": "# Description\r\n\r\n`InstructorEmbedding` was outputting numpy arrays, causing issues for vector stores and serialization.\r\n\r\n## Type of Change\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9255/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9255/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9254",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9254/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9254/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9254/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9254",
        "id": 2021361193,
        "node_id": "I_kwDOIWuq5854e4Yp",
        "number": 9254,
        "title": "[Bug]: ReActOutputParser fails to parse Action Input if the json contains nested values",
        "user": {
            "login": "snazzer",
            "id": 6411902,
            "node_id": "MDQ6VXNlcjY0MTE5MDI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6411902?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/snazzer",
            "html_url": "https://github.com/snazzer",
            "followers_url": "https://api.github.com/users/snazzer/followers",
            "following_url": "https://api.github.com/users/snazzer/following{/other_user}",
            "gists_url": "https://api.github.com/users/snazzer/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/snazzer/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/snazzer/subscriptions",
            "organizations_url": "https://api.github.com/users/snazzer/orgs",
            "repos_url": "https://api.github.com/users/snazzer/repos",
            "events_url": "https://api.github.com/users/snazzer/events{/privacy}",
            "received_events_url": "https://api.github.com/users/snazzer/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-01T18:10:18Z",
        "updated_at": "2023-12-01T18:12:33Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nIf resulting Action Input from a React flow includes nested json, the regular expression doesn't include the last enclosing '}'\r\n\r\nEasy fix would be to modify the expression in `llama_index/agent/react/output_parser.py`\r\n\r\nfrom\r\n```\r\n    pattern = r\"\\s*Thought: (.*?)\\nAction: (.*?)\\nAction Input: (\\{.*?\\})\"\r\n```\r\n\r\nto\r\n```\r\n    pattern = r\"\\s*Thought: (.*?)\\nAction: (.*?)\\nAction Input: (\\{.*\\})\"\r\n```\r\n\r\nMaking the last group a greedy parse.\n\n### Version\n\n 0.9.6.post1\n\n### Steps to Reproduce\n\nConsider the following\r\n```\r\n>>> tst = r\"\"\"\r\n... Thought: <some thought>\r\n... Action: python_asteval\r\n... Action Input: {\"python_expression\": \"{'494b035813': 0.5, '9d84b101f7': 0.5996664746664747, 'ce1ab7b0c8': 0.5996632871632872, 'f8f6fc4526': 0.5996632246632246, 'ee8a066b2b': 0.5996632246632246}\"}\r\n... \"\"\"\r\n\r\n>>> from llama_index.agent.react.output_parser import extract_tool_use\r\n>>> extract_tool_use(tst)\r\n>>> extract_tool_use(tst)[2]\r\n'{\"python_expression\": \"{\\'494b035813\\': 0.5, \\'9d84b101f7\\': 0.5996664746664747, \\'ce1ab7b0c8\\': 0.5996632871632872, \\'f8f6fc4526\\': 0.5996632246632246, \\'ee8a066b2b\\': 0.5996632246632246}'\r\n```\r\n\r\nNote that the last '}' was not included in the parsed Action Input.\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9254/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9254/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9253",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9253/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9253/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9253/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9253",
        "id": 2021318548,
        "node_id": "PR_kwDOIWuq585g7Bw8",
        "number": 9253,
        "title": "Move download datasets logic our of download_utils.py",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710949,
                "node_id": "LA_kwDOIWuq588AAAABc3-fJQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XL",
                "name": "size:XL",
                "color": "ff823f",
                "default": false,
                "description": "This PR changes 500-999 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-01T17:36:47Z",
        "updated_at": "2023-12-02T20:13:29Z",
        "closed_at": "2023-12-02T20:13:28Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9253",
            "html_url": "https://github.com/run-llama/llama_index/pull/9253",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9253.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9253.patch",
            "merged_at": "2023-12-02T20:13:28Z"
        },
        "body": "# Description\r\n\r\n- Moves our `download_llama_dataset` logic from `download/download_utils.py`\r\n- Adds new `utils.py` in `download` module -- propose to put here common download utils for both datasets and modules\r\n- Propose to rename `download_utils.py` to `module.py`\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9253/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9253/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9252",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9252/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9252/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9252/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9252",
        "id": 2021300852,
        "node_id": "PR_kwDOIWuq585g6934",
        "number": 9252,
        "title": "add headers to openai embedding clients",
        "user": {
            "login": "riccardobucco",
            "id": 9295277,
            "node_id": "MDQ6VXNlcjkyOTUyNzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9295277?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/riccardobucco",
            "html_url": "https://github.com/riccardobucco",
            "followers_url": "https://api.github.com/users/riccardobucco/followers",
            "following_url": "https://api.github.com/users/riccardobucco/following{/other_user}",
            "gists_url": "https://api.github.com/users/riccardobucco/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/riccardobucco/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/riccardobucco/subscriptions",
            "organizations_url": "https://api.github.com/users/riccardobucco/orgs",
            "repos_url": "https://api.github.com/users/riccardobucco/repos",
            "events_url": "https://api.github.com/users/riccardobucco/events{/privacy}",
            "received_events_url": "https://api.github.com/users/riccardobucco/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-01T17:26:35Z",
        "updated_at": "2023-12-01T21:37:09Z",
        "closed_at": "2023-12-01T21:37:08Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9252",
            "html_url": "https://github.com/run-llama/llama_index/pull/9252",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9252.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9252.patch",
            "merged_at": "2023-12-01T21:37:08Z"
        },
        "body": "# Description\r\n\r\nCurrently, we can add custom headers to `llama_index.llms.openai.OpenAI` (see [9082)](https://github.com/run-llama/llama_index/issues/9082) but not to `llama_index.embeddings.openai.OpenAIEmbedding`. I.e.\r\n\r\n```\r\nllm = AzureOpenAI(\r\n    ...\r\n    default_headers={\r\n        'key': 'value',\r\n    },\r\n)\r\n```\r\ncorrectly works, while\r\n```\r\nembed_model = AzureOpenAIEmbedding(\r\n    ...\r\n    default_headers={\r\n        'key': 'value',\r\n    },\r\n)\r\n```\r\ndoesn't work.\r\n\r\nThis PR fixes that and allows users to add custom headers also while using embeddings.\r\n\r\nFixes # [9211](https://github.com/run-llama/llama_index/issues/9211)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ x] I have performed a self-review of my own code\r\n- [ x] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9252/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9252/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9251",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9251/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9251/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9251/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9251",
        "id": 2021263197,
        "node_id": "I_kwDOIWuq5854egdd",
        "number": 9251,
        "title": "[Question]:  \"Initializing BM25 Retriever Using Pre-filled Vector Store Data",
        "user": {
            "login": "Leonschmitt",
            "id": 28598834,
            "node_id": "MDQ6VXNlcjI4NTk4ODM0",
            "avatar_url": "https://avatars.githubusercontent.com/u/28598834?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Leonschmitt",
            "html_url": "https://github.com/Leonschmitt",
            "followers_url": "https://api.github.com/users/Leonschmitt/followers",
            "following_url": "https://api.github.com/users/Leonschmitt/following{/other_user}",
            "gists_url": "https://api.github.com/users/Leonschmitt/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Leonschmitt/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Leonschmitt/subscriptions",
            "organizations_url": "https://api.github.com/users/Leonschmitt/orgs",
            "repos_url": "https://api.github.com/users/Leonschmitt/repos",
            "events_url": "https://api.github.com/users/Leonschmitt/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Leonschmitt/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-01T17:01:09Z",
        "updated_at": "2023-12-01T21:34:00Z",
        "closed_at": "2023-12-01T21:33:59Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am developing a Streamlit application that leverages LlamaIndex, and I'm attempting to integrate a BM25 Retriever as outlined in a tutorial available [here](https://docs.llamaindex.ai/en/latest/examples/retrievers/reciprocal_rerank_fusion.html#). My current challenge involves initializing the BM25 Retriever using an existing Weaviate vector store.\r\n\r\nHere's a snippet of my code:\r\n```\r\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\r\n\r\nvector_retriever = index.as_retriever(similarity_top_k=5)\r\n\r\nbm25_retriever = BM25Retriever.from_defaults(\r\n   docstore=index.docstore, similarity_top_k=5\r\n)\r\n```\r\n\r\n\r\nWhen executing this, I run into a **ZeroDivisionErro**r during the initialization of the BM25Retriever. My application setup includes a Weaviate database (vector_store) where PDF documents are loaded using a SimpleDirectoryReader. The data is processed with a SentenceSplitter and a TitleExtractor, and embeddings are created for each node (node), which are then stored using vector_store.add(nodes).  Here is my code snippet: \r\n\r\n```\r\nvector_store = WeaviateVectorStore(\r\n    weaviate_client=client, index_name=\"Intranet\"\r\n)\r\nnode_parser = SentenceSplitter(chunk_size=480)\r\nextractor = TitleExtractor(llm=llm)\r\n\r\n# use transforms directly\r\nnodes = node_parser(docs)\r\nnodes = extractor(nodes)\r\nfor node in nodes:\r\n    node_embedding = embed_model.get_text_embedding(\r\n        node.get_content(metadata_mode=\"all\")\r\n    )\r\n    node.embedding = node_embedding\r\nvector_store.add(nodes)\r\n```\r\n\r\nI sought solutions on Discord, but the common advice pointed to the necessity of supplying nodes to the BM25 Retriever. However, my preprocessing and storing processes are outsourced to a service and done prior in time. This leads me to question how to initialize the BM25 Retriever when I only have access to an already existing Weaviate vector store.\r\n\r\nI am unsure if I have overlooked something or made a mistake in the process. I am reaching out to see if anyone can relate to this problem or provide insights into resolving it.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9251/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9251/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9250",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9250/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9250/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9250/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9250",
        "id": 2021080752,
        "node_id": "PR_kwDOIWuq585g6Ngi",
        "number": 9250,
        "title": "Add pydantic tests for more Multi-Modal LLMs",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-01T15:15:08Z",
        "updated_at": "2023-12-01T16:58:37Z",
        "closed_at": "2023-12-01T16:58:36Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9250",
            "html_url": "https://github.com/run-llama/llama_index/pull/9250",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9250.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9250.patch",
            "merged_at": "2023-12-01T16:58:36Z"
        },
        "body": "# Description\r\n\r\n* Support CogVLM\r\n* Add more Pydantic tests for other LLMs\r\n\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9250/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9250/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9249",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9249/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9249/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9249/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9249",
        "id": 2020937521,
        "node_id": "PR_kwDOIWuq585g5ugZ",
        "number": 9249,
        "title": "Fix issue with irregular table (#9130)",
        "user": {
            "login": "hexapode",
            "id": 208554,
            "node_id": "MDQ6VXNlcjIwODU1NA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/208554?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hexapode",
            "html_url": "https://github.com/hexapode",
            "followers_url": "https://api.github.com/users/hexapode/followers",
            "following_url": "https://api.github.com/users/hexapode/following{/other_user}",
            "gists_url": "https://api.github.com/users/hexapode/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hexapode/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hexapode/subscriptions",
            "organizations_url": "https://api.github.com/users/hexapode/orgs",
            "repos_url": "https://api.github.com/users/hexapode/repos",
            "events_url": "https://api.github.com/users/hexapode/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hexapode/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-12-01T14:00:05Z",
        "updated_at": "2023-12-02T17:00:40Z",
        "closed_at": "2023-12-02T17:00:40Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9249",
            "html_url": "https://github.com/run-llama/llama_index/pull/9249",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9249.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9249.patch",
            "merged_at": "2023-12-02T17:00:40Z"
        },
        "body": "# Description\r\n\r\nOn table that where the  number of columns varie per rows, ```node_parser.unstructured _element``` was failing.\r\n\r\nTo address this issue:\r\n- Added a test to check if the number of columns of the different rows of a table is consistent.\r\n\r\nI also change the behaviours of ```node_parser.unstructured _element.extract_elements``` so that if a table fail to parse it is converted into a ```HTMLText``` elements, in order to not loose content.\r\n\r\nI also added a test with regular and irregular tables, that should failed if we loose the ability to detect irregular tables.\r\n\r\nFixes #9130\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [x] I have added tests that prove my fix is effective or that my feature works\r\n- [x] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9249/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9249/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9248",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9248/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9248/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9248/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9248",
        "id": 2020878951,
        "node_id": "PR_kwDOIWuq585g5hoE",
        "number": 9248,
        "title": "Update sbert_rerank.py: added \"device\" parameter to SentenceTransform\u2026",
        "user": {
            "login": "danilyef",
            "id": 12939044,
            "node_id": "MDQ6VXNlcjEyOTM5MDQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/12939044?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/danilyef",
            "html_url": "https://github.com/danilyef",
            "followers_url": "https://api.github.com/users/danilyef/followers",
            "following_url": "https://api.github.com/users/danilyef/following{/other_user}",
            "gists_url": "https://api.github.com/users/danilyef/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/danilyef/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/danilyef/subscriptions",
            "organizations_url": "https://api.github.com/users/danilyef/orgs",
            "repos_url": "https://api.github.com/users/danilyef/repos",
            "events_url": "https://api.github.com/users/danilyef/events{/privacy}",
            "received_events_url": "https://api.github.com/users/danilyef/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-01T13:25:28Z",
        "updated_at": "2023-12-01T16:41:57Z",
        "closed_at": "2023-12-01T16:41:15Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9248",
            "html_url": "https://github.com/run-llama/llama_index/pull/9248",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9248.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9248.patch",
            "merged_at": "2023-12-01T16:41:15Z"
        },
        "body": "\u2026erRerank\r\n\r\n added \"device\" parameter to SentenceTransformerRerank. It allows to use \"cpu\", when cuda is not available.\r\n\r\n# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Bug fix (non-breaking change which fixes an issue)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9248/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9248/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9247",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9247/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9247/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9247/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9247",
        "id": 2020752602,
        "node_id": "I_kwDOIWuq5854cjza",
        "number": 9247,
        "title": "[Question]: Can I use SentenceTransformerRerank model on cpu?",
        "user": {
            "login": "danilyef",
            "id": 12939044,
            "node_id": "MDQ6VXNlcjEyOTM5MDQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/12939044?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/danilyef",
            "html_url": "https://github.com/danilyef",
            "followers_url": "https://api.github.com/users/danilyef/followers",
            "following_url": "https://api.github.com/users/danilyef/following{/other_user}",
            "gists_url": "https://api.github.com/users/danilyef/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/danilyef/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/danilyef/subscriptions",
            "organizations_url": "https://api.github.com/users/danilyef/orgs",
            "repos_url": "https://api.github.com/users/danilyef/repos",
            "events_url": "https://api.github.com/users/danilyef/events{/privacy}",
            "received_events_url": "https://api.github.com/users/danilyef/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-12-01T12:04:52Z",
        "updated_at": "2023-12-01T17:00:52Z",
        "closed_at": "2023-12-01T17:00:52Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI want to use SentenceTransformerRerank model for the Sentence-Window Retrieval RAG.\r\n\r\nI do the following:\r\n\r\n```\r\nfrom llama_index.indices.postprocessor import MetadataReplacementPostProcessor\r\n\r\npostproc = MetadataReplacementPostProcessor(\r\n    target_metadata_key=\"window\"\r\n)\r\n\r\n\r\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\r\n\r\nrerank = SentenceTransformerRerank(\r\n    top_n=2, model=\"BAAI/bge-reranker-base\"\r\n)\r\n```\r\n\r\nthan I create a query engine:\r\n\r\n```\r\n# Query engine:\r\nsentence_window_engine = sentence_index.as_query_engine(\r\n    similarity_top_k=6, node_postprocessors=[postproc, rerank]\r\n)\r\n```\r\n\r\nand when I try to test the result I get the following error:\r\n\r\n```\r\n# Test:\r\n\r\nwindow_response = sentence_window_engine.query(\r\n    \"Tell me about Net Tarif\"\r\n)\r\n```\r\n\r\nOutOfMemoryError: CUDA out of memory.. Error happens in postprocessor\r\n\r\n- File /usr/local/lib/python3.8/dist-packages/llama_index/postprocessor/types.py:48, in BaseNodePostprocessor.postprocess_nodes(self, nodes, query_bundle, query_str)\r\n- File /usr/local/lib/python3.8/dist-packages/llama_index/postprocessor/sbert_rerank.py:64, in SentenceTransformerRerank._postprocess_nodes(self, nodes, query_bundle)\r\n- File /usr/local/lib/python3.8/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:272\r\n\r\nSo my question is: can I move SentenceTransformerRerank to cpu?\r\n\r\nWhen I tried to do the following I get and error:\r\n\r\n```\r\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\r\nfrom sentence_transformers import SentenceTransformer\r\nmodel_id = \"BAAI/bge-reranker-large\"\r\nmodel = SentenceTransformer(model_id,device = 'cpu')\r\n\r\nrerank = SentenceTransformerRerank(\r\n    top_n=2, model=model \r\n)\r\n```\r\n\r\n> OSError: Can't load the configuration of 'SentenceTransformer(\r\n>   (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \r\n>   (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\r\n> )'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'SentenceTransformer(\r\n>   (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \r\n>   (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\r\n> )' is the correct path to a directory containing a config.json file\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9247/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9247/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9246",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9246/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9246/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9246/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9246",
        "id": 2020309404,
        "node_id": "I_kwDOIWuq5854a3mc",
        "number": 9246,
        "title": "[Bug]:  `pkg_resources` is deprecated and should be replaced with `importlib.resourcs`",
        "user": {
            "login": "baggiponte",
            "id": 57922983,
            "node_id": "MDQ6VXNlcjU3OTIyOTgz",
            "avatar_url": "https://avatars.githubusercontent.com/u/57922983?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/baggiponte",
            "html_url": "https://github.com/baggiponte",
            "followers_url": "https://api.github.com/users/baggiponte/followers",
            "following_url": "https://api.github.com/users/baggiponte/following{/other_user}",
            "gists_url": "https://api.github.com/users/baggiponte/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/baggiponte/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/baggiponte/subscriptions",
            "organizations_url": "https://api.github.com/users/baggiponte/orgs",
            "repos_url": "https://api.github.com/users/baggiponte/repos",
            "events_url": "https://api.github.com/users/baggiponte/events{/privacy}",
            "received_events_url": "https://api.github.com/users/baggiponte/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-01T07:49:31Z",
        "updated_at": "2023-12-01T08:03:42Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\n`pkg_resources` is [deprecated and should be replaced](https://setuptools.pypa.io/en/latest/pkg_resources.html). Appears to be used in [two files](https://github.com/search?q=repo:run-llama/llama_index+pkg_resources&type=code) only anyway.\n\n### Version\n\n0.9.10\n\n### Steps to Reproduce\n\nInstall `llama-index` in a clean environment **without `setuptools`**.\r\n\r\nRun anything, e.g. `python -c \"import llama_index; print(llama_index.__version__)\"` and an error should be raised.\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9246/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9246/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9245",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9245/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9245/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9245/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9245",
        "id": 2020089992,
        "node_id": "I_kwDOIWuq5854aCCI",
        "number": 9245,
        "title": "[Question]: How to delete nodes using node ids from index which is VectorStoreIndex instance when created directly using nodes instead of documents?",
        "user": {
            "login": "pavansandeep2910",
            "id": 55790895,
            "node_id": "MDQ6VXNlcjU1NzkwODk1",
            "avatar_url": "https://avatars.githubusercontent.com/u/55790895?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/pavansandeep2910",
            "html_url": "https://github.com/pavansandeep2910",
            "followers_url": "https://api.github.com/users/pavansandeep2910/followers",
            "following_url": "https://api.github.com/users/pavansandeep2910/following{/other_user}",
            "gists_url": "https://api.github.com/users/pavansandeep2910/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/pavansandeep2910/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/pavansandeep2910/subscriptions",
            "organizations_url": "https://api.github.com/users/pavansandeep2910/orgs",
            "repos_url": "https://api.github.com/users/pavansandeep2910/repos",
            "events_url": "https://api.github.com/users/pavansandeep2910/events{/privacy}",
            "received_events_url": "https://api.github.com/users/pavansandeep2910/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-12-01T05:15:47Z",
        "updated_at": "2023-12-05T03:07:37Z",
        "closed_at": "2023-12-05T03:07:37Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHow to delete nodes using node ids from the index which is a VectorStoreIndex instance when created directly using nodes instead of documents?\r\n\r\nI tried the delete_nodes method to delete nodes using node IDs. but got the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/pavan/Documents/GitHub/sentinel-ai-analytics-backend/scripts/create_few_shot_samples_embeddings.py\", line 81, in <module>\r\n    index.delete_nodes(deletableNodeIds)\r\n  File \"/Users/pavan/Documents/GitHub/sentinel-ai-analytics-backend/venv/lib/python3.11/site-packages/llama_index/indices/vector_store/base.py\", line 286, in delete_nodes\r\n    raise NotImplementedError(\r\nNotImplementedError: Vector indices currently only support delete_ref_doc, which deletes nodes using the ref_doc_id of ingested documents.\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9245/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9245/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9244",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9244/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9244/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9244/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9244",
        "id": 2020042087,
        "node_id": "I_kwDOIWuq5854Z2Vn",
        "number": 9244,
        "title": "[Bug]: K\u00f9zu Graph Store, RuntimeError: Parameters must be a dict",
        "user": {
            "login": "Bing-su",
            "id": 37621276,
            "node_id": "MDQ6VXNlcjM3NjIxMjc2",
            "avatar_url": "https://avatars.githubusercontent.com/u/37621276?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Bing-su",
            "html_url": "https://github.com/Bing-su",
            "followers_url": "https://api.github.com/users/Bing-su/followers",
            "following_url": "https://api.github.com/users/Bing-su/following{/other_user}",
            "gists_url": "https://api.github.com/users/Bing-su/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Bing-su/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Bing-su/subscriptions",
            "organizations_url": "https://api.github.com/users/Bing-su/orgs",
            "repos_url": "https://api.github.com/users/Bing-su/repos",
            "events_url": "https://api.github.com/users/Bing-su/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Bing-su/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-01T04:35:13Z",
        "updated_at": "2023-12-01T04:42:26Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nWhile following the [KuzuGraphDemo](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KuzuGraphDemo.html), I got an RuntimeError.\r\n\r\nPlease view the full log in my colab notebook.\r\n\r\n### Version\r\n\r\nllama_index: 0.9.10\r\n\r\nkuzu: 0.1.0\r\n\r\n### Steps to Reproduce\r\n\r\nhere's the colab notebook: https://colab.research.google.com/drive/10cRzBdRZxmUbBPnWw_dJJWZLQVuptNZs?usp=sharing\r\n\r\n```py\r\nindex = KnowledgeGraphIndex.from_documents(\r\n    documents,\r\n    max_triplets_per_chunk=2,\r\n    storage_context=storage_context,\r\n    service_context=service_context,\r\n)\r\n```\r\n\r\nError occurs at this step.\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n```shell\r\n\u2502 /usr/local/lib/python3.10/dist-packages/llama_index/indices/knowledge_graph/base.py:221 in       \u2502\r\n\u2502 upsert_triplet                                                                                   \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   218 \u2502   \u2502   \u2502   triplet (str): Knowledge triplet                                               \u2502\r\n\u2502   219 \u2502   \u2502                                                                                      \u2502\r\n\u2502   220 \u2502   \u2502   \"\"\"                                                                                \u2502\r\n\u2502 \u2771 221 \u2502   \u2502   self._graph_store.upsert_triplet(*triplet)                                         \u2502\r\n\u2502   222 \u2502                                                                                          \u2502\r\n\u2502   223 \u2502   def add_node(self, keywords: List[str], node: BaseNode) -> None:                       \u2502\r\n\u2502   224 \u2502   \u2502   \"\"\"Add node.                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\r\n\u2502 \u2502    self = <llama_index.indices.knowledge_graph.base.KnowledgeGraphIndex object at            \u2502 \u2502\r\n\u2502 \u2502           0x77184e9b2020>                                                                    \u2502 \u2502\r\n\u2502 \u2502 triplet = ('I', 'worked on', 'writing')                                                      \u2502 \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/llama_index/graph_stores/kuzu.py:153 in upsert_triplet   \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   150 \u2502   \u2502   \u2502   \u2502   [(\"subj\", subj), (\"obj\", obj), (\"pred\", rel)],                             \u2502\r\n\u2502   151 \u2502   \u2502   \u2502   )                                                                              \u2502\r\n\u2502   152 \u2502   \u2502                                                                                      \u2502\r\n\u2502 \u2771 153 \u2502   \u2502   is_subj_exists = check_entity_exists(self.connection, subj)                        \u2502\r\n\u2502   154 \u2502   \u2502   is_obj_exists = check_entity_exists(self.connection, obj)                          \u2502\r\n\u2502   155 \u2502   \u2502                                                                                      \u2502\r\n\u2502   156 \u2502   \u2502   if not is_subj_exists:                                                             \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\r\n\u2502 \u2502 check_entity_exists = <function KuzuGraphStore.upsert_triplet.<locals>.check_entity_exists   \u2502 \u2502\r\n\u2502 \u2502                       at 0x77184e8b2d40>                                                     \u2502 \u2502\r\n\u2502 \u2502    check_rel_exists = <function KuzuGraphStore.upsert_triplet.<locals>.check_rel_exists at   \u2502 \u2502\r\n\u2502 \u2502                       0x77184e8b28c0>                                                        \u2502 \u2502\r\n\u2502 \u2502       create_entity = <function KuzuGraphStore.upsert_triplet.<locals>.create_entity at      \u2502 \u2502\r\n\u2502 \u2502                       0x77184e8b2830>                                                        \u2502 \u2502\r\n\u2502 \u2502          create_rel = <function KuzuGraphStore.upsert_triplet.<locals>.create_rel at         \u2502 \u2502\r\n\u2502 \u2502                       0x77184e8b15a0>                                                        \u2502 \u2502\r\n\u2502 \u2502                 obj = 'writing'                                                              \u2502 \u2502\r\n\u2502 \u2502                 rel = 'worked on'                                                            \u2502 \u2502\r\n\u2502 \u2502                self = <llama_index.graph_stores.kuzu.KuzuGraphStore object at                \u2502 \u2502\r\n\u2502 \u2502                       0x7f1aeb961960>                                                        \u2502 \u2502\r\n\u2502 \u2502                subj = 'I'                                                                    \u2502 \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/llama_index/graph_stores/kuzu.py:118 in                  \u2502\r\n\u2502 check_entity_exists                                                                              \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   115 \u2502   \u2502   \"\"\"Add triplet.\"\"\"                                                                 \u2502\r\n\u2502   116 \u2502   \u2502                                                                                      \u2502\r\n\u2502   117 \u2502   \u2502   def check_entity_exists(connection: Any, entity: str) -> bool:                     \u2502\r\n\u2502 \u2771 118 \u2502   \u2502   \u2502   is_exists_result = connection.execute(                                         \u2502\r\n\u2502   119 \u2502   \u2502   \u2502   \u2502   \"MATCH (n:%s) WHERE n.ID = $entity RETURN n.ID\" % self.node_table_name,    \u2502\r\n\u2502   120 \u2502   \u2502   \u2502   \u2502   [(\"entity\", entity)],                                                      \u2502\r\n\u2502   121 \u2502   \u2502   \u2502   )                                                                              \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e         \u2502\r\n\u2502 \u2502 connection = <kuzu.connection.Connection object at 0x7f1ae8e0ebf0>                   \u2502         \u2502\r\n\u2502 \u2502     entity = 'I'                                                                     \u2502         \u2502\r\n\u2502 \u2502       self = <llama_index.graph_stores.kuzu.KuzuGraphStore object at 0x7f1aeb961960> \u2502         \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f         \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/kuzu/connection.py:77 in execute                         \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    74 \u2502   \u2502   \"\"\"                                                                                \u2502\r\n\u2502    75 \u2502   \u2502   self.init_connection()                                                             \u2502\r\n\u2502    76 \u2502   \u2502   if type(parameters) != dict:                                                       \u2502\r\n\u2502 \u2771  77 \u2502   \u2502   \u2502   raise RuntimeError(\"Parameters must be a dict\")                                \u2502\r\n\u2502    78 \u2502   \u2502   prepared_statement = self.prepare(                                                 \u2502\r\n\u2502    79 \u2502   \u2502   \u2502   query) if type(query) == str else query                                        \u2502\r\n\u2502    80 \u2502   \u2502   _query_result = self._connection.execute(                                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                           \u2502\r\n\u2502 \u2502 parameters = [('entity', 'I')]                                     \u2502                           \u2502\r\n\u2502 \u2502      query = 'MATCH (n:entity) WHERE n.ID = $entity RETURN n.ID'   \u2502                           \u2502\r\n\u2502 \u2502       self = <kuzu.connection.Connection object at 0x7f1ae8e0ebf0> \u2502                           \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                           \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nRuntimeError: Parameters must be a dict\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9244/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9244/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9243",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9243/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9243/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9243/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9243",
        "id": 2020016582,
        "node_id": "I_kwDOIWuq5854ZwHG",
        "number": 9243,
        "title": "[Question]: LLM response cache for decreasing payment",
        "user": {
            "login": "sjinwoo",
            "id": 45092868,
            "node_id": "MDQ6VXNlcjQ1MDkyODY4",
            "avatar_url": "https://avatars.githubusercontent.com/u/45092868?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sjinwoo",
            "html_url": "https://github.com/sjinwoo",
            "followers_url": "https://api.github.com/users/sjinwoo/followers",
            "following_url": "https://api.github.com/users/sjinwoo/following{/other_user}",
            "gists_url": "https://api.github.com/users/sjinwoo/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sjinwoo/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sjinwoo/subscriptions",
            "organizations_url": "https://api.github.com/users/sjinwoo/orgs",
            "repos_url": "https://api.github.com/users/sjinwoo/repos",
            "events_url": "https://api.github.com/users/sjinwoo/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sjinwoo/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-12-01T04:06:56Z",
        "updated_at": "2023-12-05T15:58:08Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [x] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi. I'm building an LLM-based RAG service, and I want to return a 'cached answer' without using a token when a question with the same content is entered.\r\nThe 'cache' argument to LLMPredictor seems to disappear as the version is upgraded.\r\nIs there an alternative?\r\nThanks. :)",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9243/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 1,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9243/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9242",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9242/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9242/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9242/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9242",
        "id": 2019814680,
        "node_id": "I_kwDOIWuq5854Y-0Y",
        "number": 9242,
        "title": "[Bug]: _id field of TextNode - backwards compatibility",
        "user": {
            "login": "rcala1",
            "id": 48602053,
            "node_id": "MDQ6VXNlcjQ4NjAyMDUz",
            "avatar_url": "https://avatars.githubusercontent.com/u/48602053?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rcala1",
            "html_url": "https://github.com/rcala1",
            "followers_url": "https://api.github.com/users/rcala1/followers",
            "following_url": "https://api.github.com/users/rcala1/following{/other_user}",
            "gists_url": "https://api.github.com/users/rcala1/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rcala1/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rcala1/subscriptions",
            "organizations_url": "https://api.github.com/users/rcala1/orgs",
            "repos_url": "https://api.github.com/users/rcala1/repos",
            "events_url": "https://api.github.com/users/rcala1/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rcala1/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-12-01T00:52:33Z",
        "updated_at": "2023-12-01T00:59:45Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nWhen loading already stored vector store from llama index previous version on newest version, TextNode id_ field is not populated with doc_id field but rather it is newly generated. \r\n\r\nThis is done in legacy_json_to_doc method, where doc_id was assigned to _id variable but when constructor is called with \r\n\r\n`elif doc_type == TextNode.get_type(): `\r\n`     doc = TextNode(text=text, metadata=metadata, id=id_, relationships=relationships)`\r\n\r\nid stored in _id variable is not assigned to _id property of TextNode but rather below code is called which is in super class:\r\n\r\n`id_: str = Field(default_factory=lambda: str(uuid.uuid4()), description=\"Unique ID of the node.\")`\r\n    \r\n\n\n### Version\n\n0.9.8\n\n### Steps to Reproduce\n\nStore some vector store on previous version (for example 0.6.8)\r\nLoad vector store on newest version (0.9.8)\r\nloaded id_ property of nodes will not match previously stored doc_id on earlier version of llama index\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9242/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9242/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9241",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9241/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9241/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9241/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9241",
        "id": 2019814037,
        "node_id": "I_kwDOIWuq5854Y-qV",
        "number": 9241,
        "title": "[Question]: It is possible to build distributed GraphRAG using Llama-index?",
        "user": {
            "login": "JinSeoung-Oh",
            "id": 78573459,
            "node_id": "MDQ6VXNlcjc4NTczNDU5",
            "avatar_url": "https://avatars.githubusercontent.com/u/78573459?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/JinSeoung-Oh",
            "html_url": "https://github.com/JinSeoung-Oh",
            "followers_url": "https://api.github.com/users/JinSeoung-Oh/followers",
            "following_url": "https://api.github.com/users/JinSeoung-Oh/following{/other_user}",
            "gists_url": "https://api.github.com/users/JinSeoung-Oh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/JinSeoung-Oh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/JinSeoung-Oh/subscriptions",
            "organizations_url": "https://api.github.com/users/JinSeoung-Oh/orgs",
            "repos_url": "https://api.github.com/users/JinSeoung-Oh/repos",
            "events_url": "https://api.github.com/users/JinSeoung-Oh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/JinSeoung-Oh/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 11,
        "created_at": "2023-12-01T00:52:09Z",
        "updated_at": "2023-12-01T04:33:23Z",
        "closed_at": "2023-12-01T04:33:23Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nHi. I'm researching about possibility of distributed RAG\r\nBut I cannot find out it..\r\n\r\nThese days, I'm converting a 300-page document into a graph DB, \r\nand I'm suddenly realizing the need for distributed processing.\r\n\r\nThis does not mean something like a tokenizer, \r\nbut rather asks whether distributed processing support is possible when creating and retrieving KG with Llama-index\r\n\r\nThanks!",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9241/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9241/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9240",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9240/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9240/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9240/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9240",
        "id": 2019661501,
        "node_id": "PR_kwDOIWuq585g1YXu",
        "number": 9240,
        "title": "Modify download_utils to allow for PDF downloads in extra_files",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710935,
                "node_id": "LA_kwDOIWuq588AAAABc3-fFw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:M",
                "name": "size:M",
                "color": "ebb800",
                "default": false,
                "description": "This PR changes 30-99 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-30T23:13:40Z",
        "updated_at": "2023-12-01T17:34:11Z",
        "closed_at": "2023-12-01T17:34:10Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9240",
            "html_url": "https://github.com/run-llama/llama_index/pull/9240",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9240.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9240.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nOkay, this is getting a bit more hectic. Since `extra_files` is the catch all for all llama-hub artifact's associated files, we need to update this to allow for PDF file downloads as source documents for `LlamaDatasets` may be of this type.\r\n\r\n- added ability to download PDF if they are included as `extra_files`\r\n- added `show_progress` kwarg to allow user to see progress of downloading `extra_files` in `llama_dataset` download.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] Works in my e2e notebooks for downloading llama datasets that have PDF source files\r\n- [x] I stared at the code and made sure it makes sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9240/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9240/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9239",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9239/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9239/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9239/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9239",
        "id": 2019552920,
        "node_id": "PR_kwDOIWuq585g1AYA",
        "number": 9239,
        "title": "[version] bump to v0.9.10",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-30T21:40:57Z",
        "updated_at": "2023-11-30T21:47:30Z",
        "closed_at": "2023-11-30T21:47:29Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9239",
            "html_url": "https://github.com/run-llama/llama_index/pull/9239",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9239.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9239.patch",
            "merged_at": "2023-11-30T21:47:29Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9239/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9239/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9238",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9238/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9238/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9238/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9238",
        "id": 2019483916,
        "node_id": "PR_kwDOIWuq585g0xQX",
        "number": 9238,
        "title": "Add GPT4-V and Microsoft Table transformer experiments with PDF which has tables.",
        "user": {
            "login": "ravi03071991",
            "id": 12198101,
            "node_id": "MDQ6VXNlcjEyMTk4MTAx",
            "avatar_url": "https://avatars.githubusercontent.com/u/12198101?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ravi03071991",
            "html_url": "https://github.com/ravi03071991",
            "followers_url": "https://api.github.com/users/ravi03071991/followers",
            "following_url": "https://api.github.com/users/ravi03071991/following{/other_user}",
            "gists_url": "https://api.github.com/users/ravi03071991/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ravi03071991/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ravi03071991/subscriptions",
            "organizations_url": "https://api.github.com/users/ravi03071991/orgs",
            "repos_url": "https://api.github.com/users/ravi03071991/repos",
            "events_url": "https://api.github.com/users/ravi03071991/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ravi03071991/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2023-11-30T20:46:27Z",
        "updated_at": "2023-12-01T16:48:11Z",
        "closed_at": "2023-12-01T15:22:10Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9238",
            "html_url": "https://github.com/run-llama/llama_index/pull/9238",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9238.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9238.patch",
            "merged_at": "2023-12-01T15:22:10Z"
        },
        "body": "# Description\r\n\r\nThis PR shows experiments of GPT4-V and Microsoft Table transformer with PDF which has tables.\r\n\r\nMerge this example https://github.com/run-llama/llama_index/pull/9235 with this pr\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [x] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [x] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [x] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [x] I have performed a self-review of my own code\r\n- [x] I have commented my code, particularly in hard-to-understand areas\r\n- [x] I have made corresponding changes to the documentation\r\n- [x] I have added Google Colab support for the newly added notebooks.\r\n- [x] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [x] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9238/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9238/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9237",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9237/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9237/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9237/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9237",
        "id": 2019386665,
        "node_id": "PR_kwDOIWuq585g0boY",
        "number": 9237,
        "title": "Fix headers of Vector Store doc pages.",
        "user": {
            "login": "aaronjimv",
            "id": 67152883,
            "node_id": "MDQ6VXNlcjY3MTUyODgz",
            "avatar_url": "https://avatars.githubusercontent.com/u/67152883?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/aaronjimv",
            "html_url": "https://github.com/aaronjimv",
            "followers_url": "https://api.github.com/users/aaronjimv/followers",
            "following_url": "https://api.github.com/users/aaronjimv/following{/other_user}",
            "gists_url": "https://api.github.com/users/aaronjimv/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/aaronjimv/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/aaronjimv/subscriptions",
            "organizations_url": "https://api.github.com/users/aaronjimv/orgs",
            "repos_url": "https://api.github.com/users/aaronjimv/repos",
            "events_url": "https://api.github.com/users/aaronjimv/events{/privacy}",
            "received_events_url": "https://api.github.com/users/aaronjimv/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-30T19:45:57Z",
        "updated_at": "2023-11-30T22:21:59Z",
        "closed_at": "2023-11-30T20:22:09Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9237",
            "html_url": "https://github.com/run-llama/llama_index/pull/9237",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9237.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9237.patch",
            "merged_at": "2023-11-30T20:22:09Z"
        },
        "body": "# Description\r\n\r\nThis PR fix the header issue present in #9167\r\n\r\nI think the problem was that the subtitle header had `#` instead of `##` or `###`.\r\n\r\nFixes #9167 \r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] My changes generate no new warnings\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9237/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9237/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9236",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9236/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9236/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9236/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9236",
        "id": 2019372031,
        "node_id": "PR_kwDOIWuq585g0YaJ",
        "number": 9236,
        "title": "Add aget_general_text_embedding for vayage ai",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-30T19:35:28Z",
        "updated_at": "2023-11-30T20:14:20Z",
        "closed_at": "2023-11-30T20:14:19Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9236",
            "html_url": "https://github.com/run-llama/llama_index/pull/9236",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9236.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9236.patch",
            "merged_at": "2023-11-30T20:14:19Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9236/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9236/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9235",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9235/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9235/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9235/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9235",
        "id": 2019025301,
        "node_id": "PR_kwDOIWuq585gzLQY",
        "number": 9235,
        "title": "Init GPT4V PDF image reader",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-30T16:23:00Z",
        "updated_at": "2023-11-30T23:15:00Z",
        "closed_at": "2023-11-30T23:15:00Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9235",
            "html_url": "https://github.com/run-llama/llama_index/pull/9235",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9235.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9235.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9235/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9235/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9234",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9234/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9234/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9234/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9234",
        "id": 2018947200,
        "node_id": "PR_kwDOIWuq585gy5-a",
        "number": 9234,
        "title": "Alphabetized `modules.md` and added LocalAI",
        "user": {
            "login": "jamesbraza",
            "id": 8990777,
            "node_id": "MDQ6VXNlcjg5OTA3Nzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8990777?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jamesbraza",
            "html_url": "https://github.com/jamesbraza",
            "followers_url": "https://api.github.com/users/jamesbraza/followers",
            "following_url": "https://api.github.com/users/jamesbraza/following{/other_user}",
            "gists_url": "https://api.github.com/users/jamesbraza/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jamesbraza/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jamesbraza/subscriptions",
            "organizations_url": "https://api.github.com/users/jamesbraza/orgs",
            "repos_url": "https://api.github.com/users/jamesbraza/repos",
            "events_url": "https://api.github.com/users/jamesbraza/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jamesbraza/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318866,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/documentation",
                "name": "documentation",
                "color": "0075ca",
                "default": true,
                "description": "Improvements or additions to documentation"
            },
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": {
            "login": "jamesbraza",
            "id": 8990777,
            "node_id": "MDQ6VXNlcjg5OTA3Nzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8990777?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jamesbraza",
            "html_url": "https://github.com/jamesbraza",
            "followers_url": "https://api.github.com/users/jamesbraza/followers",
            "following_url": "https://api.github.com/users/jamesbraza/following{/other_user}",
            "gists_url": "https://api.github.com/users/jamesbraza/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jamesbraza/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jamesbraza/subscriptions",
            "organizations_url": "https://api.github.com/users/jamesbraza/orgs",
            "repos_url": "https://api.github.com/users/jamesbraza/repos",
            "events_url": "https://api.github.com/users/jamesbraza/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jamesbraza/received_events",
            "type": "User",
            "site_admin": false
        },
        "assignees": [
            {
                "login": "jamesbraza",
                "id": 8990777,
                "node_id": "MDQ6VXNlcjg5OTA3Nzc=",
                "avatar_url": "https://avatars.githubusercontent.com/u/8990777?v=4",
                "gravatar_id": "",
                "url": "https://api.github.com/users/jamesbraza",
                "html_url": "https://github.com/jamesbraza",
                "followers_url": "https://api.github.com/users/jamesbraza/followers",
                "following_url": "https://api.github.com/users/jamesbraza/following{/other_user}",
                "gists_url": "https://api.github.com/users/jamesbraza/gists{/gist_id}",
                "starred_url": "https://api.github.com/users/jamesbraza/starred{/owner}{/repo}",
                "subscriptions_url": "https://api.github.com/users/jamesbraza/subscriptions",
                "organizations_url": "https://api.github.com/users/jamesbraza/orgs",
                "repos_url": "https://api.github.com/users/jamesbraza/repos",
                "events_url": "https://api.github.com/users/jamesbraza/events{/privacy}",
                "received_events_url": "https://api.github.com/users/jamesbraza/received_events",
                "type": "User",
                "site_admin": false
            }
        ],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-30T15:45:03Z",
        "updated_at": "2023-11-30T17:17:58Z",
        "closed_at": "2023-11-30T17:17:57Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9234",
            "html_url": "https://github.com/run-llama/llama_index/pull/9234",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9234.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9234.patch",
            "merged_at": "2023-11-30T17:17:57Z"
        },
        "body": "# Description\r\n\r\n- Alphabetized `modules.md`\r\n- Added `localai.ipynb`\r\n    - Supercedes https://github.com/run-llama/llama_index/pull/9233\r\n    - Relates to https://github.com/run-llama/llama_index/issues/9223\r\n\r\n## Type of Change\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9234/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9234/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9233",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9233/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9233/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9233/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9233",
        "id": 2018915061,
        "node_id": "PR_kwDOIWuq585gyy3o",
        "number": 9233,
        "title": "LocalAI demo in sitemap",
        "user": {
            "login": "jamesbraza",
            "id": 8990777,
            "node_id": "MDQ6VXNlcjg5OTA3Nzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8990777?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jamesbraza",
            "html_url": "https://github.com/jamesbraza",
            "followers_url": "https://api.github.com/users/jamesbraza/followers",
            "following_url": "https://api.github.com/users/jamesbraza/following{/other_user}",
            "gists_url": "https://api.github.com/users/jamesbraza/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jamesbraza/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jamesbraza/subscriptions",
            "organizations_url": "https://api.github.com/users/jamesbraza/orgs",
            "repos_url": "https://api.github.com/users/jamesbraza/repos",
            "events_url": "https://api.github.com/users/jamesbraza/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jamesbraza/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318866,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/documentation",
                "name": "documentation",
                "color": "0075ca",
                "default": true,
                "description": "Improvements or additions to documentation"
            },
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": {
            "login": "jamesbraza",
            "id": 8990777,
            "node_id": "MDQ6VXNlcjg5OTA3Nzc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8990777?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jamesbraza",
            "html_url": "https://github.com/jamesbraza",
            "followers_url": "https://api.github.com/users/jamesbraza/followers",
            "following_url": "https://api.github.com/users/jamesbraza/following{/other_user}",
            "gists_url": "https://api.github.com/users/jamesbraza/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jamesbraza/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jamesbraza/subscriptions",
            "organizations_url": "https://api.github.com/users/jamesbraza/orgs",
            "repos_url": "https://api.github.com/users/jamesbraza/repos",
            "events_url": "https://api.github.com/users/jamesbraza/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jamesbraza/received_events",
            "type": "User",
            "site_admin": false
        },
        "assignees": [
            {
                "login": "jamesbraza",
                "id": 8990777,
                "node_id": "MDQ6VXNlcjg5OTA3Nzc=",
                "avatar_url": "https://avatars.githubusercontent.com/u/8990777?v=4",
                "gravatar_id": "",
                "url": "https://api.github.com/users/jamesbraza",
                "html_url": "https://github.com/jamesbraza",
                "followers_url": "https://api.github.com/users/jamesbraza/followers",
                "following_url": "https://api.github.com/users/jamesbraza/following{/other_user}",
                "gists_url": "https://api.github.com/users/jamesbraza/gists{/gist_id}",
                "starred_url": "https://api.github.com/users/jamesbraza/starred{/owner}{/repo}",
                "subscriptions_url": "https://api.github.com/users/jamesbraza/subscriptions",
                "organizations_url": "https://api.github.com/users/jamesbraza/orgs",
                "repos_url": "https://api.github.com/users/jamesbraza/repos",
                "events_url": "https://api.github.com/users/jamesbraza/events{/privacy}",
                "received_events_url": "https://api.github.com/users/jamesbraza/received_events",
                "type": "User",
                "site_admin": false
            }
        ],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-30T15:28:46Z",
        "updated_at": "2023-11-30T15:45:24Z",
        "closed_at": "2023-11-30T15:45:21Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9233",
            "html_url": "https://github.com/run-llama/llama_index/pull/9233",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9233.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9233.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nRelates to https://github.com/run-llama/llama_index/issues/9223, this PR makes `localai` demo show up in search.\r\n\r\n## Type of Change\r\n\r\n- [x] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [x] I stared at the code and made sure it makes sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9233/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9233/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9232",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9232/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9232/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9232/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9232",
        "id": 2018724962,
        "node_id": "I_kwDOIWuq5854U0xi",
        "number": 9232,
        "title": "[Bug]: ResourceWarning unclosed file <_io.TextIOWrapper>",
        "user": {
            "login": "BIGdeadLock",
            "id": 64005996,
            "node_id": "MDQ6VXNlcjY0MDA1OTk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/64005996?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BIGdeadLock",
            "html_url": "https://github.com/BIGdeadLock",
            "followers_url": "https://api.github.com/users/BIGdeadLock/followers",
            "following_url": "https://api.github.com/users/BIGdeadLock/following{/other_user}",
            "gists_url": "https://api.github.com/users/BIGdeadLock/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BIGdeadLock/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BIGdeadLock/subscriptions",
            "organizations_url": "https://api.github.com/users/BIGdeadLock/orgs",
            "repos_url": "https://api.github.com/users/BIGdeadLock/repos",
            "events_url": "https://api.github.com/users/BIGdeadLock/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BIGdeadLock/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-11-30T13:56:58Z",
        "updated_at": "2023-12-05T15:55:32Z",
        "closed_at": "2023-12-05T15:55:32Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\r\n\r\nWhen I use the JSONReader I get the error:\r\n\r\nResourceWarning: unclosed file <_io.TextIOWrapper name='.venv/lib/python3.11/site-packages/llama_index/download/llamahub_modules/requirements.txt' mode='r' encoding='utf-8'>\r\n  for item in lines:\r\nResourceWarning: Enable tracemalloc to get the object allocation traceback\r\n\r\n\r\n![Screenshot 2023-11-30 at 15 56 41](https://github.com/run-llama/llama_index/assets/64005996/b1a770a4-3618-4398-a464-a02117e6cb32)\r\n\r\n\r\n\r\n### Version\r\n\r\n0.9.4\r\n\r\n### Steps to Reproduce\r\n\r\nOn Mac Pycharm, run:\r\n\r\n`JSONReader = download_loader(\"JSONReader\") # use json file reader from llama hub`\r\n   ` loader = JSONReader()`\r\n  `documents = loader.load_data(FILE_PATH)`\r\n\r\n\r\n\r\n### Relevant Logs/Tracbacks\r\n\r\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9232/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9232/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9231",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9231/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9231/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9231/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9231",
        "id": 2018523682,
        "node_id": "I_kwDOIWuq5854UDoi",
        "number": 9231,
        "title": "[Question]: Returning the associated metadata for the ",
        "user": {
            "login": "sqpollen",
            "id": 99855230,
            "node_id": "U_kgDOBfOrfg",
            "avatar_url": "https://avatars.githubusercontent.com/u/99855230?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sqpollen",
            "html_url": "https://github.com/sqpollen",
            "followers_url": "https://api.github.com/users/sqpollen/followers",
            "following_url": "https://api.github.com/users/sqpollen/following{/other_user}",
            "gists_url": "https://api.github.com/users/sqpollen/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sqpollen/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sqpollen/subscriptions",
            "organizations_url": "https://api.github.com/users/sqpollen/orgs",
            "repos_url": "https://api.github.com/users/sqpollen/repos",
            "events_url": "https://api.github.com/users/sqpollen/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sqpollen/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-30T12:12:33Z",
        "updated_at": "2023-11-30T17:21:08Z",
        "closed_at": "2023-11-30T17:21:07Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nUsing this simple script as an example, how can I get the metadata of the documents returned in the query? I would like to return both the query result, and the metadata of the documents.\r\n\r\n```\r\nimport os.path\r\nfrom llama_index import (\r\n    VectorStoreIndex,\r\n    SimpleDirectoryReader,\r\n    StorageContext,\r\n    load_index_from_storage,\r\n)\r\n\r\n# check if storage already exists\r\nif not os.path.exists(\"./storage\"):\r\n    # load the documents and create the index\r\n    documents = SimpleDirectoryReader(\"data\").load_data()\r\n    index = VectorStoreIndex.from_documents(documents)\r\n    # store it for later\r\n    index.storage_context.persist()\r\nelse:\r\n    # load the existing index\r\n    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\r\n    index = load_index_from_storage(storage_context)\r\n\r\n# either way we can now query the index\r\nquery_engine = index.as_query_engine()\r\nresponse = query_engine.query(\"What did the author do growing up?\")\r\nprint(response)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9231/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9231/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9230",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9230/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9230/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9230/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9230",
        "id": 2018411433,
        "node_id": "I_kwDOIWuq5854ToOp",
        "number": 9230,
        "title": "[Bug]: HuggingFaceLLM not working for HuggingFaceH4/zephyr-7b-beta   Key Error mistral (or load_in_8bit=True) error depending on transformer version",
        "user": {
            "login": "defiways",
            "id": 149413460,
            "node_id": "U_kgDOCOfeVA",
            "avatar_url": "https://avatars.githubusercontent.com/u/149413460?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/defiways",
            "html_url": "https://github.com/defiways",
            "followers_url": "https://api.github.com/users/defiways/followers",
            "following_url": "https://api.github.com/users/defiways/following{/other_user}",
            "gists_url": "https://api.github.com/users/defiways/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/defiways/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/defiways/subscriptions",
            "organizations_url": "https://api.github.com/users/defiways/orgs",
            "repos_url": "https://api.github.com/users/defiways/repos",
            "events_url": "https://api.github.com/users/defiways/events{/privacy}",
            "received_events_url": "https://api.github.com/users/defiways/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-11-30T11:10:45Z",
        "updated_at": "2023-12-03T05:29:47Z",
        "closed_at": "2023-11-30T17:23:23Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\nImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` \r\n\r\nBut this can only be corrected by downgrading to transformers=4.30  and when that is done it introduces another error\r\n\r\nKeyError                                  Traceback (most recent call last)\r\n\r\n[<ipython-input-7-bb690c4074f5>](https://localhost:8080/#) in <cell line: 8>()\r\n      6 )\r\n      7 \r\n----> 8 llm = HuggingFaceLLM(\r\n      9     model_name=\"HuggingFaceH4/zephyr-7b-beta\",\r\n     10     tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\r\n\r\n3 frames\r\n\r\n[/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py](https://localhost:8080/#) in __getitem__(self, key)\r\n    669             return self._extra_content[key]\r\n    670         if key not in self._mapping:\r\n--> 671             raise KeyError(key)\r\n    672         value = self._mapping[key]\r\n    673         module_name = model_type_to_module_name(key)\r\n\r\nKeyError: 'mistral'\r\n\r\nThis model must have the latest transformers to run\n\n### Version\n\nlatest\n\n### Steps to Reproduce\n\n!pip install llama-index transformers==4.30 accelerate bitsandbytes\r\n\r\n#run this without downgrade of transformers and you will see error above. \r\n\r\nquantization_config = BitsAndBytesConfig(\r\n  load_in_4bit=True,\r\n  bnb_4bit_compute_dtype=torch.float16,\r\n  bnb_4bit_quant_type=\"nf4\",\r\n  bnb_4bit_use_double_quant=True\r\n)\r\n\r\nllm = HuggingFaceLLM(\r\n    model_name=\"HuggingFaceH4/zephyr-7b-beta\",\r\n    tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\r\n    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\r\n    context_window=3900,\r\n    max_new_tokens=256,\r\n    model_kwargs={\"quantization_config\": quantization_config},\r\n    # tokenizer_kwargs={},\r\n    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\r\n    messages_to_prompt=messages_to_prompt,\r\n    device_map=\"auto\",\r\n)\n\n### Relevant Logs/Tracbacks\n\n```shell\nKeyError                                  Traceback (most recent call last)\r\n\r\n<ipython-input-7-bb690c4074f5> in <cell line: 8>()\r\n      6 )\r\n      7 \r\n----> 8 llm = HuggingFaceLLM(\r\n      9     model_name=\"HuggingFaceH4/zephyr-7b-beta\",\r\n     10     tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py in __getitem__(self, key)\r\n    669             return self._extra_content[key]\r\n    670         if key not in self._mapping:\r\n--> 671             raise KeyError(key)\r\n    672         value = self._mapping[key]\r\n    673         module_name = model_type_to_module_name(key)\r\n\r\nKeyError: 'mistral'\n```\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9230/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9230/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9229",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9229/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9229/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9229/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9229",
        "id": 2017962044,
        "node_id": "PR_kwDOIWuq585gvhvr",
        "number": 9229,
        "title": "add aget_general_text_embedding to VoyageEmbedding",
        "user": {
            "login": "thomas0809",
            "id": 11373553,
            "node_id": "MDQ6VXNlcjExMzczNTUz",
            "avatar_url": "https://avatars.githubusercontent.com/u/11373553?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/thomas0809",
            "html_url": "https://github.com/thomas0809",
            "followers_url": "https://api.github.com/users/thomas0809/followers",
            "following_url": "https://api.github.com/users/thomas0809/following{/other_user}",
            "gists_url": "https://api.github.com/users/thomas0809/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/thomas0809/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/thomas0809/subscriptions",
            "organizations_url": "https://api.github.com/users/thomas0809/orgs",
            "repos_url": "https://api.github.com/users/thomas0809/repos",
            "events_url": "https://api.github.com/users/thomas0809/events{/privacy}",
            "received_events_url": "https://api.github.com/users/thomas0809/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710905,
                "node_id": "LA_kwDOIWuq588AAAABc3-e-Q",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:XS",
                "name": "size:XS",
                "color": "00ff00",
                "default": false,
                "description": "This PR changes 0-9 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-30T06:21:57Z",
        "updated_at": "2023-11-30T20:14:37Z",
        "closed_at": "2023-11-30T20:14:36Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9229",
            "html_url": "https://github.com/run-llama/llama_index/pull/9229",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9229.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9229.patch",
            "merged_at": null
        },
        "body": "# Description\r\n\r\nAdd aget_general_text_embedding to VoyageEmbedding. Async version of getting embedding with input_type\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n\r\n# How Has This Been Tested?\r\n\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9229/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9229/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9228",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9228/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9228/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9228/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9228",
        "id": 2017901817,
        "node_id": "PR_kwDOIWuq585gvUqp",
        "number": 9228,
        "title": "Add `LlamaDataset` notebooks and info page to docs",
        "user": {
            "login": "nerdai",
            "id": 92402603,
            "node_id": "U_kgDOBYHzqw",
            "avatar_url": "https://avatars.githubusercontent.com/u/92402603?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nerdai",
            "html_url": "https://github.com/nerdai",
            "followers_url": "https://api.github.com/users/nerdai/followers",
            "following_url": "https://api.github.com/users/nerdai/following{/other_user}",
            "gists_url": "https://api.github.com/users/nerdai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nerdai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nerdai/subscriptions",
            "organizations_url": "https://api.github.com/users/nerdai/orgs",
            "repos_url": "https://api.github.com/users/nerdai/repos",
            "events_url": "https://api.github.com/users/nerdai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nerdai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6232710946,
                "node_id": "LA_kwDOIWuq588AAAABc3-fIg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:L",
                "name": "size:L",
                "color": "eb9500",
                "default": false,
                "description": "This PR changes 100-499 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-11-30T05:26:31Z",
        "updated_at": "2023-11-30T23:08:52Z",
        "closed_at": "2023-11-30T23:08:51Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9228",
            "html_url": "https://github.com/run-llama/llama_index/pull/9228",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9228.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9228.patch",
            "merged_at": "2023-11-30T23:08:51Z"
        },
        "body": "# Description\r\n\r\nAdds a new subsection with `Module Guides` >> `Evaluation` called `Evaluating With LlamaDataset's`.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [x] Documentation\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [x] I stared at the code snippets and made sure they made sense\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9228/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9228/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9227",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9227/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9227/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9227/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9227",
        "id": 2017873995,
        "node_id": "I_kwDOIWuq5854RlBL",
        "number": 9227,
        "title": "[Bug]: MultiStepQueryEngine is requiring OpenAI model",
        "user": {
            "login": "nyabutid",
            "id": 8539281,
            "node_id": "MDQ6VXNlcjg1MzkyODE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8539281?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nyabutid",
            "html_url": "https://github.com/nyabutid",
            "followers_url": "https://api.github.com/users/nyabutid/followers",
            "following_url": "https://api.github.com/users/nyabutid/following{/other_user}",
            "gists_url": "https://api.github.com/users/nyabutid/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nyabutid/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nyabutid/subscriptions",
            "organizations_url": "https://api.github.com/users/nyabutid/orgs",
            "repos_url": "https://api.github.com/users/nyabutid/repos",
            "events_url": "https://api.github.com/users/nyabutid/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nyabutid/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5584919374,
                "node_id": "LA_kwDOIWuq588AAAABTOMbTg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/triage",
                "name": "triage",
                "color": "FBCA04",
                "default": false,
                "description": "Issue needs to be triaged/prioritized"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-11-30T04:55:00Z",
        "updated_at": "2023-11-30T05:06:59Z",
        "closed_at": "2023-11-30T04:57:23Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Bug Description\n\n`MultiStepQueryEngine `is requiring OpenAI model. `HuggingFaceLLM` is not supported.\r\n\r\n```\r\n  File \"/opt/conda/lib/python3.10/site-packages/llama_index/query_engine/multistep_query_engine.py\", line 53, in __init__\r\n    self._response_synthesizer = response_synthesizer or get_response_synthesizer(\r\n  File \"/opt/conda/lib/python3.10/site-packages/llama_index/response_synthesizers/factory.py\", line 50, in get_response_synthesizer\r\n    service_context = service_context or ServiceContext.from_defaults(\r\n  File \"/opt/conda/lib/python3.10/site-packages/llama_index/service_context.py\", line 167, in from_defaults\r\n    llm_predictor = llm_predictor or LLMPredictor(\r\n  File \"/opt/conda/lib/python3.10/site-packages/llama_index/llm_predictor/base.py\", line 104, in __init__\r\n    self._llm = resolve_llm(llm)\r\n  File \"/opt/conda/lib/python3.10/site-packages/llama_index/llms/utils.py\", line 31, in resolve_llm\r\n    raise ValueError(\r\nValueError: \r\n******\r\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\r\nOriginal error:\r\nNo API key found for OpenAI.\r\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\r\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\r\n\r\nTo disable the LLM entirely, set llm=None.\r\n******\r\n```\n\n### Version\n\n0.9.9\n\n### Steps to Reproduce\n\n```\r\nllm = HuggingFaceLLM(\r\n            model=model,\r\n            tokenizer=tokenizer,\r\n            context_window=2048,\r\n            max_new_tokens=256,\r\n            generate_kwargs={\"temperature\": 0, \"do_sample\": False},\r\n            device_map=\"auto\",\r\n            tokenizer_kwargs={\"max_length\": 2048},\r\n        )\r\n\r\n        node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\r\n\r\n        prompt_helper = PromptHelper(\r\n            context_window=2048, chunk_overlap_ratio=0.1, chunk_size_limit=None\r\n        )\r\n\r\n        service_context = ServiceContext.from_defaults(\r\n            llm=llm,\r\n            embed_model=LangchainEmbedding(embeddings),\r\n            node_parser=node_parser,\r\n            prompt_helper=prompt_helper,\r\n        )\r\n\r\n        step_decompose_transform = StepDecomposeQueryTransform(\r\n            LLMPredictor(llm=llm), verbose=True\r\n        )\r\n        q_engine = SubQuestionQueryEngine.from_defaults(\r\n            service_context=service_context, query_engine_tools=query_engine_tools\r\n        )\r\n        engine = MultiStepQueryEngine(\r\n            query_engine=q_engine,\r\n            query_transform=step_decompose_transform,\r\n            index_summary=\"summary\",\r\n        )\r\n```\n\n### Relevant Logs/Tracbacks\n\n_No response_",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9227/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9227/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9226",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9226/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9226/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9226/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/9226",
        "id": 2017783860,
        "node_id": "PR_kwDOIWuq585gu7Tc",
        "number": 9226,
        "title": "Upd Changelog",
        "user": {
            "login": "hatianzhang",
            "id": 2142132,
            "node_id": "MDQ6VXNlcjIxNDIxMzI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2142132?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/hatianzhang",
            "html_url": "https://github.com/hatianzhang",
            "followers_url": "https://api.github.com/users/hatianzhang/followers",
            "following_url": "https://api.github.com/users/hatianzhang/following{/other_user}",
            "gists_url": "https://api.github.com/users/hatianzhang/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/hatianzhang/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/hatianzhang/subscriptions",
            "organizations_url": "https://api.github.com/users/hatianzhang/orgs",
            "repos_url": "https://api.github.com/users/hatianzhang/repos",
            "events_url": "https://api.github.com/users/hatianzhang/events{/privacy}",
            "received_events_url": "https://api.github.com/users/hatianzhang/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 6225900672,
                "node_id": "LA_kwDOIWuq588AAAABcxe0gA",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/lgtm",
                "name": "lgtm",
                "color": "238636",
                "default": false,
                "description": "This PR has been approved by a maintainer"
            },
            {
                "id": 6232710919,
                "node_id": "LA_kwDOIWuq588AAAABc3-fBw",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/size:S",
                "name": "size:S",
                "color": "77b800",
                "default": false,
                "description": "This PR changes 10-29 lines, ignoring generated files."
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-11-30T03:01:59Z",
        "updated_at": "2023-11-30T03:14:16Z",
        "closed_at": "2023-11-30T03:14:15Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/9226",
            "html_url": "https://github.com/run-llama/llama_index/pull/9226",
            "diff_url": "https://github.com/run-llama/llama_index/pull/9226.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/9226.patch",
            "merged_at": "2023-11-30T03:14:15Z"
        },
        "body": "# Description\r\n\r\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\r\n\r\nFixes # (issue)\r\n\r\n## Type of Change\r\n\r\nPlease delete options that are not relevant.\r\n\r\n- [ ] Bug fix (non-breaking change which fixes an issue)\r\n- [ ] New feature (non-breaking change which adds functionality)\r\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\r\n- [ ] This change requires a documentation update\r\n\r\n# How Has This Been Tested?\r\n\r\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\r\n\r\n- [ ] Added new unit/integration tests\r\n- [ ] Added new notebook (that tests end-to-end)\r\n- [ ] I stared at the code and made sure it makes sense\r\n\r\n# Suggested Checklist:\r\n\r\n- [ ] I have performed a self-review of my own code\r\n- [ ] I have commented my code, particularly in hard-to-understand areas\r\n- [ ] I have made corresponding changes to the documentation\r\n- [ ] I have added Google Colab support for the newly added notebooks.\r\n- [ ] My changes generate no new warnings\r\n- [ ] I have added tests that prove my fix is effective or that my feature works\r\n- [ ] New and existing unit tests pass locally with my changes\r\n- [ ] I ran `make format; make lint` to appease the lint gods\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9226/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9226/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/9225",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/9225/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/9225/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/9225/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/9225",
        "id": 2017600919,
        "node_id": "I_kwDOIWuq5854QiWX",
        "number": 9225,
        "title": "Successful but Extremely Slow Response Time from LLM ",
        "user": {
            "login": "grabani",
            "id": 25469154,
            "node_id": "MDQ6VXNlcjI1NDY5MTU0",
            "avatar_url": "https://avatars.githubusercontent.com/u/25469154?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/grabani",
            "html_url": "https://github.com/grabani",
            "followers_url": "https://api.github.com/users/grabani/followers",
            "following_url": "https://api.github.com/users/grabani/following{/other_user}",
            "gists_url": "https://api.github.com/users/grabani/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/grabani/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/grabani/subscriptions",
            "organizations_url": "https://api.github.com/users/grabani/orgs",
            "repos_url": "https://api.github.com/users/grabani/repos",
            "events_url": "https://api.github.com/users/grabani/events{/privacy}",
            "received_events_url": "https://api.github.com/users/grabani/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-11-29T23:21:54Z",
        "updated_at": "2023-11-30T17:20:09Z",
        "closed_at": "2023-11-30T17:20:08Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "### Question Validation\n\n- [X] I have searched both the documentation and discord for an answer.\n\n### Question\n\nI am able to successfully receive a response running the following code, however, the response takes 4m 44.9s. When I use the same embeddings database but direct to openAI it is instance. Any ideas why the local LLM is taking so long!!\r\n \r\n```\r\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\r\nfrom llama_index.vector_stores import ChromaVectorStore\r\nfrom llama_index.storage.storage_context import StorageContext\r\nfrom llama_index.embeddings import HuggingFaceEmbedding\r\nfrom llama_index.llms import OpenAI\r\nfrom IPython.display import Markdown, display\r\nimport chromadb\r\n\r\n# define embedding function\r\n\r\nembed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\r\n\r\n# Load LLM\r\n\r\nfrom llama_index import (\r\n    SimpleDirectoryReader,\r\n    VectorStoreIndex,\r\n    ServiceContext,\r\n)\r\nfrom llama_index.llms import LlamaCPP\r\nfrom llama_index.llms.llama_utils import (\r\n    messages_to_prompt,\r\n    completion_to_prompt,\r\n)\r\n\r\nllm = LlamaCPP(\r\n    # You can pass in the URL to a GGML model to download it automatically\r\n    #model_url=model_url,\r\n    # optionally, you can set the path to a pre-downloaded model instead of model_url\r\n    model_path=r\"C:\\Users\\Home\\.cache\\lm-studio\\models\\TheBloke\\Mistral-7B-Instruct-v0.1-GGUF\\mistral-7b-instruct-v0.1.Q6_K.gguf\",\r\n    temperature=0.1,\r\n    max_new_tokens=4048,\r\n    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\r\n    context_window=8000,\r\n    # kwargs to pass to __call__()\r\n    generate_kwargs={},\r\n    # kwargs to pass to __init__()\r\n    # set to at least 1 to use GPU\r\n    #model_kwargs={\"n_gpu_layers\": 1},\r\n    # transform inputs into Llama2 format\r\n    messages_to_prompt=messages_to_prompt,\r\n    completion_to_prompt=completion_to_prompt,\r\n    verbose=True,\r\n)\r\n\r\n# load embeddings from disk\r\n\r\ndb2 = chromadb.PersistentClient(path=\"./data/jhoward_L4/chroma_db\")\r\n\r\nchroma_collection = db2.get_or_create_collection(\"DB_collection\")\r\n\r\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\r\n\r\nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model,\r\n                                               chunk_size=100,\r\n                                               chunk_overlap=20)\r\n\r\nindex = VectorStoreIndex.from_vector_store(\r\n    vector_store,\r\n    service_context=service_context,\r\n)\r\n# Response synthesis\r\nquery_engine = index.as_query_engine()\r\n\r\nresponse = query_engine.query(\"What is lesson about?\")\r\n\r\ndisplay(Markdown(f\"{response}\"))\r\n```\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/9225/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/9225/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    }
]