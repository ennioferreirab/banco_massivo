[
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3364",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3364/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3364/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3364/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3364",
        "id": 1709786886,
        "node_id": "I_kwDOIWuq585l6UcG",
        "number": 3364,
        "title": "`AuthenticationError: No API key provided` when using HuggingFace model in ServiceContext",
        "user": {
            "login": "veerlosar",
            "id": 45338519,
            "node_id": "MDQ6VXNlcjQ1MzM4NTE5",
            "avatar_url": "https://avatars.githubusercontent.com/u/45338519?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/veerlosar",
            "html_url": "https://github.com/veerlosar",
            "followers_url": "https://api.github.com/users/veerlosar/followers",
            "following_url": "https://api.github.com/users/veerlosar/following{/other_user}",
            "gists_url": "https://api.github.com/users/veerlosar/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/veerlosar/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/veerlosar/subscriptions",
            "organizations_url": "https://api.github.com/users/veerlosar/orgs",
            "repos_url": "https://api.github.com/users/veerlosar/repos",
            "events_url": "https://api.github.com/users/veerlosar/events{/privacy}",
            "received_events_url": "https://api.github.com/users/veerlosar/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-15T10:23:02Z",
        "updated_at": "2023-05-15T18:14:34Z",
        "closed_at": "2023-05-15T18:14:34Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hello, \r\n\r\nI initialise my model like this: \r\n\r\n```\r\nstablelm_predictor = HuggingFaceLLMPredictor(\r\n    max_input_size=4096, \r\n    max_new_tokens=256,\r\n    temperature=0.7,\r\n    do_sample=False,\r\n    system_prompt=system_prompt,\r\n    query_wrapper_prompt=query_wrapper_prompt,\r\n    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\r\n    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\r\n    device_map=\"auto\",\r\n    stopping_ids=[50278, 50279, 50277, 1, 0],\r\n    tokenizer_kwargs={\"max_length\": 4096},\r\n    # uncomment this if using CUDA to reduce memory usage\r\n    # model_kwargs={\"torch_dtype\": torch.float16}\r\n)\r\n```\r\n\r\nServiceContext: \r\n\r\n```\r\nservice_context = ServiceContext.from_defaults(\r\n    prompt_helper=prompt_helper, \r\n    llm_predictor=stablelm_predictor\r\n)\r\n```\r\n\r\nFinally, index:\r\n\r\n```\r\nindex = GPTVectorStoreIndex.from_documents(\r\n    documents, \r\n    service_context=service_context\r\n)\r\n```\r\n\r\nWhen I initialise the index, I get the following error: \r\n\r\n```\r\nAuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\r\n```\r\n\r\nWhy am I still getting this error if I pass a model in `service_context`? \r\n\r\nI checked the related #852 but the link https://gpt-index.readthedocs.io/en/latest/how_to/custom_llms.html#example-using-a-custom-llm-model is returning 404 error and I do not have access to Discord unfortunately.\r\n\r\nThanks in advance!\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3364/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3364/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3357",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3357/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3357/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3357/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3357",
        "id": 1709522151,
        "node_id": "I_kwDOIWuq585l5Tzn",
        "number": 3357,
        "title": "How to save conversation context and manually create conversation history?",
        "user": {
            "login": "reinershir",
            "id": 6177423,
            "node_id": "MDQ6VXNlcjYxNzc0MjM=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6177423?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/reinershir",
            "html_url": "https://github.com/reinershir",
            "followers_url": "https://api.github.com/users/reinershir/followers",
            "following_url": "https://api.github.com/users/reinershir/following{/other_user}",
            "gists_url": "https://api.github.com/users/reinershir/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/reinershir/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/reinershir/subscriptions",
            "organizations_url": "https://api.github.com/users/reinershir/orgs",
            "repos_url": "https://api.github.com/users/reinershir/repos",
            "events_url": "https://api.github.com/users/reinershir/events{/privacy}",
            "received_events_url": "https://api.github.com/users/reinershir/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-15T07:51:52Z",
        "updated_at": "2023-06-08T02:32:33Z",
        "closed_at": "2023-06-08T02:32:33Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "As far as I know, `VectorStoreRetrieverMemory` in `langchain` can support this feature. However, how to integrate it into `llama_index`?\r\n\r\nmy code:\r\n```python\r\n\r\ndef construct_index(directory_path):\r\n    # set maximum input size\r\n    max_input_size = 4096\r\n    # set number of output tokens\r\n    num_outputs = 2000\r\n    # set maximum chunk overlap\r\n    max_chunk_overlap = 20\r\n    # set chunk size limit\r\n    chunk_size_limit = 2000 \r\n\r\n\r\n    #callback_manager = CallbackManager(handlers=[StreamingStdOutCallbackHandler()])\r\n    # define LLM\r\n\r\n\r\n    llm=ChatOpenAI(temperature=0.5, model_name=\"gpt-3.5-turbo\", verbose=True,streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\r\n    llm_predictor = ChatGPTLLMPredictor(llm=llm)\r\n    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\r\n \r\n    documents = SimpleDirectoryReader(directory_path).load_data()\r\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper) #llm_predictor=llm_predictor, \r\n    #index = GPTVectorStoreIndex.from_documents(documents,service_context=service_context)\r\n\r\n    #index.storage_context.persist(\"D://storage\")\r\n\r\n\r\n    # rebuild storage context\r\n    storage_context = StorageContext.from_defaults(persist_dir='D://storage')\r\n    # load index\r\n    index = load_index_from_storage(storage_context)\r\n    #response_mode=\"compact\",\r\n    query_engine = index.as_query_engine(similarity_top_k=1,service_context=service_context) #query_engine = index.as_query_engine(similarity_top_k=1,streaming=True,service_context=service_context)\r\n\r\n    retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\r\n    memory = GPTIndexChatMemory(\r\n        index=index, \r\n        memory_key=\"chat_history\", \r\n        query_kwargs={\"response_mode\": \"compact\",\"service_context\":service_context,\"similarity_top_k\":1}, #,\"streaming\":True\r\n        # return_source returns source nodes instead of querying index\r\n        return_source=True,\r\n        # return_messages returns context in message format\r\n        return_messages=True,\r\n   \r\n #       memory=VectorStoreRetrieverMemory(retriever=retriever)\r\n        \r\n    )\r\n\r\n    tool_config = IndexToolConfig(\r\n        query_engine=query_engine, \r\n        name=f\"AI\u865a\u62df\u4e3b\u64ad\",\r\n        description=f\"\u4f60\u5c06\u626e\u6f14\u4e00\u540dAI\u865a\u62df\u4e3b\u64ad\",\r\n        tool_kwargs={\"return_direct\": True}\r\n    )\r\n\r\n    tool = LlamaIndexTool.from_tool_config(tool_config)\r\n    toolkit = LlamaToolkit(\r\n        index_configs=[tool],\r\n    )\r\n\r\n    agent_chain = create_llama_chat_agent(\r\n        toolkit,\r\n        llm,\r\n        memory=memory,\r\n        verbose=True\r\n    )\r\n    \r\n    while True: \r\n        query = input(\"What do you want to ask? \")\r\n        print(agent_chain.memory.chat_memory.messages)\r\n        response_stream = agent_chain.run(query)\r\n        #agent_chain.memory.save_context({\"Human\":query},{\"AI\":response_stream})\r\n        \r\n        if  hasattr(response_stream,'response_gen'):\r\n                for text in response_stream.response_gen:\r\n                    print(text, end=\"\")\r\n                    #todo send to client\r\n        else:\r\n            print(response_stream)\r\n            \r\n\r\n\r\nservice_context=construct_index('D://GPTIndex_Traning')\r\n\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3357/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3357/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3356",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3356/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3356/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3356/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3356",
        "id": 1709506595,
        "node_id": "PR_kwDOIWuq585Qe4yx",
        "number": 3356,
        "title": "readability: LangChain example AgentExecutor variable",
        "user": {
            "login": "zioproto",
            "id": 789701,
            "node_id": "MDQ6VXNlcjc4OTcwMQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/789701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zioproto",
            "html_url": "https://github.com/zioproto",
            "followers_url": "https://api.github.com/users/zioproto/followers",
            "following_url": "https://api.github.com/users/zioproto/following{/other_user}",
            "gists_url": "https://api.github.com/users/zioproto/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zioproto/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zioproto/subscriptions",
            "organizations_url": "https://api.github.com/users/zioproto/orgs",
            "repos_url": "https://api.github.com/users/zioproto/repos",
            "events_url": "https://api.github.com/users/zioproto/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zioproto/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-15T07:41:05Z",
        "updated_at": "2023-05-15T13:54:50Z",
        "closed_at": "2023-05-15T13:54:50Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3356",
            "html_url": "https://github.com/run-llama/llama_index/pull/3356",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3356.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3356.patch",
            "merged_at": "2023-05-15T13:54:50Z"
        },
        "body": "The function `initialize_agent` returns a class\r\n`langchain.agents.agent.AgentExecutor`.\r\nCalling the variable `agent_chain` suggests that the type is a class in `langchain.chains.*` and it is hard to understand the code. This commit changes the variable name to `agent_executor`.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3356/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3356/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3355",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3355/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3355/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3355/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3355",
        "id": 1709285872,
        "node_id": "I_kwDOIWuq585l4aHw",
        "number": 3355,
        "title": "Having gpt-index/llama-index respond to simple things, like hi, thanks, etc.",
        "user": {
            "login": "scooter7",
            "id": 114838,
            "node_id": "MDQ6VXNlcjExNDgzOA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/114838?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/scooter7",
            "html_url": "https://github.com/scooter7",
            "followers_url": "https://api.github.com/users/scooter7/followers",
            "following_url": "https://api.github.com/users/scooter7/following{/other_user}",
            "gists_url": "https://api.github.com/users/scooter7/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/scooter7/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/scooter7/subscriptions",
            "organizations_url": "https://api.github.com/users/scooter7/orgs",
            "repos_url": "https://api.github.com/users/scooter7/repos",
            "events_url": "https://api.github.com/users/scooter7/events{/privacy}",
            "received_events_url": "https://api.github.com/users/scooter7/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-15T04:15:38Z",
        "updated_at": "2023-07-22T18:59:19Z",
        "closed_at": "2023-07-22T18:59:19Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I have an app that is querying documents and doing an excellent job providing answers based upon the content being queried. However, the chatbot responds with \"none\" when prompting with hi/hello/thanks, etc.\r\n\r\nIs there a way to enable responses to simple things like that in addition to restricting the app to the content being queried?\r\n\r\nThanks!",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3355/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3355/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3354",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3354/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3354/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3354/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3354",
        "id": 1709191521,
        "node_id": "I_kwDOIWuq585l4DFh",
        "number": 3354,
        "title": "This page on the llamaindex defining LLM page has broken link",
        "user": {
            "login": "normanlove222",
            "id": 2139671,
            "node_id": "MDQ6VXNlcjIxMzk2NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2139671?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/normanlove222",
            "html_url": "https://github.com/normanlove222",
            "followers_url": "https://api.github.com/users/normanlove222/followers",
            "following_url": "https://api.github.com/users/normanlove222/following{/other_user}",
            "gists_url": "https://api.github.com/users/normanlove222/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/normanlove222/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/normanlove222/subscriptions",
            "organizations_url": "https://api.github.com/users/normanlove222/orgs",
            "repos_url": "https://api.github.com/users/normanlove222/repos",
            "events_url": "https://api.github.com/users/normanlove222/events{/privacy}",
            "received_events_url": "https://api.github.com/users/normanlove222/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318866,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/documentation",
                "name": "documentation",
                "color": "0075ca",
                "default": true,
                "description": "Improvements or additions to documentation"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-05-15T02:05:54Z",
        "updated_at": "2023-10-30T04:29:31Z",
        "closed_at": "2023-10-30T04:29:21Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "page with link error is \r\n[page with error](https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html)\r\n\r\nunder title: Example: Changing the underlying LLM. \r\nyou will see link to word LLM, that link is 404 not found. I know I needed to look at langchain known models and I could not. \ud83d\ude0a\r\n\r\nI didnt check the rest of the page though. ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3354/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3354/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3353",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3353/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3353/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3353/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3353",
        "id": 1709159422,
        "node_id": "I_kwDOIWuq585l37P-",
        "number": 3353,
        "title": "AttributeError: 'LLMPredictor' object has no attribute 'get_text_from_nodes'",
        "user": {
            "login": "jma7889",
            "id": 225801,
            "node_id": "MDQ6VXNlcjIyNTgwMQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/225801?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jma7889",
            "html_url": "https://github.com/jma7889",
            "followers_url": "https://api.github.com/users/jma7889/followers",
            "following_url": "https://api.github.com/users/jma7889/following{/other_user}",
            "gists_url": "https://api.github.com/users/jma7889/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jma7889/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jma7889/subscriptions",
            "organizations_url": "https://api.github.com/users/jma7889/orgs",
            "repos_url": "https://api.github.com/users/jma7889/repos",
            "events_url": "https://api.github.com/users/jma7889/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jma7889/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-15T01:27:11Z",
        "updated_at": "2023-05-15T13:51:21Z",
        "closed_at": "2023-05-15T13:51:20Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "When using custom service context, i got following errors. With the latest llama-index  0.6.6 and langchain  0.0.168 \r\nIf service_context is not used, it works.\r\n\r\n```\r\nfrom llama_index import GPTTreeIndex\r\n...\r\n\r\ndefault_prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\r\ndefault_predictor = LLMPredictor()\r\n\r\nservice_context = ServiceContext.from_defaults(llm_predictor=default_prompt_helper, prompt_helper=default_prompt_helper)\r\nindex = GPTTreeIndex.from_documents(documents, service_context=service_context)\r\n\r\n```\r\n\r\nerror messages\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[8], line 4\r\n      1 from llama_index import GPTTreeIndex\r\n      3 # Custom model config\r\n----> 4 index = GPTTreeIndex.from_documents(documents, service_context=service_context)\r\n      6 # default model config\r\n      7 #index = GPTTreeIndex.from_documents(documents)\r\n      8 \r\n      9 # save index to file\r\n     10 #index.storage_context.persist()\r\n\r\nFile [~/miniconda3/envs/rfp-annotation/lib/python3.10/site-packages/llama_index/indices/base.py:93](https://file+.vscode-resource.vscode-cdn.net/Users/jma/dev/airpunchai/llm-kb/llm-kb/src/gpt_index_poc/~/miniconda3/envs/rfp-annotation/lib/python3.10/site-packages/llama_index/indices/base.py:93), in BaseGPTIndex.from_documents(cls, documents, storage_context, service_context, **kwargs)\r\n     89     docstore.set_document_hash(doc.get_doc_id(), doc.get_doc_hash())\r\n     91 nodes = service_context.node_parser.get_nodes_from_documents(documents)\r\n---> 93 return cls(\r\n     94     nodes=nodes,\r\n     95     storage_context=storage_context,\r\n     96     service_context=service_context,\r\n     97     **kwargs,\r\n     98 )\r\n\r\nFile [~/miniconda3/envs/rfp-annotation/lib/python3.10/site-packages/llama_index/indices/tree/base.py:77](https://file+.vscode-resource.vscode-cdn.net/Users/jma/dev/airpunchai/llm-kb/llm-kb/src/gpt_index_poc/~/miniconda3/envs/rfp-annotation/lib/python3.10/site-packages/llama_index/indices/tree/base.py:77), in GPTTreeIndex.__init__(self, nodes, index_struct, service_context, summary_template, insert_prompt, num_children, build_tree, use_async, **kwargs)\r\n     75 self.build_tree = build_tree\r\n     76 self._use_async = use_async\r\n...\r\n     90     )\r\n     91     indices.append(i)\r\n     92     cur_nodes_chunks.append(cur_nodes_chunk)\r\n\r\nAttributeError: 'LLMPredictor' object has no attribute 'get_text_from_nodes'\r\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?5d0f3945-a4a9-4bba-a308-6a66247b7dee) or open in a [text editor](command:workbench.action.openLargeOutput?5d0f3945-a4a9-4bba-a308-6a66247b7dee). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3353/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3353/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3352",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3352/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3352/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3352/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3352",
        "id": 1709135572,
        "node_id": "PR_kwDOIWuq585Qdp9N",
        "number": 3352,
        "title": "Refactor file parsers to file readers to be consistent with llama hub",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-15T00:34:44Z",
        "updated_at": "2023-05-15T19:05:38Z",
        "closed_at": "2023-05-15T19:05:36Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3352",
            "html_url": "https://github.com/run-llama/llama_index/pull/3352",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3352.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3352.patch",
            "merged_at": "2023-05-15T19:05:36Z"
        },
        "body": "### Summary\r\n* Remove the concept of `BaseParser` in favor of using `BaseReader` (this is already done in llama hub)\r\n* We no longer support concatenate option in `SimpleDirectoryReader` (this is already true in llama hub)\r\n\r\n\r\n### Future work\r\n* Completely remove duplicate code for readers, in favor of single implementation via llama hub. ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3352/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3352/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3351",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3351/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3351/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3351/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3351",
        "id": 1709112688,
        "node_id": "I_kwDOIWuq585l3v1w",
        "number": 3351,
        "title": "Feature request:  OneDrive adaptor",
        "user": {
            "login": "bitcoinbrisbane",
            "id": 8411406,
            "node_id": "MDQ6VXNlcjg0MTE0MDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8411406?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bitcoinbrisbane",
            "html_url": "https://github.com/bitcoinbrisbane",
            "followers_url": "https://api.github.com/users/bitcoinbrisbane/followers",
            "following_url": "https://api.github.com/users/bitcoinbrisbane/following{/other_user}",
            "gists_url": "https://api.github.com/users/bitcoinbrisbane/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/bitcoinbrisbane/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/bitcoinbrisbane/subscriptions",
            "organizations_url": "https://api.github.com/users/bitcoinbrisbane/orgs",
            "repos_url": "https://api.github.com/users/bitcoinbrisbane/repos",
            "events_url": "https://api.github.com/users/bitcoinbrisbane/events{/privacy}",
            "received_events_url": "https://api.github.com/users/bitcoinbrisbane/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-14T23:37:01Z",
        "updated_at": "2023-09-12T16:20:57Z",
        "closed_at": "2023-09-12T16:20:56Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Adding here to see if theres any interest.  Happy to try dev it myself.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3351/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3351/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3350",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3350/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3350/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3350/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3350",
        "id": 1709064315,
        "node_id": "PR_kwDOIWuq585QdcON",
        "number": 3350,
        "title": "feat: use fsspec to manage storage contexts",
        "user": {
            "login": "stillmatic",
            "id": 4743676,
            "node_id": "MDQ6VXNlcjQ3NDM2NzY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4743676?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/stillmatic",
            "html_url": "https://github.com/stillmatic",
            "followers_url": "https://api.github.com/users/stillmatic/followers",
            "following_url": "https://api.github.com/users/stillmatic/following{/other_user}",
            "gists_url": "https://api.github.com/users/stillmatic/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/stillmatic/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/stillmatic/subscriptions",
            "organizations_url": "https://api.github.com/users/stillmatic/orgs",
            "repos_url": "https://api.github.com/users/stillmatic/repos",
            "events_url": "https://api.github.com/users/stillmatic/events{/privacy}",
            "received_events_url": "https://api.github.com/users/stillmatic/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-05-14T20:30:40Z",
        "updated_at": "2023-05-18T20:38:04Z",
        "closed_at": "2023-05-18T18:00:34Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3350",
            "html_url": "https://github.com/run-llama/llama_index/pull/3350",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3350.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3350.patch",
            "merged_at": "2023-05-18T18:00:34Z"
        },
        "body": "The goal of the PR is to enable abstract filesystems, not just local, when dealing with storage files. As an example, this allows us to directly write to a S3 bucket, useful for serving the results in production. Tested working on Cloudflare R2 and local storage, but via fsspec, should support many more, including Azure / GCS / whatever.\r\n\r\nTo do so, we use [fsspec](https://filesystem-spec.readthedocs.io/en/latest/index.html), a popular and well-understood library for abstracting file protocols. This adds fsspec as a core dependency to the package. Note that in order to use S3, the user needs to install [s3fs](https://github.com/fsspec/s3fs) in their local environment. I suggest against requiring fsspec implementations, up to the user to find their own. \r\n\r\nThe API is probably not the cleanest, open to feedback for how to structure it.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3350/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3350/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3349",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3349/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3349/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3349/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3349",
        "id": 1709044482,
        "node_id": "PR_kwDOIWuq585QdYXz",
        "number": 3349,
        "title": "[version] bump version to 0.6.7",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-14T19:11:40Z",
        "updated_at": "2023-05-14T19:14:59Z",
        "closed_at": "2023-05-14T19:14:58Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3349",
            "html_url": "https://github.com/run-llama/llama_index/pull/3349",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3349.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3349.patch",
            "merged_at": "2023-05-14T19:14:58Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3349/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3349/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3347",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3347/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3347/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3347/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3347",
        "id": 1709026933,
        "node_id": "PR_kwDOIWuq585QdVA0",
        "number": 3347,
        "title": "add llm reranker nb",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-14T18:07:03Z",
        "updated_at": "2023-05-14T18:10:14Z",
        "closed_at": "2023-05-14T18:10:13Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3347",
            "html_url": "https://github.com/run-llama/llama_index/pull/3347",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3347.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3347.patch",
            "merged_at": "2023-05-14T18:10:13Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3347/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3347/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3346",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3346/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3346/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3346/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3346",
        "id": 1709018309,
        "node_id": "PR_kwDOIWuq585QdTXn",
        "number": 3346,
        "title": "Support page label for PDF",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-14T17:34:51Z",
        "updated_at": "2023-05-15T19:19:58Z",
        "closed_at": "2023-05-15T19:19:57Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3346",
            "html_url": "https://github.com/run-llama/llama_index/pull/3346",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3346.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3346.patch",
            "merged_at": "2023-05-15T19:19:57Z"
        },
        "body": "### Summary\r\n* Parse page label from pdf and inject as document metadata\r\n\r\n### Additional changes\r\n* switch from using PyPDF2 to pypdf\r\n* parse PDF into page chunks, instead of concatenating everything into a single string",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3346/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3346/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3290",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3290/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3290/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3290/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3290",
        "id": 1708932076,
        "node_id": "I_kwDOIWuq585l3Dvs",
        "number": 3290,
        "title": "I am trying to use HuggingFace Hub model hosted on HuggingFace using HFAPIToken and Llamaindex, but it is asking for OpenAIAPI Key. Can someone point me in the right direction? ",
        "user": {
            "login": "j-amit04",
            "id": 133578780,
            "node_id": "U_kgDOB_ZAHA",
            "avatar_url": "https://avatars.githubusercontent.com/u/133578780?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/j-amit04",
            "html_url": "https://github.com/j-amit04",
            "followers_url": "https://api.github.com/users/j-amit04/followers",
            "following_url": "https://api.github.com/users/j-amit04/following{/other_user}",
            "gists_url": "https://api.github.com/users/j-amit04/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/j-amit04/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/j-amit04/subscriptions",
            "organizations_url": "https://api.github.com/users/j-amit04/orgs",
            "repos_url": "https://api.github.com/users/j-amit04/repos",
            "events_url": "https://api.github.com/users/j-amit04/events{/privacy}",
            "received_events_url": "https://api.github.com/users/j-amit04/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-05-14T12:44:59Z",
        "updated_at": "2023-06-15T00:33:57Z",
        "closed_at": "2023-05-15T10:36:04Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I am trying to connect HuggingFace model hosted on HuggingFace using HFAPI Token and Llamaindex. Here is the code below, but while running this code, it says \"Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)\"\r\n\r\nfrom llama_index import GPTListIndex, SimpleDirectoryReader, ServiceContext,GPTVectorStoreIndex\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\nfrom llama_index import LangchainEmbedding\r\nfrom llama_index.llm_predictor import HuggingFaceLLMPredictor\r\nimport os\r\n\r\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_XXXXX\"\r\n# load in HF embedding model from langchain\r\n\r\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"))\r\n\r\n# configure\r\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\r\n\r\ndocuments = SimpleDirectoryReader('docs').load_data()\r\nindex = GPTVectorStoreIndex.from_documents(documents,service_context=service_context)\r\n\r\nquery_engine = index.as_query_engine(\r\n    retriever_mode=\"embedding\", \r\n    service_context=service_context, \r\n)\r\nresponse = query_engine.query(\r\n    \"What is the population of India?\", \r\n)\r\n\r\nprint(str(response))",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3290/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3290/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3284",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3284/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3284/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3284/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3284",
        "id": 1708889542,
        "node_id": "I_kwDOIWuq585l25XG",
        "number": 3284,
        "title": "Error wile loading saved index from pinecone",
        "user": {
            "login": "raghavpatnecha",
            "id": 25167912,
            "node_id": "MDQ6VXNlcjI1MTY3OTEy",
            "avatar_url": "https://avatars.githubusercontent.com/u/25167912?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/raghavpatnecha",
            "html_url": "https://github.com/raghavpatnecha",
            "followers_url": "https://api.github.com/users/raghavpatnecha/followers",
            "following_url": "https://api.github.com/users/raghavpatnecha/following{/other_user}",
            "gists_url": "https://api.github.com/users/raghavpatnecha/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/raghavpatnecha/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/raghavpatnecha/subscriptions",
            "organizations_url": "https://api.github.com/users/raghavpatnecha/orgs",
            "repos_url": "https://api.github.com/users/raghavpatnecha/repos",
            "events_url": "https://api.github.com/users/raghavpatnecha/events{/privacy}",
            "received_events_url": "https://api.github.com/users/raghavpatnecha/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-14T10:11:49Z",
        "updated_at": "2023-08-30T13:14:41Z",
        "closed_at": "2023-06-11T06:44:50Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I definned the storage context as pinecone. But now it gives me value error if I tried to load my vector store index\r\n````\r\npinecone.init(\r\napi_key=os.environ['PINECONE_API_KEY'],\r\nenvironment=os.environ['PINECONE_ENVIRONMENT']\r\n)\r\npinecone_index = pinecone.Index(\"test\")\r\n\r\nvector_store = PineconeVectorStore(\r\npinecone_index=pinecone_index,\r\n)\r\n\r\nvector_store = PineconeVectorStore(pinecone_index=pinecone_index)\r\n\r\nstorage_context = StorageContext.from_defaults(vector_store)\r\nindex = load_index_from_storage(storage_context=storage_context)`\r\n```\r\n\r\nError : ValueError: No index in storage context, check if you specified the right persist_dir.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3284/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3284/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3283",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3283/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3283/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3283/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3283",
        "id": 1708858287,
        "node_id": "I_kwDOIWuq585l2xuv",
        "number": 3283,
        "title": "Querying Index using local models with no access to download from the hub(s)",
        "user": {
            "login": "sasidhar-danturti",
            "id": 8132557,
            "node_id": "MDQ6VXNlcjgxMzI1NTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8132557?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sasidhar-danturti",
            "html_url": "https://github.com/sasidhar-danturti",
            "followers_url": "https://api.github.com/users/sasidhar-danturti/followers",
            "following_url": "https://api.github.com/users/sasidhar-danturti/following{/other_user}",
            "gists_url": "https://api.github.com/users/sasidhar-danturti/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sasidhar-danturti/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sasidhar-danturti/subscriptions",
            "organizations_url": "https://api.github.com/users/sasidhar-danturti/orgs",
            "repos_url": "https://api.github.com/users/sasidhar-danturti/repos",
            "events_url": "https://api.github.com/users/sasidhar-danturti/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sasidhar-danturti/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-14T08:15:17Z",
        "updated_at": "2023-07-22T19:02:13Z",
        "closed_at": "2023-07-22T19:02:13Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi,\r\n\r\nI want to use llama index on my documents using the local models . The  models/tokenizers etc can not be programmatically downloaded due to the proxies . I am bale to build an index using the locally downloaded models. sample code below :\r\n\r\n```\r\n`\r\nfrom llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex ,PromptHelper\r\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\r\nfrom llama_index import LLMPredictor,ServiceContext\r\nimport torch\r\nfrom langchain.llms.base import LLM\r\nfrom transformers import pipeline\r\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\r\nfrom llama_index.node_parser import SimpleNodeParser\r\nfrom langchain.text_splitter import CharacterTextSplitter\r\n\r\nclass FlanLLM(LLM):\r\n    model_name = \"/local/flan_t5_models/flan-t5-xl\"\r\n    pipeline = pipeline(\"text2text-generation\", model=model_name, device=\"cpu\", model_kwargs={\"torch_dtype\":torch.bfloat16})\r\n\r\n    def _call(self, prompt, stop=None):\r\n        return self.pipeline(prompt, max_length=9999)[0][\"generated_text\"]\r\n\r\n    def _identifying_params(self):\r\n        return {\"name_of_model\": self.model_name}\r\n\r\n    def _llm_type(self):\r\n        return \"custom\"\r\n\r\n\r\nllm_predictor = LLMPredictor(llm=FlanLLM())\r\ntokenizer = T5Tokenizer.from_pretrained(\"/local/flan_t5_models/flan-t5-xl\")\r\n\r\n\r\nembed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"/flan_t5_models/flan-t5-xl\"),\r\n                                 tokenizer=tokenizer)\r\n\r\ndocuments = SimpleDirectoryReader('/data').load_data()\r\nnum_output = 150\r\nmax_input_size = 512\r\nmax_chunk_overlap = 20\r\n\r\nprompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap,tokenizer=tokenizer)\r\ntext_splitter = CharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=100, chunk_overlap=0)\r\nnode_parser = SimpleNodeParser(text_splitter=text_splitter)\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor,embed_model=embed_model,\r\n                                              prompt_helper=prompt_helper,node_parser=node_parser)\r\nindex = GPTListIndex.from_documents(documents, \r\n                                   service_context=service_context)\r\n`\r\n```\r\n\r\n\r\nHowever, when i try to query the index, instead of using the models/tokenizers supplied in service context, an attempt is being made to download from the internet. Below is the code for querying the index,\r\n```\r\n\r\nquery = \"When did the accident occur?\"\r\nquery_engine = index.as_query_engine(service_context=service_context)\r\nresponse = query_engine.query(query)\r\n\r\n\r\n```\r\nThis is being blocked.   Please let me know how i can use the models from local folders for querying. \r\n\r\nThe full, stack trace is below\r\n\r\n```\r\n _query---------------------------------------------------------------------------\r\nSSLEOFError                               Traceback (most recent call last)\r\n/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    699             if is_new_proxy_conn and http_tunnel_required:\r\n--> 700                 self._prepare_proxy(conn)\r\n    701 \r\n\r\n/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py in _prepare_proxy(self, conn)\r\n    995 \r\n--> 996         conn.connect()\r\n    997 \r\n\r\n/opt/conda/lib/python3.8/site-packages/urllib3/connection.py in connect(self)\r\n    413 \r\n--> 414         self.sock = ssl_wrap_socket(\r\n    415             sock=conn,\r\n\r\n/opt/conda/lib/python3.8/site-packages/urllib3/util/ssl_.py in ssl_wrap_socket(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\r\n    448     if send_sni:\r\n--> 449         ssl_sock = _ssl_wrap_socket_impl(\r\n    450             sock, context, tls_in_tls, server_hostname=server_hostname\r\n\r\n/opt/conda/lib/python3.8/site-packages/urllib3/util/ssl_.py in _ssl_wrap_socket_impl(sock, ssl_context, tls_in_tls, server_hostname)\r\n    492     if server_hostname:\r\n--> 493         return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\r\n    494     else:\r\n\r\n/opt/conda/lib/python3.8/ssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\r\n    499         # ctx._wrap_socket()\r\n--> 500         return self.sslsocket_class._create(\r\n    501             sock=sock,\r\n\r\n/opt/conda/lib/python3.8/ssl.py in _create(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\r\n   1039                         raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\r\n-> 1040                     self.do_handshake()\r\n   1041             except (OSError, ValueError):\r\n\r\n/opt/conda/lib/python3.8/ssl.py in do_handshake(self, block)\r\n   1308                 self.settimeout(None)\r\n-> 1309             self._sslobj.do_handshake()\r\n   1310         finally:\r\n\r\nSSLEOFError: EOF occurred in violation of protocol (_ssl.c:1125)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMaxRetryError                             Traceback (most recent call last)\r\n/opt/conda/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    488             if not chunked:\r\n--> 489                 resp = conn.urlopen(\r\n    490                     method=request.method,\r\n\r\n/opt/conda/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\r\n    786 \r\n--> 787             retries = retries.increment(\r\n    788                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n\r\n/opt/conda/lib/python3.8/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)\r\n    591         if new_retry.is_exhausted():\r\n--> 592             raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\n    593 \r\n\r\nMaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2/resolve/main/tokenizer_config.json (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)')))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nSSLError                                  Traceback (most recent call last)\r\n<ipython-input-29-7f8f9381f2d8> in <module>\r\n      1 query = \"What was the accident?\"\r\n      2 query_engine = index.as_query_engine(service_context=service_context)\r\n----> 3 response = query_engine.query(query)\r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/indices/query/base.py in query(self, str_or_query_bundle)\r\n     18         if isinstance(str_or_query_bundle, str):\r\n     19             str_or_query_bundle = QueryBundle(str_or_query_bundle)\r\n---> 20         return self.(str_or_query_bundle)\r\n     21 \r\n     22     async def aquery(self, str_or_query_bundle: QueryType) -> RESPONSE_TYPE:\r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/query_engine/retriever_query_engine.py in _query(self, query_bundle)\r\n    143 \r\n    144         synth_id = self.callback_manager.on_event_start(CBEventType.SYNTHESIZE)\r\n--> 145         response = self._response_synthesizer.synthesize(\r\n    146             query_bundle=query_bundle,\r\n    147             nodes=nodes,\r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/indices/query/response_synthesis.py in synthesize(self, query_bundle, nodes, additional_source_nodes)\r\n    161         if self._response_mode != ResponseMode.NO_TEXT:\r\n    162             assert self._response_builder is not None\r\n--> 163             response_str = self._response_builder.get_response(\r\n    164                 query_str=query_bundle.query_str,\r\n    165                 text_chunks=text_chunks,\r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/indices/response/response_builder.py in get_response(self, query_str, text_chunks, prev_response, **response_kwargs)\r\n    293                 max_prompt, text_chunks\r\n    294             )\r\n--> 295             response = super().get_response(\r\n    296                 query_str=query_str, text_chunks=new_texts, prev_response=prev_response\r\n    297             )\r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/token_counter/token_counter.py in wrapped_llm_predict(_self, *args, **kwargs)\r\n     76         def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\r\n     77             with wrapper_logic(_self):\r\n---> 78                 f_return_val = f(_self, *args, **kwargs)\r\n     79 \r\n     80             return f_return_val\r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/indices/response/response_builder.py in get_response(self, query_str, text_chunks, prev_response, **response_kwargs)\r\n    132                 # if this is the first chunk, and text chunk already\r\n    133                 # is an answer, then return it\r\n--> 134                 response = self._give_response_single(\r\n    135                     query_str,\r\n    136                     text_chunk,\r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/indices/response/response_builder.py in _give_response_single(self, query_str, text_chunk, **response_kwargs)\r\n    169                     response,\r\n    170                     formatted_prompt,\r\n--> 171                 ) = self._service_context.llm_predictor.predict(\r\n    172                     text_qa_template,\r\n    173                     context_str=cur_text_chunk,\r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/llm_predictor/base.py in predict(self, prompt, **prompt_args)\r\n    245         # We assume that the value of formatted_prompt is exactly the thing\r\n    246         # eventually sent to OpenAI, or whatever LLM downstream\r\n--> 247         prompt_tokens_count = self._count_tokens(formatted_prompt)\r\n    248         prediction_tokens_count = self._count_tokens(llm_prediction)\r\n    249         self._total_tokens_used += prompt_tokens_count + prediction_tokens_count\r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/llm_predictor/base.py in _count_tokens(self, text)\r\n    294 \r\n    295     def _count_tokens(self, text: str) -> int:\r\n--> 296         tokens = globals_helper.tokenizer(text)\r\n    297         return len(tokens)\r\n    298 \r\n\r\n/opt/conda/lib/python3.8/site-packages/llama_index/utils.py in tokenizer(self)\r\n     59                     )\r\n     60 \r\n---> 61                 tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\r\n     62 \r\n     63                 def tokenizer_fn(text: str) -> List:\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\r\n   1698                 # Try to get the tokenizer config to see if there are versioned tokenizer files.\r\n   1699                 fast_tokenizer_file = FULL_TOKENIZER_FILE\r\n-> 1700                 resolved_config_file = cached_file(\r\n   1701                     pretrained_model_name_or_path,\r\n   1702                     TOKENIZER_CONFIG_FILE,\r\n\r\n/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\r\n    407     try:\r\n    408         # Load from URL or cache if already cached\r\n--> 409         resolved_file = hf_hub_download(\r\n    410             path_or_repo_id,\r\n    411             filename,\r\n\r\n/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py in hf_hub_download(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\r\n   1051         try:\r\n   1052             try:\r\n-> 1053                 metadata = get_hf_file_metadata(\r\n   1054                     url=url,\r\n   1055                     use_auth_token=use_auth_token,\r\n\r\n/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py in get_hf_file_metadata(url, use_auth_token, proxies, timeout)\r\n   1348 \r\n   1349     # Retrieve metadata\r\n-> 1350     r = _request_wrapper(\r\n   1351         method=\"HEAD\",\r\n   1352         url=url,\r\n\r\n/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py in _request_wrapper(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\r\n    396     # 2. Force relative redirection\r\n    397     if follow_relative_redirects:\r\n--> 398         response = _request_wrapper(\r\n    399             method=method,\r\n    400             url=url,\r\n\r\n/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py in _request_wrapper(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\r\n    431 \r\n    432     # 3. Exponential backoff\r\n--> 433     return http_backoff(\r\n    434         method=method,\r\n    435         url=url,\r\n\r\n/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_http.py in http_backoff(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\r\n    103         try:\r\n    104             # Perform request and return if status_code is not in the retry list.\r\n--> 105             response = requests.request(method=method, url=url, **kwargs)\r\n    106             if response.status_code not in retry_on_status_codes:\r\n    107                 return response\r\n\r\n/opt/conda/lib/python3.8/site-packages/requests/api.py in request(method, url, **kwargs)\r\n     57     # cases, and look like a memory leak in others.\r\n     58     with sessions.Session() as session:\r\n---> 59         return session.request(method=method, url=url, **kwargs)\r\n     60 \r\n     61 \r\n\r\n/opt/conda/lib/python3.8/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\r\n    585         }\r\n    586         send_kwargs.update(settings)\r\n--> 587         resp = self.send(prep, **send_kwargs)\r\n    588 \r\n    589         return resp\r\n\r\n/opt/conda/lib/python3.8/site-packages/requests/sessions.py in send(self, request, **kwargs)\r\n    699 \r\n    700         # Send the request\r\n--> 701         r = adapter.send(request, **kwargs)\r\n    702 \r\n    703         # Total elapsed time of the request (approximately)\r\n\r\n/opt/conda/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    561             if isinstance(e.reason, _SSLError):\r\n    562                 # This branch is for urllib3 v1.22 and later.\r\n--> 563                 raise SSLError(e, request=request)\r\n    564 \r\n    565             raise ConnectionError(e, request=request)\r\n\r\nSSLError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /gpt2/resolve/main/tokenizer_config.json (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)')))\r\n```\r\n\r\nPlease let me know if there is a way out of this.\r\n\r\nMany Thanks\r\nSasidhar D",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3283/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3283/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3282",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3282/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3282/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3282/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3282",
        "id": 1708848692,
        "node_id": "I_kwDOIWuq585l2vY0",
        "number": 3282,
        "title": "Querying Index using Local models/tokenizers",
        "user": {
            "login": "sasidhar-danturti",
            "id": 8132557,
            "node_id": "MDQ6VXNlcjgxMzI1NTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8132557?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sasidhar-danturti",
            "html_url": "https://github.com/sasidhar-danturti",
            "followers_url": "https://api.github.com/users/sasidhar-danturti/followers",
            "following_url": "https://api.github.com/users/sasidhar-danturti/following{/other_user}",
            "gists_url": "https://api.github.com/users/sasidhar-danturti/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sasidhar-danturti/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sasidhar-danturti/subscriptions",
            "organizations_url": "https://api.github.com/users/sasidhar-danturti/orgs",
            "repos_url": "https://api.github.com/users/sasidhar-danturti/repos",
            "events_url": "https://api.github.com/users/sasidhar-danturti/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sasidhar-danturti/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-14T07:37:13Z",
        "updated_at": "2023-05-14T08:09:40Z",
        "closed_at": "2023-05-14T08:09:40Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3282/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3282/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3277",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3277/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3277/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3277/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3277",
        "id": 1708787150,
        "node_id": "I_kwDOIWuq585l2gXO",
        "number": 3277,
        "title": "gpt-4 and gpt-4-32k support",
        "user": {
            "login": "jma7889",
            "id": 225801,
            "node_id": "MDQ6VXNlcjIyNTgwMQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/225801?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jma7889",
            "html_url": "https://github.com/jma7889",
            "followers_url": "https://api.github.com/users/jma7889/followers",
            "following_url": "https://api.github.com/users/jma7889/following{/other_user}",
            "gists_url": "https://api.github.com/users/jma7889/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jma7889/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jma7889/subscriptions",
            "organizations_url": "https://api.github.com/users/jma7889/orgs",
            "repos_url": "https://api.github.com/users/jma7889/repos",
            "events_url": "https://api.github.com/users/jma7889/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jma7889/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-14T01:37:32Z",
        "updated_at": "2023-05-14T15:45:35Z",
        "closed_at": "2023-05-14T15:45:35Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Does the llama_index support gpt-4 's 8k input or gpt-4-32k ' s 32 k input? I tried to used them but give me error such as\r\n```\r\nInvalidRequestError                       Traceback (most recent call last)\r\nCell In[37], line 5\r\n      2 from langchain import OpenAI\r\n      4 # index = GPTSimpleVectorIndex(documents, llm_predictor=selected_predictor)\r\n----> 5 index = GPTTreeIndex.from_documents(documents, service_context=service_context)\r\n      7 # save index to file\r\n      8 index.storage_context.persist()\r\n\r\nFile [~/miniconda3/envs/rfp-annotation/lib/python3.10/site-packages/llama_index/indices/base.py:93](https://file+.vscode-resource.vscode-cdn.net/Users/jma/dev/airpunchai/llm-kb/llm-kb/src/gpt_index_poc/~/miniconda3/envs/rfp-annotation/lib/python3.10/site-packages/llama_index/indices/base.py:93), in BaseGPTIndex.from_documents(cls, documents, storage_context, service_context, **kwargs)\r\n     89     docstore.set_document_hash(doc.get_doc_id(), doc.get_doc_hash())\r\n     91 nodes = service_context.node_parser.get_nodes_from_documents(documents)\r\n---> 93 return cls(\r\n     94     nodes=nodes,\r\n     95     storage_context=storage_context,\r\n     96     service_context=service_context,\r\n     97     **kwargs,\r\n     98 )\r\n\r\nFile [~/miniconda3/envs/rfp-annotation/lib/python3.10/site-packages/llama_index/indices/tree/base.py:77](https://file+.vscode-resource.vscode-cdn.net/Users/jma/dev/airpunchai/llm-kb/llm-kb/src/gpt_index_poc/~/miniconda3/envs/rfp-annotation/lib/python3.10/site-packages/llama_index/indices/tree/base.py:77), in GPTTreeIndex.__init__(self, nodes, index_struct, service_context, summary_template, insert_prompt, num_children, build_tree, use_async, **kwargs)\r\n     75 self.build_tree = build_tree\r\n     76 self._use_async = use_async\r\n---> 77 super().__init__(\r\n     78     nodes=nodes,\r\n     79     index_struct=index_struct,\r\n...\r\n    683         rbody, rcode, resp.data, rheaders, stream_error=stream_error\r\n    684     )\r\n    685 return resp\r\n\r\nInvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 6356 tokens. Please reduce the length of the messages.\r\n```\r\nmy code to select the model:\r\n```\r\ngpt4_32_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-4-32k\"))\r\ngpt4_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-4\"))\r\n\r\nselected_predictor = gpt4_predictor\r\n\r\n# define prompt helper\r\n# set maximum input size\r\nmax_input_size = 4096\r\n# set number of output tokens\r\nnum_output = 256\r\n# set maximum chunk overlap\r\nmax_chunk_overlap = 20\r\ndefault_prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\r\ngpt4_prompt_helper = PromptHelper(8191, num_output, max_chunk_overlap)\r\ngpt4_32_prompt_helper = PromptHelper(32765, num_output, max_chunk_overlap)\r\n\r\nselected_prompt_helper = gpt4_prompt_helper\r\n\r\nservice_context = ServiceContext.from_defaults(llm_predictor=gpt35_predictor, prompt_helper=selected_prompt_helper)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3277/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3277/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3276",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3276/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3276/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3276/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3276",
        "id": 1708752564,
        "node_id": "PR_kwDOIWuq585Qcgfc",
        "number": 3276,
        "title": "add llm reranker",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-13T22:07:48Z",
        "updated_at": "2023-05-14T18:04:03Z",
        "closed_at": "2023-05-14T18:04:02Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3276",
            "html_url": "https://github.com/run-llama/llama_index/pull/3276",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3276.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3276.patch",
            "merged_at": "2023-05-14T18:04:02Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3276/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3276/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3275",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3275/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3275/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3275/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3275",
        "id": 1708510990,
        "node_id": "PR_kwDOIWuq585Qbyo3",
        "number": 3275,
        "title": "fix: GPTSimpleKeywordTableIndex query error with no keywords mentioned",
        "user": {
            "login": "aidenchen01",
            "id": 132824290,
            "node_id": "U_kgDOB-q84g",
            "avatar_url": "https://avatars.githubusercontent.com/u/132824290?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/aidenchen01",
            "html_url": "https://github.com/aidenchen01",
            "followers_url": "https://api.github.com/users/aidenchen01/followers",
            "following_url": "https://api.github.com/users/aidenchen01/following{/other_user}",
            "gists_url": "https://api.github.com/users/aidenchen01/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/aidenchen01/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/aidenchen01/subscriptions",
            "organizations_url": "https://api.github.com/users/aidenchen01/orgs",
            "repos_url": "https://api.github.com/users/aidenchen01/repos",
            "events_url": "https://api.github.com/users/aidenchen01/events{/privacy}",
            "received_events_url": "https://api.github.com/users/aidenchen01/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-13T08:06:29Z",
        "updated_at": "2023-06-27T18:23:55Z",
        "closed_at": "2023-06-27T18:23:54Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3275",
            "html_url": "https://github.com/run-llama/llama_index/pull/3275",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3275.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3275.patch",
            "merged_at": null
        },
        "body": "While we query something that is not mentioned in original doc , it fetches zero node, and pop up ZeroDivisionError: integer division or modulo by zero\r\n\r\nHere is my original program\r\n```python\r\nindex = GPTSimpleKeywordTableIndex(nodes,storage_context=storage_context, service_context=service_context)\r\nretriever = index.as_retriever(retriever_mode='simple', similarity_top_k=1)\r\nresponse_synthesizer = ResponseSynthesizer.from_args(\r\n    node_postprocessors=[\r\n        SimilarityPostprocessor(similarity_cutoff=0.7)\r\n    ],service_context=service_context\r\n)\r\nquery_engine = RetrieverQueryEngine.from_args(\r\n    retriever=retriever,\r\n    response_synthesizer=response_synthesizer,\r\n    response_mode=\"tree_summarize\",\r\n    verbose=True,\r\n    service_context=service_context\r\n)\r\nquery=\"xxxxxx\"\r\nresponse = query_engine.query(query)\r\n```\r\nwith error like below:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/username/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-a1d9bf635e11>\", line 1, in <module>\r\n    runfile('/tmp/pycharm_project_78/test llamindex basic.py', wdir='/tmp/pycharm_project_78/')\r\n  File \"/home/username/.pycharm_helpers/pydev/_pydev_bundle/pydev_umd.py\", line 198, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/home/username/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/tmp/pycharm_project_78/test llamindex basic.py\", line 157, in <module>\r\n    response = get_response_with_en(query,service_context,query_engine)\r\n  File \"/tmp/pycharm_project_78/user_module/utils.py\", line 206, in get_response_with_en\r\n    response = query_engine.query(query)\r\n  File \"/tmp/pycharm_project_78/llama_index/indices/query/base.py\", line 20, in query\r\n    return self._query(str_or_query_bundle)\r\n  File \"/tmp/pycharm_project_78/llama_index/query_engine/retriever_query_engine.py\", line 145, in _query\r\n    response = self._response_synthesizer.synthesize(\r\n  File \"/tmp/pycharm_project_78/llama_index/indices/query/response_synthesis.py\", line 163, in synthesize\r\n    response_str = self._response_builder.get_response(\r\n  File \"/tmp/pycharm_project_78/llama_index/token_counter/token_counter.py\", line 78, in wrapped_llm_predict\r\n    f_return_val = f(_self, *args, **kwargs)\r\n  File \"/tmp/pycharm_project_78/llama_index/indices/response/response_builder.py\", line 376, in get_response\r\n    return self._get_tree_response_over_root_nodes(\r\n  File \"/tmp/pycharm_project_78/llama_index/indices/response/response_builder.py\", line 421, in _get_tree_response_over_root_nodes\r\n    node_text = self._service_context.prompt_helper.get_text_from_nodes(\r\n  File \"/tmp/pycharm_project_78/llama_index/indices/prompt_helper.py\", line 178, in get_text_from_nodes\r\n    text_splitter = self.get_text_splitter_given_prompt(\r\n  File \"/tmp/pycharm_project_78/llama_index/indices/prompt_helper.py\", line 159, in get_text_splitter_given_prompt\r\n    chunk_size = self.get_chunk_size_given_prompt(\r\n  File \"/tmp/pycharm_project_78/llama_index/indices/prompt_helper.py\", line 105, in get_chunk_size_given_prompt\r\n    result = (\r\nZeroDivisionError: integer division or modulo by zero\r\n```\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3275/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3275/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3274",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3274/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3274/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3274/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3274",
        "id": 1708454234,
        "node_id": "PR_kwDOIWuq585Qbmlj",
        "number": 3274,
        "title": "add attribution ",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-13T06:17:41Z",
        "updated_at": "2023-05-13T12:45:38Z",
        "closed_at": "2023-05-13T12:45:37Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3274",
            "html_url": "https://github.com/run-llama/llama_index/pull/3274",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3274.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3274.patch",
            "merged_at": "2023-05-13T12:45:37Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3274/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3274/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3273",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3273/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3273/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3273/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3273",
        "id": 1708452496,
        "node_id": "PR_kwDOIWuq585QbmQ5",
        "number": 3273,
        "title": "add list llm retriever",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-13T06:09:52Z",
        "updated_at": "2023-05-14T04:40:49Z",
        "closed_at": "2023-05-14T04:40:48Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3273",
            "html_url": "https://github.com/run-llama/llama_index/pull/3273",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3273.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3273.patch",
            "merged_at": "2023-05-14T04:40:48Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3273/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3273/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3272",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3272/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3272/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3272/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3272",
        "id": 1708341358,
        "node_id": "PR_kwDOIWuq585QbQKk",
        "number": 3272,
        "title": "[version] bump version to 0.6.6",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-12T23:38:39Z",
        "updated_at": "2023-05-13T03:04:45Z",
        "closed_at": "2023-05-13T03:04:45Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3272",
            "html_url": "https://github.com/run-llama/llama_index/pull/3272",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3272.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3272.patch",
            "merged_at": "2023-05-13T03:04:45Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3272/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3272/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3271",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3271/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3271/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3271/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3271",
        "id": 1708329622,
        "node_id": "PR_kwDOIWuq585QbNip",
        "number": 3271,
        "title": "Raise value error for Redis for metadata filter ",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-12T23:20:23Z",
        "updated_at": "2023-05-12T23:26:23Z",
        "closed_at": "2023-05-12T23:26:22Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3271",
            "html_url": "https://github.com/run-llama/llama_index/pull/3271",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3271.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3271.patch",
            "merged_at": "2023-05-12T23:26:22Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3271/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3271/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3270",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3270/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3270/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3270/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3270",
        "id": 1708226827,
        "node_id": "I_kwDOIWuq585l0XkL",
        "number": 3270,
        "title": "query without access LLM",
        "user": {
            "login": "NaelsonAccountDrive",
            "id": 122898249,
            "node_id": "U_kgDOB1NHSQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/122898249?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/NaelsonAccountDrive",
            "html_url": "https://github.com/NaelsonAccountDrive",
            "followers_url": "https://api.github.com/users/NaelsonAccountDrive/followers",
            "following_url": "https://api.github.com/users/NaelsonAccountDrive/following{/other_user}",
            "gists_url": "https://api.github.com/users/NaelsonAccountDrive/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/NaelsonAccountDrive/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/NaelsonAccountDrive/subscriptions",
            "organizations_url": "https://api.github.com/users/NaelsonAccountDrive/orgs",
            "repos_url": "https://api.github.com/users/NaelsonAccountDrive/repos",
            "events_url": "https://api.github.com/users/NaelsonAccountDrive/events{/privacy}",
            "received_events_url": "https://api.github.com/users/NaelsonAccountDrive/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-12T21:22:41Z",
        "updated_at": "2023-06-06T05:45:29Z",
        "closed_at": "2023-06-06T05:45:28Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I have  construction commented, disable GPT, it seems be smart answer yet, why it? \r\n```\r\n# index = construct_index(\"docs\")\r\n```\r\nHaving info of consume tokens same ChatOpenAI commented.\r\n\r\nINFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2048 tokens\r\n> [get_response] Total LLM token usage: 2048 tokens\r\n\r\n```\r\n    index = GPTSimpleVectorIndex.load_from_disk('index.json')\r\n    response = index.query(input_text, response_mode=\"compact\")\r\n```\r\n`def construct_index(directory_path):\r\n    max_input_size = 4096\r\n    num_outputs = 512\r\n    max_chunk_overlap = 20\r\n    chunk_size_limit = 600\r\n\r\n    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\r\n    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\r\n\r\n    documents = SimpleDirectoryReader(directory_path).load_data()\r\n    pprint(documents)\r\n    \r\n    index = GPTSimpleVectorIndex(documents)\r\n\r\n    index.save_to_disk('index.json')\r\n\r\n    return index\r\n\r\ndef chatbot(input_text):\r\n    index = GPTSimpleVectorIndex.load_from_disk('index.json')\r\n    response = index.query(input_text, response_mode=\"compact\")\r\n    return response.response\r\n\r\niface = gr.Interface(fn=chatbot,\r\n                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\r\n                     outputs=\"text\",\r\n                     title=\"Custom-trained AI Chatbot\")\r\n\r\n# index = construct_index(\"docs\")\r\niface.launch(share=True, server_port=3000)`",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3270/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3270/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3269",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3269/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3269/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3269/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3269",
        "id": 1708011349,
        "node_id": "PR_kwDOIWuq585QaHni",
        "number": 3269,
        "title": "Do not duplicate extra_info into text when saving Node into vector store",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-12T18:01:51Z",
        "updated_at": "2023-05-12T19:39:03Z",
        "closed_at": "2023-05-12T19:39:01Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3269",
            "html_url": "https://github.com/run-llama/llama_index/pull/3269",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3269.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3269.patch",
            "merged_at": "2023-05-12T19:39:01Z"
        },
        "body": "### Summary\r\n* Previously, we were using `Node.get_text()` to save Node text into vector store, while also saving the extra info, this means that after loading the Node back, calling `Node.get_text()` would have the metadata injected twice into the Node text. \r\n* This fixes the bug, by saving the `Node.text` to avoid duplicating the metadata.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3269/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3269/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3268",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3268/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3268/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3268/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3268",
        "id": 1707744760,
        "node_id": "PR_kwDOIWuq585QZOZh",
        "number": 3268,
        "title": "quick payload fix for llm callback",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-12T14:34:34Z",
        "updated_at": "2023-11-14T03:38:41Z",
        "closed_at": "2023-05-12T14:38:11Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3268",
            "html_url": "https://github.com/run-llama/llama_index/pull/3268",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3268.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3268.patch",
            "merged_at": "2023-05-12T14:38:11Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3268/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3268/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3267",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3267/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3267/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3267/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3267",
        "id": 1707659407,
        "node_id": "PR_kwDOIWuq585QY77f",
        "number": 3267,
        "title": "docs: update docstore readme to include proper import",
        "user": {
            "login": "jack-hughes",
            "id": 4501812,
            "node_id": "MDQ6VXNlcjQ1MDE4MTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4501812?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jack-hughes",
            "html_url": "https://github.com/jack-hughes",
            "followers_url": "https://api.github.com/users/jack-hughes/followers",
            "following_url": "https://api.github.com/users/jack-hughes/following{/other_user}",
            "gists_url": "https://api.github.com/users/jack-hughes/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jack-hughes/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jack-hughes/subscriptions",
            "organizations_url": "https://api.github.com/users/jack-hughes/orgs",
            "repos_url": "https://api.github.com/users/jack-hughes/repos",
            "events_url": "https://api.github.com/users/jack-hughes/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jack-hughes/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-12T13:40:52Z",
        "updated_at": "2023-05-12T18:03:35Z",
        "closed_at": "2023-05-12T18:03:35Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3267",
            "html_url": "https://github.com/run-llama/llama_index/pull/3267",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3267.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3267.patch",
            "merged_at": "2023-05-12T18:03:35Z"
        },
        "body": "Looks like there was a broken import in the documentation, this should resolve.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3267/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3267/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3266",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3266/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3266/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3266/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3266",
        "id": 1707448775,
        "node_id": "I_kwDOIWuq585lxZnH",
        "number": 3266,
        "title": "How to get `data_wiki` data",
        "user": {
            "login": "ayulockin",
            "id": 31141479,
            "node_id": "MDQ6VXNlcjMxMTQxNDc5",
            "avatar_url": "https://avatars.githubusercontent.com/u/31141479?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ayulockin",
            "html_url": "https://github.com/ayulockin",
            "followers_url": "https://api.github.com/users/ayulockin/followers",
            "following_url": "https://api.github.com/users/ayulockin/following{/other_user}",
            "gists_url": "https://api.github.com/users/ayulockin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ayulockin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ayulockin/subscriptions",
            "organizations_url": "https://api.github.com/users/ayulockin/orgs",
            "repos_url": "https://api.github.com/users/ayulockin/repos",
            "events_url": "https://api.github.com/users/ayulockin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ayulockin/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-12T11:23:07Z",
        "updated_at": "2023-05-12T11:26:22Z",
        "closed_at": "2023-05-12T11:26:21Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "In the notebook, `docs/examples/usecases/City_Analysis-Decompose-KeywordTable.ipynb`, `Path(\"data_wiki\")` is created. I am not able to find `data_wiki` in the repo or able to find any download link.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3266/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3266/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3265",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3265/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3265/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3265/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3265",
        "id": 1707408371,
        "node_id": "I_kwDOIWuq585lxPvz",
        "number": 3265,
        "title": "Is this response normal for diff models? it is different from gpt-3.5-turbo model and local model ",
        "user": {
            "login": "joostshao",
            "id": 3615951,
            "node_id": "MDQ6VXNlcjM2MTU5NTE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3615951?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/joostshao",
            "html_url": "https://github.com/joostshao",
            "followers_url": "https://api.github.com/users/joostshao/followers",
            "following_url": "https://api.github.com/users/joostshao/following{/other_user}",
            "gists_url": "https://api.github.com/users/joostshao/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/joostshao/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/joostshao/subscriptions",
            "organizations_url": "https://api.github.com/users/joostshao/orgs",
            "repos_url": "https://api.github.com/users/joostshao/repos",
            "events_url": "https://api.github.com/users/joostshao/events{/privacy}",
            "received_events_url": "https://api.github.com/users/joostshao/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-12T10:53:30Z",
        "updated_at": "2023-05-14T14:41:53Z",
        "closed_at": "2023-05-14T14:41:53Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "below is the code \r\ngpt-3.5-turbo streaming\r\n```\r\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\r\nfrom llama_index import StorageContext, load_index_from_storage\r\nfrom llama_index import (\r\n    GPTVectorStoreIndex,\r\n    SimpleDirectoryReader,\r\n    ServiceContext,\r\n    LLMPredictor,\r\n)\r\nfrom langchain import OpenAI\r\nfrom langchain.llms.openai import OpenAIChat\r\nfrom langchain.chat_models import ChatOpenAI\r\nimport os\r\nfrom llama_index import QuestionAnswerPrompt\r\nos.environ[\"OPENAI_API_KEY\"] = \"sk-ssss\"\r\n\r\nllm_predictor = LLMPredictor(\r\n    llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True)\r\n)\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\n\r\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\r\nindex = load_index_from_storage(storage_context, service_context=service_context)\r\n\r\nquery_engine = index.as_query_engine(streaming=True, similarity_top_k=1)\r\nstreaming_response = query_engine.query(\r\n    \"summary it in 100 words?\",\r\n)\r\nstreaming_response.print_response_stream()\r\n```\r\n\r\n---------------------\r\n\r\nlocal llm\r\n```\r\nfrom llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\r\nfrom llama_index import StorageContext, load_index_from_storage\r\nfrom langchain import OpenAI\r\n\r\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\r\nindex = load_index_from_storage(storage_context)\r\nquery_engine = index.as_query_engine()\r\nprint(query_engine.query(\"summary it in 100 words?\"))\r\n```\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3265/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3265/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3264",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3264/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3264/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3264/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3264",
        "id": 1707259535,
        "node_id": "I_kwDOIWuq585lwraP",
        "number": 3264,
        "title": "Error in v0.6.5 in Azure environment",
        "user": {
            "login": "ryugonomura",
            "id": 7021453,
            "node_id": "MDQ6VXNlcjcwMjE0NTM=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7021453?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ryugonomura",
            "html_url": "https://github.com/ryugonomura",
            "followers_url": "https://api.github.com/users/ryugonomura/followers",
            "following_url": "https://api.github.com/users/ryugonomura/following{/other_user}",
            "gists_url": "https://api.github.com/users/ryugonomura/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ryugonomura/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ryugonomura/subscriptions",
            "organizations_url": "https://api.github.com/users/ryugonomura/orgs",
            "repos_url": "https://api.github.com/users/ryugonomura/repos",
            "events_url": "https://api.github.com/users/ryugonomura/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ryugonomura/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 11,
        "created_at": "2023-05-12T09:11:53Z",
        "updated_at": "2023-05-30T16:19:30Z",
        "closed_at": "2023-05-15T05:45:17Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I would like to have this fixed in #3140 , but the following error is still occurring.\r\nIn fact, the following error occurs when executing query_engine.query(\"sample string\").\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/home/product/.cache/pypoetry/virtualenvs/chatbot-eVeRnpX_-py3.11/lib/python3.11/site-packages/tenacity/__init__.py\", line 382, in __call__\r\n    result = fn(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/product/.cache/pypoetry/virtualenvs/chatbot-eVeRnpX_-py3.11/lib/python3.11/site-packages/llama_index/embeddings/openai.py\", line 105, in get_embedding\r\n    return openai.Embedding.create(input=[text], model=engine, **kwargs)[\"data\"][0][\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/product/.cache/pypoetry/virtualenvs/chatbot-eVeRnpX_-py3.11/lib/python3.11/site-packages/openai/api_resources/embedding.py\", line 33, in create\r\n    response = super().create(*args, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/product/.cache/pypoetry/virtualenvs/chatbot-eVeRnpX_-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 149, in create\r\n    ) = cls.__prepare_create_request(\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/product/.cache/pypoetry/virtualenvs/chatbot-eVeRnpX_-py3.11/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 83, in __prepare_create_request\r\n    raise error.InvalidRequestError(\r\nopenai.error.InvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.embedding.Embedding'>\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3264/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3264/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3263",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3263/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3263/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3263/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3263",
        "id": 1707237502,
        "node_id": "I_kwDOIWuq585lwmB-",
        "number": 3263,
        "title": "is GPTSimpleVectorIndex still being used?",
        "user": {
            "login": "normanlove222",
            "id": 2139671,
            "node_id": "MDQ6VXNlcjIxMzk2NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2139671?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/normanlove222",
            "html_url": "https://github.com/normanlove222",
            "followers_url": "https://api.github.com/users/normanlove222/followers",
            "following_url": "https://api.github.com/users/normanlove222/following{/other_user}",
            "gists_url": "https://api.github.com/users/normanlove222/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/normanlove222/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/normanlove222/subscriptions",
            "organizations_url": "https://api.github.com/users/normanlove222/orgs",
            "repos_url": "https://api.github.com/users/normanlove222/repos",
            "events_url": "https://api.github.com/users/normanlove222/events{/privacy}",
            "received_events_url": "https://api.github.com/users/normanlove222/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2023-05-12T08:59:07Z",
        "updated_at": "2023-05-19T15:25:56Z",
        "closed_at": "2023-05-14T17:44:03Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "![Screenshot (698)](https://github.com/jerryjliu/llama_index/assets/2139671/023634a3-a9b2-4d33-a11a-c7bed214ed70)\r\n\r\nis **GPTSimpleVectorIndex** still being used? Cause I see in some places, here on the issues section, that its been replaced by another Method. \r\n\r\nYet as you can see in llamahub.ai, a companion site of this project, it shows its still being used. \r\nCan we get some clarity, and maybe even pointed to a page that has some of the recent changes, so we can update the example code. ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3263/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3263/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3262",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3262/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3262/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3262/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3262",
        "id": 1707126252,
        "node_id": "I_kwDOIWuq585lwK3s",
        "number": 3262,
        "title": "Problem using a custom LLM in int8 format",
        "user": {
            "login": "lborcard",
            "id": 51543572,
            "node_id": "MDQ6VXNlcjUxNTQzNTcy",
            "avatar_url": "https://avatars.githubusercontent.com/u/51543572?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lborcard",
            "html_url": "https://github.com/lborcard",
            "followers_url": "https://api.github.com/users/lborcard/followers",
            "following_url": "https://api.github.com/users/lborcard/following{/other_user}",
            "gists_url": "https://api.github.com/users/lborcard/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lborcard/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lborcard/subscriptions",
            "organizations_url": "https://api.github.com/users/lborcard/orgs",
            "repos_url": "https://api.github.com/users/lborcard/repos",
            "events_url": "https://api.github.com/users/lborcard/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lborcard/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-12T07:43:24Z",
        "updated_at": "2023-07-22T19:02:54Z",
        "closed_at": "2023-07-22T19:02:54Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi,\r\n\r\nI am trying to use a model from huggingface using bits and bytes and it is throwing an error saying I cannot use a 8 bit model. Is it a problem with the pipeline class of the transformers package?\r\n\r\nhere is my pipeline \r\n\r\n```\r\ntransformers v.4.29\r\n\r\nmodel_name = \"chaoyi-wu/PMC_LLAMA_7B\"\r\n\r\n## Model\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n  model_name,\r\n  device_map='auto',\r\n  load_in_8bit=True,\r\n  max_memory=max_memory)\r\n\r\n## llm class\r\n\r\nclass CustomLLM(LLM):\r\n    \r\n    pipeline = pipeline(\"text-generation\",tokenizer = tokenizer, model=model, device=\"cuda:0\")\r\n\r\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\r\n        prompt_length = len(prompt)\r\n        response = self.pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\r\n\r\n        # only return newly generated tokens\r\n        return response[prompt_length:]\r\n\r\n    @property\r\n    def _identifying_params(self) -> Mapping[str, Any]:\r\n        return {\"name_of_model\": self.model_name}\r\n\r\n    @property\r\n    def _llm_type(self) -> str:\r\n        return \"custom\"\r\n\r\n## Error\r\n\r\n   1879         # Checks if the model has been loaded in 8-bit\r\n   1880         if getattr(self, \"is_loaded_in_8bit\", False):\r\n-> 1881             raise ValueError(\r\n   1882                 \"`.to` is not supported for `8-bit` models. Please use the model as it is, since the\"\r\n   1883                 \" model has already been set to the correct devices and casted to the correct `dtype`.\"\r\n\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3262/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3262/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3261",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3261/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3261/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3261/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3261",
        "id": 1707096852,
        "node_id": "I_kwDOIWuq585lwDsU",
        "number": 3261,
        "title": "Hello I faced a small problem while using \"RetrieverQueryEngine\"",
        "user": {
            "login": "dglee-rbrain",
            "id": 132996108,
            "node_id": "U_kgDOB-1cDA",
            "avatar_url": "https://avatars.githubusercontent.com/u/132996108?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dglee-rbrain",
            "html_url": "https://github.com/dglee-rbrain",
            "followers_url": "https://api.github.com/users/dglee-rbrain/followers",
            "following_url": "https://api.github.com/users/dglee-rbrain/following{/other_user}",
            "gists_url": "https://api.github.com/users/dglee-rbrain/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dglee-rbrain/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dglee-rbrain/subscriptions",
            "organizations_url": "https://api.github.com/users/dglee-rbrain/orgs",
            "repos_url": "https://api.github.com/users/dglee-rbrain/repos",
            "events_url": "https://api.github.com/users/dglee-rbrain/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dglee-rbrain/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-12T07:22:23Z",
        "updated_at": "2023-05-15T18:18:24Z",
        "closed_at": "2023-05-15T18:18:17Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "So I was trying to get response from RetrieverQueryEngine like this\r\n\r\n**index = GPTVectorStoreIndex.from_documents(nodes, service_context=service_context)\r\nretriever = index.as_retriever()\r\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='tree_summarize')\r\nresponse = query_engine.query(\"long summary of the document\")\r\nprint(str(response))**\r\n\r\nAnd I do get the response just fine. The problem is somehow the response text seems to be limited to 255 tokens, so if it goes longer than that, it is cutted right in the middle of the sentence. I'd like to see it printing out longer than 255 tokens. Do you know how?\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3261/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3261/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3259",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3259/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3259/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3259/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3259",
        "id": 1706917792,
        "node_id": "PR_kwDOIWuq585QWb5w",
        "number": 3259,
        "title": "Fix a minor typo",
        "user": {
            "login": "ayulockin",
            "id": 31141479,
            "node_id": "MDQ6VXNlcjMxMTQxNDc5",
            "avatar_url": "https://avatars.githubusercontent.com/u/31141479?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ayulockin",
            "html_url": "https://github.com/ayulockin",
            "followers_url": "https://api.github.com/users/ayulockin/followers",
            "following_url": "https://api.github.com/users/ayulockin/following{/other_user}",
            "gists_url": "https://api.github.com/users/ayulockin/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ayulockin/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ayulockin/subscriptions",
            "organizations_url": "https://api.github.com/users/ayulockin/orgs",
            "repos_url": "https://api.github.com/users/ayulockin/repos",
            "events_url": "https://api.github.com/users/ayulockin/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ayulockin/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-12T04:46:50Z",
        "updated_at": "2023-05-12T06:11:10Z",
        "closed_at": "2023-05-12T06:11:10Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3259",
            "html_url": "https://github.com/run-llama/llama_index/pull/3259",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3259.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3259.patch",
            "merged_at": "2023-05-12T06:11:10Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3259/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3259/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3258",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3258/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3258/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3258/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3258",
        "id": 1706870867,
        "node_id": "I_kwDOIWuq585lvMhT",
        "number": 3258,
        "title": "GPTDocumentSummaryIndex query use Chinese has errors",
        "user": {
            "login": "wsf1990",
            "id": 8774884,
            "node_id": "MDQ6VXNlcjg3NzQ4ODQ=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8774884?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wsf1990",
            "html_url": "https://github.com/wsf1990",
            "followers_url": "https://api.github.com/users/wsf1990/followers",
            "following_url": "https://api.github.com/users/wsf1990/following{/other_user}",
            "gists_url": "https://api.github.com/users/wsf1990/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wsf1990/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wsf1990/subscriptions",
            "organizations_url": "https://api.github.com/users/wsf1990/orgs",
            "repos_url": "https://api.github.com/users/wsf1990/repos",
            "events_url": "https://api.github.com/users/wsf1990/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wsf1990/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5860091515,
                "node_id": "LA_kwDOIWuq588AAAABXUnmew",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/stale",
                "name": "stale",
                "color": "dadada",
                "default": false,
                "description": "Issue has not had recent activity or appears to be solved. Stale issues will be automatically closed"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 8,
        "created_at": "2023-05-12T03:46:20Z",
        "updated_at": "2023-12-13T16:02:43Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "The latest llama-index version is: 0.6.5\r\nWhen query use chinese it raise some errors,Such as:\r\n\r\n1. invalid literal for int() with base 10: '\u76f8\u5173\u5ea610'\r\n\r\n2. list index out of range\r\n```\r\nprint(retriever.retrieve('hi'))\r\n  File \"venv/lib/python3.10/site-packages/llama_index/indices/base_retriever.py\", line 21, in retrieve\r\n    return self._retrieve(str_or_query_bundle)\r\n  File \"venv/lib/python3.10/site-packages/llama_index/indices/document_summary/retrievers.py\", line 128, in _retrieve\r\n    raw_choices, relevances = self._parse_choice_select_answer_fn(\r\n  File \"venv/lib/python3.10/site-packages/llama_index/indices/document_summary/retrievers.py\", line 69, in default_parse_choice_select_answer_fn\r\n    answer_num = int(line_tokens[0].split(\":\")[1].strip())\r\nIndexError: list index out of range\r\n```\r\n\r\n3. integer division or modulo by zero\r\n\r\nDo I miss sth?\r\nThanks!",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3258/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3258/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3257",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3257/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3257/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3257/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3257",
        "id": 1706852026,
        "node_id": "I_kwDOIWuq585lvH66",
        "number": 3257,
        "title": "Athena as source Data",
        "user": {
            "login": "praysml",
            "id": 85272140,
            "node_id": "MDQ6VXNlcjg1MjcyMTQw",
            "avatar_url": "https://avatars.githubusercontent.com/u/85272140?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/praysml",
            "html_url": "https://github.com/praysml",
            "followers_url": "https://api.github.com/users/praysml/followers",
            "following_url": "https://api.github.com/users/praysml/following{/other_user}",
            "gists_url": "https://api.github.com/users/praysml/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/praysml/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/praysml/subscriptions",
            "organizations_url": "https://api.github.com/users/praysml/orgs",
            "repos_url": "https://api.github.com/users/praysml/repos",
            "events_url": "https://api.github.com/users/praysml/events{/privacy}",
            "received_events_url": "https://api.github.com/users/praysml/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-12T03:12:16Z",
        "updated_at": "2023-05-12T17:36:04Z",
        "closed_at": "2023-05-12T17:36:04Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hello, I want to ask if source data using Amazon Athena will be featured?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3257/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3257/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3256",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3256/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3256/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3256/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3256",
        "id": 1706783943,
        "node_id": "PR_kwDOIWuq585QV_4z",
        "number": 3256,
        "title": "Change RedisVectorStore index logic",
        "user": {
            "login": "Spartee",
            "id": 13009163,
            "node_id": "MDQ6VXNlcjEzMDA5MTYz",
            "avatar_url": "https://avatars.githubusercontent.com/u/13009163?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Spartee",
            "html_url": "https://github.com/Spartee",
            "followers_url": "https://api.github.com/users/Spartee/followers",
            "following_url": "https://api.github.com/users/Spartee/following{/other_user}",
            "gists_url": "https://api.github.com/users/Spartee/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Spartee/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Spartee/subscriptions",
            "organizations_url": "https://api.github.com/users/Spartee/orgs",
            "repos_url": "https://api.github.com/users/Spartee/repos",
            "events_url": "https://api.github.com/users/Spartee/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Spartee/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-12T01:25:41Z",
        "updated_at": "2023-05-12T06:04:38Z",
        "closed_at": "2023-05-12T06:04:38Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3256",
            "html_url": "https://github.com/run-llama/llama_index/pull/3256",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3256.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3256.patch",
            "merged_at": "2023-05-12T06:04:38Z"
        },
        "body": "This PR changes the index creation/deletion logic within ``RedisVectorStore`` and address a subtle bug\r\n\r\nThe general change is allowing users to add to existing indices when they don't specify overwrite. This use case came up when needing to continually add batches of documents to a redis store through an HTTPS route. the logic now allows for\r\n\r\n1. if no index is created by that name, one is created and documents are added to that index\r\n2. If an index exists, but overwrite is false, documents are added to that index if by the same index name and prefix\r\n3. if an index exists, and overwrite is true, the index is deleted, created, and documents are added.\r\n\r\nBug:\r\nPassing an empty set of documents resulted in a IndexError. This should simply return as no documents were added.\r\n\r\nGeneral error handling and cleanup was added as well.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3256/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3256/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3255",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3255/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3255/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3255/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3255",
        "id": 1706705150,
        "node_id": "I_kwDOIWuq585lukD-",
        "number": 3255,
        "title": "Is it possible to retrieve a list of indexed documents (paths and/or title/metadata), and the vector store itself, from a GPTVectorStoreIndex?",
        "user": {
            "login": "maspotts",
            "id": 4096446,
            "node_id": "MDQ6VXNlcjQwOTY0NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4096446?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/maspotts",
            "html_url": "https://github.com/maspotts",
            "followers_url": "https://api.github.com/users/maspotts/followers",
            "following_url": "https://api.github.com/users/maspotts/following{/other_user}",
            "gists_url": "https://api.github.com/users/maspotts/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/maspotts/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/maspotts/subscriptions",
            "organizations_url": "https://api.github.com/users/maspotts/orgs",
            "repos_url": "https://api.github.com/users/maspotts/repos",
            "events_url": "https://api.github.com/users/maspotts/events{/privacy}",
            "received_events_url": "https://api.github.com/users/maspotts/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 10,
        "created_at": "2023-05-11T23:38:40Z",
        "updated_at": "2023-11-01T18:18:09Z",
        "closed_at": "2023-06-11T06:47:14Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi: I mistakenly asked https://github.com/jerryjliu/llama_index/issues/3249 but have now realised the correct question is: is there a way to query my GPTVectorStoreIndex in order to retrieve the list of documents that were originally indexed?  And also their text snippets?  And also to fetch the vectors their snippets mapped to? ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3255/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3255/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3254",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3254/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3254/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3254/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3254",
        "id": 1706594836,
        "node_id": "I_kwDOIWuq585luJIU",
        "number": 3254,
        "title": "Install documentation appears to be incomplete",
        "user": {
            "login": "normanlove222",
            "id": 2139671,
            "node_id": "MDQ6VXNlcjIxMzk2NzE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2139671?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/normanlove222",
            "html_url": "https://github.com/normanlove222",
            "followers_url": "https://api.github.com/users/normanlove222/followers",
            "following_url": "https://api.github.com/users/normanlove222/following{/other_user}",
            "gists_url": "https://api.github.com/users/normanlove222/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/normanlove222/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/normanlove222/subscriptions",
            "organizations_url": "https://api.github.com/users/normanlove222/orgs",
            "repos_url": "https://api.github.com/users/normanlove222/repos",
            "events_url": "https://api.github.com/users/normanlove222/events{/privacy}",
            "received_events_url": "https://api.github.com/users/normanlove222/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-11T21:39:25Z",
        "updated_at": "2023-05-11T22:35:42Z",
        "closed_at": "2023-05-11T22:35:42Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I have Llama index installed on my desktop for windows and I have code from about 30 days ago that is working fine on desktop.\r\n\r\nThe issue is im now on my laptop which is linux and trying to recreate a development enviornment and im unable to install llama index. I have followed the latest instructions at https://gpt-index.readthedocs.io/en/latest/getting_started/installation.html, but these appear to be inaccurate or incomplete cause even though Ive followed them to the T, Im unable to properly install llama index. key word being properly. Its installed but my python script cant find the location.\r\n\r\nsome obvious issues with install page is it says:\r\ngit clone git@github.com:jerryjliu/llama_index.git and terminal does not like that and suggests using https at beginning , which then works. \r\n\r\nanother issue is the install page then suggests you do pip install -r requirements.txt. which would not work unless you actually change to the directory you installed lamma_index to. this is not as obvious as the developers may feel as installation instructions are supposed to include ALL the steps neccessary to properly install the project.\r\n\r\nEven with figuring all this out, iI still get unknown location for llamada index when I run my index.py sample code. \r\n\r\nMay I sugest that someone actually create new instructions that will properly install this project on BOTH windows and Linux. or state , hey, this is just for windows etc. \r\n\r\nThis is especially important when your code is changing as rapidly as yours is. old code is no longer working after 30 days. \r\n\r\nSo in summary after following the install page instructions I get:\r\n\r\n`python3.10 index.py\r\nTraceback (most recent call last):\r\n  File \"/var/www/html/Text/index.py\", line 5, in <module>\r\n    from llama_index import GPTVectorStoreIndex, download_loader\r\nImportError: cannot import name 'GPTVectorStoreIndex' from 'llama_index' (unknown location)\r\n\r\n`\r\nfor running this code:\r\n\r\n`#current working module that indexes knowledgebase via llamaindex and then prompts via terminal only\r\nimport os\r\nimport configparser\r\nimport logging\r\nfrom llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\r\n\r\n# Configure logging settings for llama_index to suppress unwanted messages\r\nllama_index_logger = logging.getLogger('llama_index')\r\nllama_index_logger.setLevel(logging.ERROR)\r\n\r\nconfig = configparser.ConfigParser()\r\nconfig.read('config.ini')\r\n\r\napi_key = config.get('openai', 'api_key')\r\nos.environ['OPENAI_API_KEY'] = api_key\r\n\r\n\r\ndocuments = SimpleDirectoryReader('data').load_data()\r\nindex = GPTVectorStoreIndex.from_documents(documents)\r\n\r\n# save to disk\r\n# index.save_to_disk('index.json')\r\n# # load from disk\r\n# index = GPTSimpleVectorIndex.load_from_disk('index.json')\r\n\r\nwhile True:\r\n   # Get input question from user\r\n    question = input(\"Ask a question: \")\r\n\r\n    # Use llama index to search for the answer\r\n    response = index.query(question)\r\n\r\n    # Print the answer\r\n    \r\n    print(response)\r\n`",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3254/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3254/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3253",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3253/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3253/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3253/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3253",
        "id": 1706399273,
        "node_id": "PR_kwDOIWuq585QUsUO",
        "number": 3253,
        "title": "Allow Empty Vector Indexes",
        "user": {
            "login": "logan-markewich",
            "id": 22285038,
            "node_id": "MDQ6VXNlcjIyMjg1MDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/22285038?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logan-markewich",
            "html_url": "https://github.com/logan-markewich",
            "followers_url": "https://api.github.com/users/logan-markewich/followers",
            "following_url": "https://api.github.com/users/logan-markewich/following{/other_user}",
            "gists_url": "https://api.github.com/users/logan-markewich/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logan-markewich/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logan-markewich/subscriptions",
            "organizations_url": "https://api.github.com/users/logan-markewich/orgs",
            "repos_url": "https://api.github.com/users/logan-markewich/repos",
            "events_url": "https://api.github.com/users/logan-markewich/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logan-markewich/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-11T19:13:18Z",
        "updated_at": "2023-11-14T03:38:42Z",
        "closed_at": "2023-05-12T00:54:32Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3253",
            "html_url": "https://github.com/run-llama/llama_index/pull/3253",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3253.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3253.patch",
            "merged_at": "2023-05-12T00:54:32Z"
        },
        "body": "Some vector indexes did not allow users to create empty indexes:\r\n\r\n`index = GPTVectorStoreIndex([], storage_context=storage_context)`\r\n\r\nThis would hit an error in the `add()` function of most vector indexes\r\n\r\nThis PR adds extra checking to avoid errors for empty indexes",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3253/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3253/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3252",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3252/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3252/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3252/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3252",
        "id": 1706239868,
        "node_id": "I_kwDOIWuq585lsyd8",
        "number": 3252,
        "title": "Issue with a limit output with chatgpt bot",
        "user": {
            "login": "qwzxqwzx",
            "id": 47888379,
            "node_id": "MDQ6VXNlcjQ3ODg4Mzc5",
            "avatar_url": "https://avatars.githubusercontent.com/u/47888379?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/qwzxqwzx",
            "html_url": "https://github.com/qwzxqwzx",
            "followers_url": "https://api.github.com/users/qwzxqwzx/followers",
            "following_url": "https://api.github.com/users/qwzxqwzx/following{/other_user}",
            "gists_url": "https://api.github.com/users/qwzxqwzx/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/qwzxqwzx/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/qwzxqwzx/subscriptions",
            "organizations_url": "https://api.github.com/users/qwzxqwzx/orgs",
            "repos_url": "https://api.github.com/users/qwzxqwzx/repos",
            "events_url": "https://api.github.com/users/qwzxqwzx/events{/privacy}",
            "received_events_url": "https://api.github.com/users/qwzxqwzx/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-05-11T17:19:46Z",
        "updated_at": "2023-09-10T16:59:29Z",
        "closed_at": "2023-09-10T16:59:29Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I have this script that creates a chatbot. The problem is it gives me very limited outputs as an answer is there anything in the script that I can change so I can get the maximum output possible?\r\nCan anyone help me with this issue I would really appreciate it.?\r\n`\r\nfrom gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\r\nfrom langchain.chat_models import ChatOpenAI\r\nimport gradio as gr\r\nimport sys\r\nimport os\r\nimport keyring\r\nimport openai\r\n\r\n\r\n\r\napi_key = keyring.get_password('OPENAI', 'bbb')\r\n\r\nos.environ[\"OPENAI_API_KEY\"] = api_key\r\n\r\ndef construct_index(directory_path):\r\n    max_input_size = 4096\r\n    num_outputs = 4000\r\n    max_chunk_overlap = 20\r\n    chunk_size_limit = 600\r\n\r\n    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\r\n\r\n\r\n\r\n    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\r\n\r\n    documents = SimpleDirectoryReader(directory_path).load_data()\r\n\r\n    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\r\n\r\n    index.save_to_disk('index.json')\r\n\r\n    return index\r\n\r\ndef chatbot(input_text):\r\n    index = GPTSimpleVectorIndex.load_from_disk('index.json')\r\n    response = index.query(input_text, response_mode=\"default\")\r\n    return response.response\r\n\r\niface = gr.Interface(fn=chatbot,\r\n                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\r\n                     outputs=\"text\",\r\n                     title=\"Custom-trained AI Chatbot\")\r\n\r\nindex = construct_index(\"docs\")\r\niface.launch(share=True)`",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3252/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3252/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3251",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3251/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3251/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3251/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3251",
        "id": 1706047522,
        "node_id": "I_kwDOIWuq585lsDgi",
        "number": 3251,
        "title": "Customized text chunking",
        "user": {
            "login": "sid8491",
            "id": 8565062,
            "node_id": "MDQ6VXNlcjg1NjUwNjI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8565062?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sid8491",
            "html_url": "https://github.com/sid8491",
            "followers_url": "https://api.github.com/users/sid8491/followers",
            "following_url": "https://api.github.com/users/sid8491/following{/other_user}",
            "gists_url": "https://api.github.com/users/sid8491/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sid8491/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sid8491/subscriptions",
            "organizations_url": "https://api.github.com/users/sid8491/orgs",
            "repos_url": "https://api.github.com/users/sid8491/repos",
            "events_url": "https://api.github.com/users/sid8491/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sid8491/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-05-11T15:24:53Z",
        "updated_at": "2023-07-22T19:03:07Z",
        "closed_at": "2023-07-22T19:03:07Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "How do I write my own logic for text chunking?\r\nWhich classes do I need to extend, and how do I return the final chunking output?\r\n\r\nAny examples/documentation would be appreciated.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3251/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3251/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3250",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3250/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3250/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3250/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3250",
        "id": 1706043410,
        "node_id": "I_kwDOIWuq585lsCgS",
        "number": 3250,
        "title": "Is there a way to not load documents in memory in order to be able to query an index? ",
        "user": {
            "login": "rfarahmand",
            "id": 1120862,
            "node_id": "MDQ6VXNlcjExMjA4NjI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1120862?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/rfarahmand",
            "html_url": "https://github.com/rfarahmand",
            "followers_url": "https://api.github.com/users/rfarahmand/followers",
            "following_url": "https://api.github.com/users/rfarahmand/following{/other_user}",
            "gists_url": "https://api.github.com/users/rfarahmand/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/rfarahmand/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/rfarahmand/subscriptions",
            "organizations_url": "https://api.github.com/users/rfarahmand/orgs",
            "repos_url": "https://api.github.com/users/rfarahmand/repos",
            "events_url": "https://api.github.com/users/rfarahmand/events{/privacy}",
            "received_events_url": "https://api.github.com/users/rfarahmand/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-11T15:22:17Z",
        "updated_at": "2023-05-12T18:39:46Z",
        "closed_at": "2023-05-12T18:39:08Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hello,\r\nI am not sure if I understand the flow of the following program correctly, but suppose I have indexed thousands or millions of documents along with their embeddings in a VectorStore , Say an OpenSearchVectorStore. Why still do I still need to load the documents in the memory in order to to be able to access or query the index? I am thinking that in one call we do a semantic search to the OpenSearchVectorStore using a cosine similarity to get the right documents and then pass one or two of them as the context to the OpenAPI prompt as the subsequent call to do summarization or any other instructions we pass to the OpenAI. This is my program: \r\n```[python]\r\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\r\nclient = OpensearchVectorClient(endpoint, idx, 1536, embedding_field=embedding_field, text_field=text_field,auth=auth)\r\nindex = GPTOpensearchIndex.from_documents(documents=documents, client=client)\r\nresponse = index.query(\"What did the author do growing up?\")\r\n```\r\nwhat I intended to do is to ingest millions of document, get embedding for them, and store them in a vector store. Then in a separate program that is supposed to only query the index, get the relevant documents so that I can make subsequent calls to OpenAI models with prompts that has the relevant document as the context so that  I don't have to load the documents in the memory but just connect to the vector store? is there a way to access the index without having documents loaded in memory but rather only have them in the vector store? In other words, is there a way to get the `GPTOpensearchIndex` without passing the documents and only the `client`?\r\n\r\nAlternatively, I suppose I can pass the cosine similarity script query myself? like \r\n```\r\nrdr = ElasticsearchReader(endpoint, idx,httpx_client_args ={\"verify\":False,\"auth\":(\"admin\",\"admin\")})\r\nrdr.load_data(embedding_field,{\r\n \"size\": 4,\r\n \"query\": {\r\n   \"script_score\": {\r\n     \"query\": {\r\n       \"match_all\": {}\r\n     },\r\n     \"script\": {\r\n       \"source\": \"knn_score\",\r\n       \"lang\": \"knn\",\r\n       \"params\": {\r\n         \"field\": embedding_field,\r\n         \"query_value\": [vector for the question goes here],\r\n         \"space_type\": \"cosinesimil\"\r\n       }\r\n     }\r\n   }\r\n }\r\n}\r\n)\r\n```\r\n is that correct?\r\n\r\nThanks for your help?\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3250/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3250/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3249",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3249/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3249/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3249/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3249",
        "id": 1705992069,
        "node_id": "I_kwDOIWuq585lr1-F",
        "number": 3249,
        "title": "what happened to index.doc_previews (how should I list indexed documents?)",
        "user": {
            "login": "maspotts",
            "id": 4096446,
            "node_id": "MDQ6VXNlcjQwOTY0NDY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4096446?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/maspotts",
            "html_url": "https://github.com/maspotts",
            "followers_url": "https://api.github.com/users/maspotts/followers",
            "following_url": "https://api.github.com/users/maspotts/following{/other_user}",
            "gists_url": "https://api.github.com/users/maspotts/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/maspotts/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/maspotts/subscriptions",
            "organizations_url": "https://api.github.com/users/maspotts/orgs",
            "repos_url": "https://api.github.com/users/maspotts/repos",
            "events_url": "https://api.github.com/users/maspotts/events{/privacy}",
            "received_events_url": "https://api.github.com/users/maspotts/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-11T14:55:57Z",
        "updated_at": "2023-05-11T23:48:36Z",
        "closed_at": "2023-05-11T23:48:36Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi: I was using `index.doc_previews` to fetch a list of indexed documents; then in an upgrade I had to convert to `index.doc_previews()`, but now in the latest upgrade I can't find any reference to `doc_previews` (I did find `index.docstore.get_nodes()` but that needs a list of node ids).  What's the recommended method for listing the indexed documents now?  Thanks!",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3249/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3249/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3248",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3248/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3248/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3248/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3248",
        "id": 1705910086,
        "node_id": "PR_kwDOIWuq585QTC9C",
        "number": 3248,
        "title": "Adding kwargs to OpenAI get_embedding calls",
        "user": {
            "login": "roh26it",
            "id": 971978,
            "node_id": "MDQ6VXNlcjk3MTk3OA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/971978?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/roh26it",
            "html_url": "https://github.com/roh26it",
            "followers_url": "https://api.github.com/users/roh26it/followers",
            "following_url": "https://api.github.com/users/roh26it/following{/other_user}",
            "gists_url": "https://api.github.com/users/roh26it/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/roh26it/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/roh26it/subscriptions",
            "organizations_url": "https://api.github.com/users/roh26it/orgs",
            "repos_url": "https://api.github.com/users/roh26it/repos",
            "events_url": "https://api.github.com/users/roh26it/events{/privacy}",
            "received_events_url": "https://api.github.com/users/roh26it/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-11T14:15:55Z",
        "updated_at": "2023-05-15T19:25:20Z",
        "closed_at": "2023-05-15T19:25:20Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3248",
            "html_url": "https://github.com/run-llama/llama_index/pull/3248",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3248.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3248.patch",
            "merged_at": "2023-05-15T19:25:20Z"
        },
        "body": "Sorry, I'm new to open source and I accidentally closed [my last PR](https://github.com/jerryjliu/llama_index/pull/2527)\r\n\r\n### Why is this needed?\r\n* OpenAI now supports another parameter (user) while generating embeddings and might support more in the future.\r\n* Passing headers to the Embeddings API endpoints is also limited in the current implementation\r\n\r\n### What is included in this commit?\r\n* Added kwargs to the BaseEmbedding and OpenAIEmbedding classes and functions to support extra parameters to be passed as needed.\r\n\r\nI've reverted the changes discussed in the last PR. We still need to add kwargs to the base embeddings class as well, I felt removing everything from kwargs in the OpenAI class might be more messy.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3248/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3248/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3247",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3247/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3247/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3247/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3247",
        "id": 1705698091,
        "node_id": "I_kwDOIWuq585lquMr",
        "number": 3247,
        "title": "Streaming error in version 0.6.5",
        "user": {
            "login": "willhamlam",
            "id": 2692086,
            "node_id": "MDQ6VXNlcjI2OTIwODY=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2692086?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/willhamlam",
            "html_url": "https://github.com/willhamlam",
            "followers_url": "https://api.github.com/users/willhamlam/followers",
            "following_url": "https://api.github.com/users/willhamlam/following{/other_user}",
            "gists_url": "https://api.github.com/users/willhamlam/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/willhamlam/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/willhamlam/subscriptions",
            "organizations_url": "https://api.github.com/users/willhamlam/orgs",
            "repos_url": "https://api.github.com/users/willhamlam/repos",
            "events_url": "https://api.github.com/users/willhamlam/events{/privacy}",
            "received_events_url": "https://api.github.com/users/willhamlam/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2023-05-11T12:08:15Z",
        "updated_at": "2023-07-12T06:35:49Z",
        "closed_at": "2023-06-07T16:33:47Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Following the [guide](https://gpt-index.readthedocs.io/en/latest/how_to/customization/streaming.html) but still got the error below: (both 'gpt-3.5-turbo' and 'text-davinci-003' models)\r\n`ValueError: LLM must support streaming and set streaming=True.`\r\n\r\nHere is my code:\r\n```\r\nfrom llama_index import SimpleDirectoryReader\r\n\r\ndocuments = SimpleDirectoryReader('data').load_data()\r\n```\r\n```\r\nfrom llama_index import LLMPredictor, PromptHelper, ServiceContext\r\nfrom langchain import OpenAI\r\n\r\nllm = OpenAI(\r\n  temperature=0, \r\n  model_name=\"gpt-3.5-turbo\",\r\n  frequency_penalty = 0.5,\r\n  max_tokens=2048,\r\n  streaming=True,\r\n)\r\nllm_predictor = LLMPredictor(llm=llm)\r\n\r\nprompt_helper = PromptHelper(\r\n    max_input_size = 4096, \r\n    num_output = 2048, \r\n    max_chunk_overlap = 20, \r\n    chunk_size_limit = 700\r\n)\r\n\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\r\n```\r\n```\r\nfrom llama_index import GPTVectorStoreIndex\r\n\r\nindex = GPTVectorStoreIndex.from_documents(\r\n    documents, service_context=service_context\r\n)\r\n```\r\n```\r\nfrom llama_index.retrievers import VectorIndexRetriever\r\nfrom llama_index.query_engine import RetrieverQueryEngine\r\nfrom llama_index import ResponseSynthesizer\r\n\r\n# configure retriever\r\nretriever = VectorIndexRetriever(\r\n    index=index,\r\n    streaming=True,\r\n    similarity_top_k=3,\r\n)\r\n```\r\n```\r\n# define custom QuestionAnswerPrompt\r\nfrom llama_index import QuestionAnswerPrompt\r\n\r\nquery_str = \"XXXX\"\r\nQA_PROMPT_TMPL = (\r\n    \"We have provided context information below. \\n\"\r\n    \"---------------------\\n\"\r\n    \"{context_str}\"\r\n    \"\\n---------------------\\n\"\r\n    \"Given all this information, please answer the following questions,\"\r\n    \"You MUST use the SAME language as the question:\\n\"\r\n    \"{query_str}\\n\"\r\n)\r\nQA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\r\n```\r\n```\r\nresponse_synthesizer = ResponseSynthesizer.from_args(\r\n    streaming=True,\r\n    text_qa_template=QA_PROMPT\r\n)\r\n\r\nquery_engine = RetrieverQueryEngine(\r\n    retriever=retriever,\r\n    response_synthesizer=response_synthesizer\r\n)\r\nresponse_stream = query_engine.query(query_str)\r\nresponse_stream.print_response_stream()\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3247/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3247/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3246",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3246/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3246/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3246/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3246",
        "id": 1705618978,
        "node_id": "I_kwDOIWuq585lqa4i",
        "number": 3246,
        "title": "Indexing Time Logging",
        "user": {
            "login": "bradley-pearson6597",
            "id": 44512815,
            "node_id": "MDQ6VXNlcjQ0NTEyODE1",
            "avatar_url": "https://avatars.githubusercontent.com/u/44512815?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bradley-pearson6597",
            "html_url": "https://github.com/bradley-pearson6597",
            "followers_url": "https://api.github.com/users/bradley-pearson6597/followers",
            "following_url": "https://api.github.com/users/bradley-pearson6597/following{/other_user}",
            "gists_url": "https://api.github.com/users/bradley-pearson6597/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/bradley-pearson6597/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/bradley-pearson6597/subscriptions",
            "organizations_url": "https://api.github.com/users/bradley-pearson6597/orgs",
            "repos_url": "https://api.github.com/users/bradley-pearson6597/repos",
            "events_url": "https://api.github.com/users/bradley-pearson6597/events{/privacy}",
            "received_events_url": "https://api.github.com/users/bradley-pearson6597/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-11T11:20:51Z",
        "updated_at": "2023-07-22T19:04:02Z",
        "closed_at": "2023-07-22T19:04:02Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "It would be useful if there was an indication of how many documents have been indexed so far/how long the rest of the indexing process will take. I am indexing 2000+ documents and I have no idea how long it will take. \r\n\r\nIf this already exists I look forward to hearing how to do it! ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3246/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3246/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3245",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3245/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3245/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3245/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3245",
        "id": 1705611643,
        "node_id": "PR_kwDOIWuq585QSDAo",
        "number": 3245,
        "title": "Takahiro update dockerfile",
        "user": {
            "login": "takahirosir",
            "id": 98999068,
            "node_id": "U_kgDOBeabHA",
            "avatar_url": "https://avatars.githubusercontent.com/u/98999068?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/takahirosir",
            "html_url": "https://github.com/takahirosir",
            "followers_url": "https://api.github.com/users/takahirosir/followers",
            "following_url": "https://api.github.com/users/takahirosir/following{/other_user}",
            "gists_url": "https://api.github.com/users/takahirosir/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/takahirosir/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/takahirosir/subscriptions",
            "organizations_url": "https://api.github.com/users/takahirosir/orgs",
            "repos_url": "https://api.github.com/users/takahirosir/repos",
            "events_url": "https://api.github.com/users/takahirosir/events{/privacy}",
            "received_events_url": "https://api.github.com/users/takahirosir/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-11T11:17:30Z",
        "updated_at": "2023-05-11T11:18:20Z",
        "closed_at": "2023-05-11T11:18:20Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3245",
            "html_url": "https://github.com/run-llama/llama_index/pull/3245",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3245.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3245.patch",
            "merged_at": null
        },
        "body": "add a new ling in dockerfile && rewrite test.py in example",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3245/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3245/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3244",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3244/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3244/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3244/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3244",
        "id": 1705565863,
        "node_id": "PR_kwDOIWuq585QR5Gz",
        "number": 3244,
        "title": "Init dockefile with line of example",
        "user": {
            "login": "Quartzing",
            "id": 16685937,
            "node_id": "MDQ6VXNlcjE2Njg1OTM3",
            "avatar_url": "https://avatars.githubusercontent.com/u/16685937?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Quartzing",
            "html_url": "https://github.com/Quartzing",
            "followers_url": "https://api.github.com/users/Quartzing/followers",
            "following_url": "https://api.github.com/users/Quartzing/following{/other_user}",
            "gists_url": "https://api.github.com/users/Quartzing/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Quartzing/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Quartzing/subscriptions",
            "organizations_url": "https://api.github.com/users/Quartzing/orgs",
            "repos_url": "https://api.github.com/users/Quartzing/repos",
            "events_url": "https://api.github.com/users/Quartzing/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Quartzing/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-11T10:52:52Z",
        "updated_at": "2023-05-11T10:55:52Z",
        "closed_at": "2023-05-11T10:55:52Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3244",
            "html_url": "https://github.com/run-llama/llama_index/pull/3244",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3244.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3244.patch",
            "merged_at": null
        },
        "body": "Add an example line in the dockerfile.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3244/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3244/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3243",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3243/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3243/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3243/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3243",
        "id": 1705520887,
        "node_id": "I_kwDOIWuq585lqC73",
        "number": 3243,
        "title": "load data from dictionary in llamaindex",
        "user": {
            "login": "javixeneize",
            "id": 12761574,
            "node_id": "MDQ6VXNlcjEyNzYxNTc0",
            "avatar_url": "https://avatars.githubusercontent.com/u/12761574?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/javixeneize",
            "html_url": "https://github.com/javixeneize",
            "followers_url": "https://api.github.com/users/javixeneize/followers",
            "following_url": "https://api.github.com/users/javixeneize/following{/other_user}",
            "gists_url": "https://api.github.com/users/javixeneize/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/javixeneize/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/javixeneize/subscriptions",
            "organizations_url": "https://api.github.com/users/javixeneize/orgs",
            "repos_url": "https://api.github.com/users/javixeneize/repos",
            "events_url": "https://api.github.com/users/javixeneize/events{/privacy}",
            "received_events_url": "https://api.github.com/users/javixeneize/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-11T10:21:54Z",
        "updated_at": "2023-07-22T19:04:41Z",
        "closed_at": "2023-07-22T19:04:41Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi\r\n\r\nHow can i load data from a dictionary in llamaindex? I have seen all the examples loading data from a file, but cant see how to load from a dictionary, and load every item as an individual document",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3243/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3243/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3242",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3242/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3242/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3242/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3242",
        "id": 1705437448,
        "node_id": "I_kwDOIWuq585lpukI",
        "number": 3242,
        "title": "Token indices sequence length is longer than the specified maximum sequence length for this model (2503 > 1024). Running this sequence through the model will result in indexing errors?",
        "user": {
            "login": "Bruce337f",
            "id": 127927670,
            "node_id": "U_kgDOB6AFdg",
            "avatar_url": "https://avatars.githubusercontent.com/u/127927670?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Bruce337f",
            "html_url": "https://github.com/Bruce337f",
            "followers_url": "https://api.github.com/users/Bruce337f/followers",
            "following_url": "https://api.github.com/users/Bruce337f/following{/other_user}",
            "gists_url": "https://api.github.com/users/Bruce337f/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Bruce337f/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Bruce337f/subscriptions",
            "organizations_url": "https://api.github.com/users/Bruce337f/orgs",
            "repos_url": "https://api.github.com/users/Bruce337f/repos",
            "events_url": "https://api.github.com/users/Bruce337f/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Bruce337f/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-05-11T09:33:01Z",
        "updated_at": "2023-09-16T16:17:49Z",
        "closed_at": "2023-09-16T16:17:48Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "[https://github.com/jerryjliu/llama_index/issues/987#issuecomment-1493259768](url)\r\n\r\nI try to use the method here, but it doesn't work, is it because the embedding ada model only supports 1024 maximum tokens?\r\n\r\n# NOTE: set a chunk size limit to < 1024 tokens \r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\r\nindex = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3242/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3242/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3241",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3241/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3241/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3241/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3241",
        "id": 1705422901,
        "node_id": "I_kwDOIWuq585lprA1",
        "number": 3241,
        "title": "Querying GPTPandasIndex is failing when requires to perform mathematical calculations on data",
        "user": {
            "login": "ghane100",
            "id": 86785872,
            "node_id": "MDQ6VXNlcjg2Nzg1ODcy",
            "avatar_url": "https://avatars.githubusercontent.com/u/86785872?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ghane100",
            "html_url": "https://github.com/ghane100",
            "followers_url": "https://api.github.com/users/ghane100/followers",
            "following_url": "https://api.github.com/users/ghane100/following{/other_user}",
            "gists_url": "https://api.github.com/users/ghane100/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ghane100/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ghane100/subscriptions",
            "organizations_url": "https://api.github.com/users/ghane100/orgs",
            "repos_url": "https://api.github.com/users/ghane100/repos",
            "events_url": "https://api.github.com/users/ghane100/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ghane100/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-05-11T09:24:30Z",
        "updated_at": "2023-12-13T16:01:47Z",
        "closed_at": "2023-12-13T16:01:46Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Using GPTPandasIndex I created an index on a dataset (pandas drataframes) which I loaded from a csv file. By initializing query engine on this index, I am asking a query that requires some mathematical calculations to do (internally) on DOB/Age column value and the program is failing with below error (in file pandas_query.py) :\r\n\r\n----------------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\<User>\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\llama_index\\indices\\struct_store\\pandas_query.py\", line 58, in default_output_processor\r\n    raise e\r\n  File \"c:\\Users\\<User>\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\llama_index\\indices\\struct_store\\pandas_query.py\", line 56, in default_output_processor\r\n    return str(eval(module_end_str, {}, local_vars))\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<string>\", line 1, in <module>\r\nNameError: name 'pd' is not defined\r\n\r\n----------------------------------------------------------------------------\r\n\r\n**Note:** This issue does exist from version in which \"GPTPandasIndex\" is introduced.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3241/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3241/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3240",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3240/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3240/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3240/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3240",
        "id": 1705421286,
        "node_id": "I_kwDOIWuq585lpqnm",
        "number": 3240,
        "title": " cannot import name 'load_index_from_storage' from 'llama_index'",
        "user": {
            "login": "Crowdparti",
            "id": 22471367,
            "node_id": "MDQ6VXNlcjIyNDcxMzY3",
            "avatar_url": "https://avatars.githubusercontent.com/u/22471367?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Crowdparti",
            "html_url": "https://github.com/Crowdparti",
            "followers_url": "https://api.github.com/users/Crowdparti/followers",
            "following_url": "https://api.github.com/users/Crowdparti/following{/other_user}",
            "gists_url": "https://api.github.com/users/Crowdparti/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Crowdparti/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Crowdparti/subscriptions",
            "organizations_url": "https://api.github.com/users/Crowdparti/orgs",
            "repos_url": "https://api.github.com/users/Crowdparti/repos",
            "events_url": "https://api.github.com/users/Crowdparti/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Crowdparti/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-11T09:23:33Z",
        "updated_at": "2023-05-12T03:28:40Z",
        "closed_at": "2023-05-12T03:28:40Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Please is there anyone who can advise me? \r\nI met this error when I tried to run the sample code in the git of Llamaindex 0.6.5\r\nplease, refer to this link: https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/SimpleIndexDemo.html#load-documents-build-the-gptvectorstoreindex. ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3240/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3240/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3239",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3239/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3239/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3239/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3239",
        "id": 1705169318,
        "node_id": "PR_kwDOIWuq585QQjvu",
        "number": 3239,
        "title": "Oldversion",
        "user": {
            "login": "jensbech",
            "id": 8881797,
            "node_id": "MDQ6VXNlcjg4ODE3OTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8881797?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jensbech",
            "html_url": "https://github.com/jensbech",
            "followers_url": "https://api.github.com/users/jensbech/followers",
            "following_url": "https://api.github.com/users/jensbech/following{/other_user}",
            "gists_url": "https://api.github.com/users/jensbech/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jensbech/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jensbech/subscriptions",
            "organizations_url": "https://api.github.com/users/jensbech/orgs",
            "repos_url": "https://api.github.com/users/jensbech/repos",
            "events_url": "https://api.github.com/users/jensbech/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jensbech/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-11T06:41:24Z",
        "updated_at": "2023-05-11T06:42:01Z",
        "closed_at": "2023-05-11T06:41:58Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3239",
            "html_url": "https://github.com/run-llama/llama_index/pull/3239",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3239.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3239.patch",
            "merged_at": null
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3239/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3239/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3236",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3236/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3236/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3236/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3236",
        "id": 1705068929,
        "node_id": "PR_kwDOIWuq585QQOKj",
        "number": 3236,
        "title": "VectorIndexAutoRetriever",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-11T05:00:54Z",
        "updated_at": "2023-05-12T22:13:29Z",
        "closed_at": "2023-05-12T22:13:28Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3236",
            "html_url": "https://github.com/run-llama/llama_index/pull/3236",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3236.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3236.patch",
            "merged_at": "2023-05-12T22:13:28Z"
        },
        "body": "### Summary\r\n* Introduce `VectorIndexAutoRetriever`, which use LLM to set vector store query `VectorStoreQuerySpec`\r\n* Right now supports exact metadata filter, and top_k params\r\n\r\n\r\n### Future work\r\n* More advanced metadata filtering (AND/OR, comparators)\r\n* Support query without query string",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3236/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3236/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3221",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3221/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3221/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3221/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3221",
        "id": 1704965156,
        "node_id": "I_kwDOIWuq585ln7Qk",
        "number": 3221,
        "title": "[Feature Request] Enable Streaming for Azure openai ChatGPT Models",
        "user": {
            "login": "Bruce337f",
            "id": 127927670,
            "node_id": "U_kgDOB6AFdg",
            "avatar_url": "https://avatars.githubusercontent.com/u/127927670?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Bruce337f",
            "html_url": "https://github.com/Bruce337f",
            "followers_url": "https://api.github.com/users/Bruce337f/followers",
            "following_url": "https://api.github.com/users/Bruce337f/following{/other_user}",
            "gists_url": "https://api.github.com/users/Bruce337f/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Bruce337f/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Bruce337f/subscriptions",
            "organizations_url": "https://api.github.com/users/Bruce337f/orgs",
            "repos_url": "https://api.github.com/users/Bruce337f/repos",
            "events_url": "https://api.github.com/users/Bruce337f/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Bruce337f/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-11T02:56:14Z",
        "updated_at": "2023-06-06T05:08:04Z",
        "closed_at": "2023-06-06T05:08:04Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Langchain does have support, but it seems the mechanism (via callbacks) is different from the current implementation.\r\n\r\nSupport for this would go a long way in reducing costs while maintaining that low perceived latency.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3221/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3221/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3219",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3219/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3219/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3219/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3219",
        "id": 1704779772,
        "node_id": "PR_kwDOIWuq585QPO5m",
        "number": 3219,
        "title": "[version] bump version to 0.6.5",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-11T00:19:04Z",
        "updated_at": "2023-05-11T00:36:11Z",
        "closed_at": "2023-05-11T00:36:10Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3219",
            "html_url": "https://github.com/run-llama/llama_index/pull/3219",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3219.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3219.patch",
            "merged_at": "2023-05-11T00:36:10Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3219/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3219/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3218",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3218/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3218/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3218/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3218",
        "id": 1704779177,
        "node_id": "PR_kwDOIWuq585QPOxi",
        "number": 3218,
        "title": "fix streaming notebooks",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-11T00:18:01Z",
        "updated_at": "2023-05-11T00:31:58Z",
        "closed_at": "2023-05-11T00:31:57Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3218",
            "html_url": "https://github.com/run-llama/llama_index/pull/3218",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3218.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3218.patch",
            "merged_at": "2023-05-11T00:31:57Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3218/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3218/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3211",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3211/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3211/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3211/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3211",
        "id": 1704652403,
        "node_id": "I_kwDOIWuq585lmu5z",
        "number": 3211,
        "title": "Unified Query Framework: ZeroDivisionError: integer division or modulo by zero",
        "user": {
            "login": "jensbech",
            "id": 8881797,
            "node_id": "MDQ6VXNlcjg4ODE3OTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8881797?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jensbech",
            "html_url": "https://github.com/jensbech",
            "followers_url": "https://api.github.com/users/jensbech/followers",
            "following_url": "https://api.github.com/users/jensbech/following{/other_user}",
            "gists_url": "https://api.github.com/users/jensbech/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jensbech/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jensbech/subscriptions",
            "organizations_url": "https://api.github.com/users/jensbech/orgs",
            "repos_url": "https://api.github.com/users/jensbech/repos",
            "events_url": "https://api.github.com/users/jensbech/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jensbech/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-10T21:26:50Z",
        "updated_at": "2023-05-11T16:34:53Z",
        "closed_at": "2023-05-11T16:34:53Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "There seems to be an issue with setting up Graph.\r\nFollowing this guide: https://gpt-index.readthedocs.io/en/latest/guides/tutorials/unified_query.html\r\n\r\nIssue is for both `0.6.0` and `0.6.4`. It is as if no tokens are sent.\r\n\r\nInitial error: `172, in get_text_splitter_given_prompt\r\n    chunk_overlap=self.max_chunk_overlap // num_chunks,\r\nZeroDivisionError: integer division or modulo by zero`\r\n\r\nAfter editing `prompt_helper.py`, hereunder `get_text_splitter_given_prompt()` and `get_chunk_size_given_prompt()`, to force away this error, new error is:\r\n```\r\n\u279c  paul_graham_essay git:(main) \u2717 python3 v2.py\r\nWARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\r\nservice_context.chunk_size_limit:  1024\r\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\r\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 3859 tokens\r\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\r\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 19817 tokens\r\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\r\nINFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\r\n```\r\n\r\nFull script is as follows (https://github.com/jensbech/kazar-llama-index-chatbot/blob/main/examples/paul_graham_essay/v2.py):\r\n```\r\nimport os\r\nfrom llama_index import GPTSimpleKeywordTableIndex, LLMPredictor, SimpleDirectoryReader\r\nfrom dotenv import load_dotenv\r\nimport logging\r\nimport sys\r\nfrom llama_index import GPTVectorStoreIndex, ServiceContext, StorageContext\r\nfrom langchain.chat_models import ChatOpenAI\r\nfrom llama_index.indices.composability import ComposableGraph\r\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\r\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\r\n\r\n\r\nload_dotenv()\r\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\r\nDISCORD_TOKEN = os.getenv(\"DISCORD_TOKEN\")\r\n\r\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\r\n\r\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\r\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\r\n\r\nwiki_titles = [\"player-characters\", \"expeditions\"]\r\ndocuments = {}\r\nfor title in wiki_titles:\r\n    documents[title] = SimpleDirectoryReader(input_dir=f\"data/{title}\").load_data()\r\n\r\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\r\nservice_context = ServiceContext.from_defaults(\r\n    llm_predictor=llm_predictor, chunk_size_limit=1024\r\n)\r\n\r\n\r\nvector_indexes = {}\r\nfor wiki_title in wiki_titles:\r\n    storage_context = StorageContext.from_defaults()\r\n    vector_indexes[wiki_title] = GPTVectorStoreIndex.from_documents(\r\n        documents[wiki_title],\r\n        service_context=service_context,\r\n        storage_context=storage_context,\r\n    )\r\n    vector_indexes[wiki_title].index_struct.index_id = wiki_title\r\n\r\nindex_summaries = {}\r\nfor wiki_title in wiki_titles:\r\n    # set summary for text file.\r\n    index_summaries[wiki_title] = (\r\n        f\"This content contains articles about {wiki_title}.\"\r\n        f\"Use this index if you need to lookup specific facts about {wiki_title}.\"\r\n    )\r\n\r\ngraph = ComposableGraph.from_indices(\r\n    GPTSimpleKeywordTableIndex,\r\n    [index for _, index in vector_indexes.items()],\r\n    [summary for _, summary in index_summaries.items()],\r\n    max_keywords_per_chunk=50,\r\n)\r\n\r\nroot_index = graph.get_index(graph.index_struct.index_id)\r\nroot_index.set_index_id(\"compare_contrast\")\r\nroot_summary = (\r\n    \"This index contains articles about ....\"\r\n    \"Use this index if you want to compare things about the world.\"\r\n)\r\n\r\ndecompose_transform = DecomposeQueryTransform(llm_predictor, verbose=True)\r\n\r\n\r\ncustom_query_engines = {}\r\nfor index in vector_indexes.values():\r\n    query_engine = index.as_query_engine(service_context=service_context)\r\n    query_engine = TransformQueryEngine(\r\n        query_engine,\r\n        query_transform=decompose_transform,\r\n        transform_extra_info={\"index_summary\": index.index_struct.summary},\r\n    )\r\n    custom_query_engines[index.index_id] = query_engine\r\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\r\n    retriever_mode=\"simple\",\r\n    response_mode=\"tree_summarize\",\r\n    service_context=service_context,\r\n)\r\n\r\n# define query engine\r\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\r\n\r\n# query the graph\r\nresponse = query_engine.query(\"how are you?\")\r\n```\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3211/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3211/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3210",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3210/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3210/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3210/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3210",
        "id": 1704638011,
        "node_id": "I_kwDOIWuq585lmrY7",
        "number": 3210,
        "title": "Only data from the index is used, and it is necessary to supplement the general knowledge of chatGPT with data from the index...",
        "user": {
            "login": "IvanPigarev",
            "id": 97641648,
            "node_id": "U_kgDOBdHksA",
            "avatar_url": "https://avatars.githubusercontent.com/u/97641648?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/IvanPigarev",
            "html_url": "https://github.com/IvanPigarev",
            "followers_url": "https://api.github.com/users/IvanPigarev/followers",
            "following_url": "https://api.github.com/users/IvanPigarev/following{/other_user}",
            "gists_url": "https://api.github.com/users/IvanPigarev/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/IvanPigarev/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/IvanPigarev/subscriptions",
            "organizations_url": "https://api.github.com/users/IvanPigarev/orgs",
            "repos_url": "https://api.github.com/users/IvanPigarev/repos",
            "events_url": "https://api.github.com/users/IvanPigarev/events{/privacy}",
            "received_events_url": "https://api.github.com/users/IvanPigarev/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-10T21:13:10Z",
        "updated_at": "2023-06-06T05:14:34Z",
        "closed_at": "2023-06-06T05:14:33Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I apologize if this is a silly question, I'm just getting started. Llama index creates indexes from the pdf folder and everything works. But chat only has knowledge from these pdfs. If I ask him about other topics and I want him to give answers based on his previous learning, but to prioritize the information from the pdf - can I do that?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3210/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3210/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3208",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3208/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3208/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3208/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3208",
        "id": 1704498682,
        "node_id": "PR_kwDOIWuq585QOUAI",
        "number": 3208,
        "title": "Better metadata filtering support for vector stores",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-10T19:18:38Z",
        "updated_at": "2023-07-03T23:26:12Z",
        "closed_at": "2023-05-12T06:31:40Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3208",
            "html_url": "https://github.com/run-llama/llama_index/pull/3208",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3208.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3208.patch",
            "merged_at": "2023-05-12T06:31:39Z"
        },
        "body": "### Summary\r\n* Ingestion/indexing time \r\n  * standardize how Node data (i.e. extra_info, node_info, relationships) is represented in vector DB as metadata dict \r\n  * notably: store `extra_info` at top level, to allow metadata filtering (for vectorDBs that support this), store `node_info` as json blob \r\n  * keep old accessing logic as fallback, for backwards compatibility\r\n* Query time\r\n  *  do not take in metadata filter in vector store ctor (not sure why this made sense), move to be specified at query time\r\n  *  create a standard metadata filtering interface (right now only support exact match)\r\n  * allow implementation specific query kwargs to be injected at query time\r\n\r\n### Follow ups\r\n- [ ] auto vector store retriever (LLM-based parameter selection)\r\n- [ ] [maybe] expose vector store as structured tool",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3208/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3208/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3205",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3205/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3205/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3205/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3205",
        "id": 1704367065,
        "node_id": "I_kwDOIWuq585llpPZ",
        "number": 3205,
        "title": "Documentation Example are broken",
        "user": {
            "login": "FFX01",
            "id": 6164955,
            "node_id": "MDQ6VXNlcjYxNjQ5NTU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6164955?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/FFX01",
            "html_url": "https://github.com/FFX01",
            "followers_url": "https://api.github.com/users/FFX01/followers",
            "following_url": "https://api.github.com/users/FFX01/following{/other_user}",
            "gists_url": "https://api.github.com/users/FFX01/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/FFX01/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/FFX01/subscriptions",
            "organizations_url": "https://api.github.com/users/FFX01/orgs",
            "repos_url": "https://api.github.com/users/FFX01/repos",
            "events_url": "https://api.github.com/users/FFX01/events{/privacy}",
            "received_events_url": "https://api.github.com/users/FFX01/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318866,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUg",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/documentation",
                "name": "documentation",
                "color": "0075ca",
                "default": true,
                "description": "Improvements or additions to documentation"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-05-10T17:36:27Z",
        "updated_at": "2023-06-11T19:51:06Z",
        "closed_at": "2023-06-11T19:51:06Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi there! Really interesting project here!\r\n\r\nI've been trying to run through the examples in the documentation and I can't seem to get a single one of the tutorial projects to work without major changes. It seems like maybe the examples are out of date?\r\n\r\nI've experienced the following issues:\r\n\r\n## Dependency Version Conflicts\r\nSo far I've only experienced this with numpy and argilla. However, it illustrates that dependencies are not handled well.\r\n\r\n## Broken Example Code\r\nMany pieces of example code are simply broken. They have missing/incorrect imports, calls to methods that don't exist, missing dependencies, and references to undefined variables.\r\n### Example: \r\n```python\r\nresponse = vector_indices[\"Toronto\"].query(\"What are the sports teams in Toronto?\")\r\nprint(str(response))\r\n```\r\nThis example will not work. It needs to be changed to:\r\n```python\r\nresponse = vector_indices[\"Toronto\"].as_query_engine().query(\"What are the sports teams in Toronto?\")\r\nprint(str(response))\r\n```\r\n\r\n## General Documentation Quality\r\nI know this project is fairly new and I shouldn't expect perfect documentation. However, the documentation as it stands is confusing and outdated(I think) in many places. I would expect that the getting started and tutorial sections at least would work and explain things well, but they do not. Unfortunately, this makes it very difficult for me to test llama-index and next to impossible to recommend adoption at my organization.\r\n\r\nI am willing to help improve the documentation, but I honestly have a poor understanding of how the library works and would not be able to update the documentation myself. I want to be clear that I'm not just complaining; I am very willing to help in any way I can. I have some suggestions:\r\n\r\n## Suggestions:\r\n1. All example code should \"just work\". This means that as long as I have the dependencies installed, I should be able to copy-paste the code and run it without any major issues. Of course, any issues with 3rd-party tools/APIs are excluded from this.\r\n2. Ensure all code examples are up-to-date with every release.\r\n3. Restructure tutorials and guides for less confusing control-flow. For example,I ran into several issues where variables defined inside loops were used outside of the loop which requires re-running the loop in order to ensure the variable is present or having to re-write the example code myself\r\n4. Ensure all tutorials and guides explicitly call out required dependencies.\r\n5. Ensure all tutorials and guides are actually importing the modules/functions/classes/etc needed for the code to run.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3205/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3205/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3203",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3203/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3203/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3203/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3203",
        "id": 1704246210,
        "node_id": "I_kwDOIWuq585llLvC",
        "number": 3203,
        "title": "Potential bug in building_a_chatbot tutorial",
        "user": {
            "login": "astro313",
            "id": 1796979,
            "node_id": "MDQ6VXNlcjE3OTY5Nzk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1796979?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/astro313",
            "html_url": "https://github.com/astro313",
            "followers_url": "https://api.github.com/users/astro313/followers",
            "following_url": "https://api.github.com/users/astro313/following{/other_user}",
            "gists_url": "https://api.github.com/users/astro313/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/astro313/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/astro313/subscriptions",
            "organizations_url": "https://api.github.com/users/astro313/orgs",
            "repos_url": "https://api.github.com/users/astro313/repos",
            "events_url": "https://api.github.com/users/astro313/events{/privacy}",
            "received_events_url": "https://api.github.com/users/astro313/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-10T16:07:39Z",
        "updated_at": "2023-06-06T05:38:42Z",
        "closed_at": "2023-06-06T05:38:42Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "In the [tutorial](https://gpt-index.readthedocs.io/en/v0.6.2/guides/tutorials/building_a_chatbot.html) for the version I'm using  (v0.6.2), it says in the text: \"\"\"Below, we define a IndexToolConfig for our graph. \"\"\", but after the code snippet, it says \"\"\"Besides the GraphToolConfig object, we also define an IndexToolConfig corresponding to each index:\"\"\". \r\n\r\nBut we never used `GraphToolConfig`. Which one should we use for the `graph_config`?\r\nIf we were to use `GraphToolConfig`, it seems like it's not defined in the codebase (see `langchain_helpers/agents/__init__.py`)\r\n\r\nSecondly, we defined custom retrievers `custom_query_engines`, but it's never used anywhere it seems? At the end, it seems like only `index_configs` and `graph_config` were passed to the `create_llama_chat_agent` .. Should we pass `custom_query_engines` to `IndexToolConfig` when constructing `graph_config`? ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3203/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3203/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3202",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3202/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3202/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3202/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3202",
        "id": 1704230557,
        "node_id": "I_kwDOIWuq585llH6d",
        "number": 3202,
        "title": "root_id when loading graph",
        "user": {
            "login": "astro313",
            "id": 1796979,
            "node_id": "MDQ6VXNlcjE3OTY5Nzk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1796979?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/astro313",
            "html_url": "https://github.com/astro313",
            "followers_url": "https://api.github.com/users/astro313/followers",
            "following_url": "https://api.github.com/users/astro313/following{/other_user}",
            "gists_url": "https://api.github.com/users/astro313/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/astro313/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/astro313/subscriptions",
            "organizations_url": "https://api.github.com/users/astro313/orgs",
            "repos_url": "https://api.github.com/users/astro313/repos",
            "events_url": "https://api.github.com/users/astro313/events{/privacy}",
            "received_events_url": "https://api.github.com/users/astro313/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-05-10T15:58:41Z",
        "updated_at": "2023-10-23T04:24:28Z",
        "closed_at": "2023-09-27T16:02:15Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "When loading graph, we need to provide the root_id, but how would I know the root_id? For example, \r\n\r\n```\r\nfrom llama_index.indices.composability import ComposableGraph\r\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, max_tokens=512, model_name='gpt-3.5-turbo'))\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\nstorage_context = StorageContext.from_defaults()\r\n\r\ngraph = ComposableGraph.from_indices(\r\n    GPTListIndex,\r\n    [essay_index, empty_index],\r\n    index_summaries=[essay_index_summary, empty_index_summary],\r\n    service_context=service_context,\r\n    storage_context=storage_context,\r\n)\r\n```\r\nThen when we reload: \r\n```\r\nfrom llama_index.indices.loading import load_graph_from_storage\r\nstorage_context.persist('data/heterogenous_graph_test')\r\ngraph=  load_graph_from_storage(storage_context, root_id=graph.root_id)\r\n```\r\nBut `graph` would not be defined at reloading without reconstructing `graph`",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3202/reactions",
            "total_count": 3,
            "+1": 3,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3202/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3201",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3201/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3201/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3201/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3201",
        "id": 1704126806,
        "node_id": "I_kwDOIWuq585lkulW",
        "number": 3201,
        "title": "Customized segments of text vs text chunking via ServiceContext",
        "user": {
            "login": "astro313",
            "id": 1796979,
            "node_id": "MDQ6VXNlcjE3OTY5Nzk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1796979?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/astro313",
            "html_url": "https://github.com/astro313",
            "followers_url": "https://api.github.com/users/astro313/followers",
            "following_url": "https://api.github.com/users/astro313/following{/other_user}",
            "gists_url": "https://api.github.com/users/astro313/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/astro313/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/astro313/subscriptions",
            "organizations_url": "https://api.github.com/users/astro313/orgs",
            "repos_url": "https://api.github.com/users/astro313/repos",
            "events_url": "https://api.github.com/users/astro313/events{/privacy}",
            "received_events_url": "https://api.github.com/users/astro313/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-10T14:57:09Z",
        "updated_at": "2023-06-06T05:13:18Z",
        "closed_at": "2023-06-06T05:13:09Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "When indexing from a single text document, Is there a way to not use `ServiceContext` and `chunk_size_limit`, which seems to be calling token splitting `TokenTextSplitter`, but instead just pass a list of strings that are already segmented on this single document? \r\n\r\nInside `ServiceContext.from_default()`, `PromptHelper.from_llm_predictor` is also using `chunk_size_limit` and calling some methods that performs text splitting given some prompt. \r\n\r\nSo can we have a feature that allows customized segments of text for both `ServiceContext` and `PromptHelper`? ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3201/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3201/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3195",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3195/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3195/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3195/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3195",
        "id": 1703960918,
        "node_id": "PR_kwDOIWuq585QMfxq",
        "number": 3195,
        "title": "fix typo in advanced",
        "user": {
            "login": "nathan-az",
            "id": 42650258,
            "node_id": "MDQ6VXNlcjQyNjUwMjU4",
            "avatar_url": "https://avatars.githubusercontent.com/u/42650258?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nathan-az",
            "html_url": "https://github.com/nathan-az",
            "followers_url": "https://api.github.com/users/nathan-az/followers",
            "following_url": "https://api.github.com/users/nathan-az/following{/other_user}",
            "gists_url": "https://api.github.com/users/nathan-az/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nathan-az/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nathan-az/subscriptions",
            "organizations_url": "https://api.github.com/users/nathan-az/orgs",
            "repos_url": "https://api.github.com/users/nathan-az/repos",
            "events_url": "https://api.github.com/users/nathan-az/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nathan-az/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-10T13:38:59Z",
        "updated_at": "2023-05-10T17:17:40Z",
        "closed_at": "2023-05-10T17:17:40Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3195",
            "html_url": "https://github.com/run-llama/llama_index/pull/3195",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3195.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3195.patch",
            "merged_at": "2023-05-10T17:17:40Z"
        },
        "body": "Pretty sure this breaks the global record for highest value PR. Might take a while to review.\r\n\r\nOn a serious note, ran a quick inspection over the rest of the project and didn't find any other obvious typos :)",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3195/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3195/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3193",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3193/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3193/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3193/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3193",
        "id": 1703924587,
        "node_id": "I_kwDOIWuq585lj9Nr",
        "number": 3193,
        "title": "Optimize the query time for retrieval engine",
        "user": {
            "login": "balasurajp",
            "id": 34001536,
            "node_id": "MDQ6VXNlcjM0MDAxNTM2",
            "avatar_url": "https://avatars.githubusercontent.com/u/34001536?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balasurajp",
            "html_url": "https://github.com/balasurajp",
            "followers_url": "https://api.github.com/users/balasurajp/followers",
            "following_url": "https://api.github.com/users/balasurajp/following{/other_user}",
            "gists_url": "https://api.github.com/users/balasurajp/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balasurajp/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balasurajp/subscriptions",
            "organizations_url": "https://api.github.com/users/balasurajp/orgs",
            "repos_url": "https://api.github.com/users/balasurajp/repos",
            "events_url": "https://api.github.com/users/balasurajp/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balasurajp/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318869,
                "node_id": "LA_kwDOIWuq588AAAABGzNfVQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/enhancement",
                "name": "enhancement",
                "color": "a2eeef",
                "default": true,
                "description": "New feature or request"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-10T13:21:08Z",
        "updated_at": "2023-06-11T19:49:46Z",
        "closed_at": "2023-06-11T19:49:46Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "<img width=\"1025\" alt=\"image\" src=\"https://github.com/jerryjliu/llama_index/assets/34001536/055a706f-e663-4239-a412-01543de0bbe8\">\r\n\r\nplease replace the cosine similarity query retrieval with the dot product from sklearn or numpy which will speed up the query time by multifold.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3193/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3193/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3189",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3189/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3189/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3189/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3189",
        "id": 1703863426,
        "node_id": "I_kwDOIWuq585ljuSC",
        "number": 3189,
        "title": "Persisted IndexDict with empty nodes and docs",
        "user": {
            "login": "logivasz",
            "id": 114065723,
            "node_id": "U_kgDOBsyBOw",
            "avatar_url": "https://avatars.githubusercontent.com/u/114065723?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/logivasz",
            "html_url": "https://github.com/logivasz",
            "followers_url": "https://api.github.com/users/logivasz/followers",
            "following_url": "https://api.github.com/users/logivasz/following{/other_user}",
            "gists_url": "https://api.github.com/users/logivasz/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/logivasz/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/logivasz/subscriptions",
            "organizations_url": "https://api.github.com/users/logivasz/orgs",
            "repos_url": "https://api.github.com/users/logivasz/repos",
            "events_url": "https://api.github.com/users/logivasz/events{/privacy}",
            "received_events_url": "https://api.github.com/users/logivasz/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-10T12:52:58Z",
        "updated_at": "2023-06-11T19:47:15Z",
        "closed_at": "2023-06-11T19:47:15Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "        service_context=self.initServiceContext()\r\n        documents=self.loadDocuments(dataFolder)\r\n        for document in documents:\r\n            document.doc_id=document.get_doc_hash()\r\n\r\n        environment = 'us-west1-gcp-free'\r\n        index_name = 'myindex'\r\n        uri=\"mongodb://root:example@mongo:27017\"\r\n\r\n        pinecone.init(environment=environment)\r\n        pinecone_index = pinecone.Index(index_name=index_name)\r\n        vectoreStore = PineconeVectorStore(pinecone_index=pinecone_index)       \r\n        docstore = MongoDocumentStore.from_uri(uri=uri,db_name='doc_store')       \r\n        indexStore=MongoIndexStore.from_uri(uri=uri,db_name='index_store')\r\n        indexStruct=indexStore.index_structs()\r\n        storage_context = StorageContext.from_defaults(docstore=docstore,index_store=indexStore,vector_store=vectoreStore)\r\n        if len(indexStruct):\r\n           index = load_index_from_storage(storage_context)\r\n           print(index._index_struct)\r\n           upd = index.refresh(documents)\r\n           print(upd)\r\n        else:\r\n            index = GPTVectorStoreIndex.from_documents(documents,storage_context=storage_context,service_context=service_context)        \r\n            print(index._index_struct)\r\n\r\nThe index_struct always looks like this\r\nIndexDict(index_id='e00a2c45-2857-41d2-a3c6-d53c79b2722d', summary=None, nodes_dict={}, doc_id_dict={}, embeddings_dict={})\r\nnodes_dict={}, doc_id_dict={} is always empty  but  at the first use (index = GPTVectorStoreIndex.from_documents) the index is works fine, I can run queries on it.\r\nWhen loaded using load_index_from_storage, I get an errors when I run queries on it.\r\n'EOL while scanning string literal (<unknown>, line 4)'\r\nWhen the data is persisted to the disk everything is ok\r\nWhat am I doing wrong?\r\n \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3189/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3189/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3177",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3177/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3177/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3177/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3177",
        "id": 1703783597,
        "node_id": "I_kwDOIWuq585ljayt",
        "number": 3177,
        "title": "how to load .json index in new llama_index version",
        "user": {
            "login": "vimalrajan242001",
            "id": 64751374,
            "node_id": "MDQ6VXNlcjY0NzUxMzc0",
            "avatar_url": "https://avatars.githubusercontent.com/u/64751374?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vimalrajan242001",
            "html_url": "https://github.com/vimalrajan242001",
            "followers_url": "https://api.github.com/users/vimalrajan242001/followers",
            "following_url": "https://api.github.com/users/vimalrajan242001/following{/other_user}",
            "gists_url": "https://api.github.com/users/vimalrajan242001/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vimalrajan242001/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vimalrajan242001/subscriptions",
            "organizations_url": "https://api.github.com/users/vimalrajan242001/orgs",
            "repos_url": "https://api.github.com/users/vimalrajan242001/repos",
            "events_url": "https://api.github.com/users/vimalrajan242001/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vimalrajan242001/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-05-10T12:07:42Z",
        "updated_at": "2023-06-06T06:23:35Z",
        "closed_at": "2023-06-06T06:23:34Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I created to vector index using **GPTSimpleVectorIndex** and then saved in json format.\r\nNow I updated to latest llama_index version, in this GPTVectorStoreIndex' has no attribute 'load_from_disk'\r\nhow can we load json vector index in latest llama_index. can anyone explain this",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3177/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3177/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3174",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3174/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3174/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3174/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3174",
        "id": 1703728285,
        "node_id": "I_kwDOIWuq585ljNSd",
        "number": 3174,
        "title": "AzureOpenAi GPTDocumentSummaryIndex error engine or deployment_id",
        "user": {
            "login": "mrcmoresi",
            "id": 12852824,
            "node_id": "MDQ6VXNlcjEyODUyODI0",
            "avatar_url": "https://avatars.githubusercontent.com/u/12852824?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mrcmoresi",
            "html_url": "https://github.com/mrcmoresi",
            "followers_url": "https://api.github.com/users/mrcmoresi/followers",
            "following_url": "https://api.github.com/users/mrcmoresi/following{/other_user}",
            "gists_url": "https://api.github.com/users/mrcmoresi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mrcmoresi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mrcmoresi/subscriptions",
            "organizations_url": "https://api.github.com/users/mrcmoresi/orgs",
            "repos_url": "https://api.github.com/users/mrcmoresi/repos",
            "events_url": "https://api.github.com/users/mrcmoresi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mrcmoresi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-10T11:39:34Z",
        "updated_at": "2023-07-05T00:10:46Z",
        "closed_at": "2023-07-05T00:10:46Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi Everyone, \r\nI'm following this tutorial\r\nhttps://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/doc_summary/DocSummary.ipynb\r\n\r\nand I'm trying to replicate it using Azure OpenAi service, but I'm facing a problem \r\n\r\nI'm using:\r\nllama_index 0.6.4\r\nPython 3.9.16\r\n\r\n```\r\nllm_predictor_chatgpt = LLMPredictor(\r\n    llm=AzureChatOpenAI(\r\n    temperature=0,\r\n    deployment_name=\"gpt-35-turbo\",\r\n    openai_api_version=\"2023-03-15-preview\",\r\n    ))\r\n\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt, chunk_size_limit=1024)\r\n\r\nresponse_synthesizer = ResponseSynthesizer.from_args(response_mode=\"tree_summarize\", use_async=True)\r\ndoc_summary_index = GPTDocumentSummaryIndex.from_documents(\r\n    city_docs, \r\n    service_context=service_context,\r\n    response_synthesizer=response_synthesizer\r\n)\r\n```\r\n\r\nthis is the error:\r\n\r\n```\r\nWARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\r\nUnknown max input size for gpt-3.5-turbo, using defaults.\r\n<llama_index.indices.query.response_synthesis.ResponseSynthesizer object at 0x7fcf5e80a520>\r\ncurrent doc id: Toronto\r\nINFO:llama_index.indices.common_tree.base:> Building index from nodes: 7 chunks\r\n> Building index from nodes: 7 chunks\r\n---------------------------------------------------------------------------\r\nInvalidRequestError                       Traceback (most recent call last)\r\nCell In[11], line 11\r\n      8 service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt, chunk_size_limit=1024)\r\n     10 response_synthesizer = ResponseSynthesizer.from_args(response_mode=\"tree_summarize\", use_async=True)\r\n---> 11 doc_summary_index = GPTDocumentSummaryIndex.from_documents(\r\n     12     city_docs, \r\n     13     service_context=service_context,\r\n     14     response_synthesizer=response_synthesizer\r\n     15 )\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/indices/base.py:100, in BaseGPTIndex.from_documents(cls, documents, storage_context, service_context, **kwargs)\r\n     95 nodes = service_context.node_parser.get_nodes_from_documents(documents)\r\n     96 service_context.callback_manager.on_event_end(\r\n     97     CBEventType.CHUNKING, payload={\"nodes\": nodes}, event_id=event_id\r\n     98 )\r\n--> 100 return cls(\r\n    101     nodes=nodes,\r\n    102     storage_context=storage_context,\r\n    103     service_context=service_context,\r\n    104     **kwargs,\r\n    105 )\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/indices/document_summary/base.py:65, in GPTDocumentSummaryIndex.__init__(self, nodes, index_struct, service_context, response_synthesizer, summary_query, **kwargs)\r\n     61 self._response_synthesizer = (\r\n     62     response_synthesizer or ResponseSynthesizer.from_args()\r\n     63 )\r\n     64 self._summary_query = summary_query or \"summarize:\"\r\n---> 65 super().__init__(\r\n     66     nodes=nodes,\r\n     67     index_struct=index_struct,\r\n     68     service_context=service_context,\r\n     69     **kwargs,\r\n     70 )\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/indices/base.py:66, in BaseGPTIndex.__init__(self, nodes, index_struct, storage_context, service_context, **kwargs)\r\n     64 if index_struct is None:\r\n     65     assert nodes is not None\r\n---> 66     index_struct = self.build_index_from_nodes(nodes)\r\n     67 self._index_struct = index_struct\r\n     68 self._storage_context.index_store.add_index_struct(self._index_struct)\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/token_counter/token_counter.py:78, in llm_token_counter..wrap..wrapped_llm_predict(_self, *args, **kwargs)\r\n     76 def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\r\n     77     with wrapper_logic(_self):\r\n---> 78         f_return_val = f(_self, *args, **kwargs)\r\n     80     return f_return_val\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/indices/base.py:160, in BaseGPTIndex.build_index_from_nodes(self, nodes)\r\n    158 \"\"\"Build the index from nodes.\"\"\"\r\n    159 self._docstore.add_documents(nodes, allow_update=True)\r\n--> 160 return self._build_index_from_nodes(nodes)\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/indices/document_summary/base.py:150, in GPTDocumentSummaryIndex._build_index_from_nodes(self, nodes)\r\n    147 # first get doc_id to nodes_dict, generate a summary for each doc_id,\r\n    148 # then build the index struct\r\n    149 index_struct = IndexDocumentSummary()\r\n--> 150 self._add_nodes_to_index(index_struct, nodes)\r\n    151 return index_struct\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/indices/document_summary/base.py:128, in GPTDocumentSummaryIndex._add_nodes_to_index(self, index_struct, nodes)\r\n    126 nodes_with_scores = [NodeWithScore(n) for n in nodes]\r\n    127 # get the summary for each doc_id\r\n--> 128 summary_response = self._response_synthesizer.synthesize(\r\n    129     query_bundle=QueryBundle(self._summary_query),\r\n    130     nodes=nodes_with_scores,\r\n    131 )\r\n    132 summary_response = cast(Response, summary_response)\r\n    133 summary_node_dict[doc_id] = Node(\r\n    134     summary_response.response,\r\n    135     relationships={DocumentRelationship.SOURCE: doc_id},\r\n    136 )\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/indices/query/response_synthesis.py:163, in ResponseSynthesizer.synthesize(self, query_bundle, nodes, additional_source_nodes)\r\n    161 if self._response_mode != ResponseMode.NO_TEXT:\r\n    162     assert self._response_builder is not None\r\n--> 163     response_str = self._response_builder.get_response(\r\n    164         query_str=query_bundle.query_str,\r\n    165         text_chunks=text_chunks,\r\n    166         **self._response_kwargs,\r\n    167     )\r\n    168 else:\r\n    169     response_str = None\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/token_counter/token_counter.py:78, in llm_token_counter..wrap..wrapped_llm_predict(_self, *args, **kwargs)\r\n     76 def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\r\n     77     with wrapper_logic(_self):\r\n---> 78         f_return_val = f(_self, *args, **kwargs)\r\n     80     return f_return_val\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/indices/response/response_builder.py:422, in TreeSummarize.get_response(self, query_str, text_chunks, prev_response, num_children, **response_kwargs)\r\n    420 for node in nodes:\r\n    421     index_graph.insert(node)\r\n--> 422 index_graph = index_builder.build_index_from_nodes(\r\n    423     index_graph, index_graph.all_nodes, index_graph.all_nodes\r\n    424 )\r\n    425 root_node_ids = index_graph.root_nodes\r\n    426 root_nodes = {\r\n    427     index: index_builder.docstore.get_node(node_id)\r\n    428     for index, node_id in root_node_ids.items()\r\n    429 }\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/indices/common_tree/base.py:151, in GPTTreeIndexBuilder.build_index_from_nodes(self, index_graph, cur_node_ids, all_node_ids, level)\r\n    144 if self._use_async:\r\n    145     tasks = [\r\n    146         self._service_context.llm_predictor.apredict(\r\n    147             self.summary_prompt, context_str=text_chunk\r\n    148         )\r\n    149         for text_chunk in text_chunks\r\n    150     ]\r\n--> 151     outputs: List[Tuple[str, str]] = run_async_tasks(tasks)\r\n    152     summaries = [output[0] for output in outputs]\r\n    153 else:\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/async_utils.py:12, in run_async_tasks(tasks)\r\n      9 async def _gather() -> List[Any]:\r\n     10     return await asyncio.gather(*tasks)\r\n---> 12 outputs: List[Any] = asyncio.run(_gather())\r\n     13 return outputs\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/nest_asyncio.py:35, in _patch_asyncio..run(main, debug)\r\n     33 task = asyncio.ensure_future(main)\r\n     34 try:\r\n---> 35     return loop.run_until_complete(task)\r\n     36 finally:\r\n     37     if not task.done():\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/nest_asyncio.py:90, in _patch_loop..run_until_complete(self, future)\r\n     87 if not f.done():\r\n     88     raise RuntimeError(\r\n     89         'Event loop stopped before Future completed.')\r\n---> 90 return f.result()\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/asyncio/futures.py:201, in Future.result(self)\r\n    199 self.__log_traceback = False\r\n    200 if self._exception is not None:\r\n--> 201     raise self._exception\r\n    202 return self._result\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/asyncio/tasks.py:258, in Task.__step(***failed resolving arguments***)\r\n    256         result = coro.send(None)\r\n    257     else:\r\n--> 258         result = coro.throw(exc)\r\n    259 except StopIteration as exc:\r\n    260     if self._must_cancel:\r\n    261         # Task is cancelled right before coro stops.\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/async_utils.py:10, in run_async_tasks.._gather()\r\n      9 async def _gather() -> List[Any]:\r\n---> 10     return await asyncio.gather(*tasks)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/asyncio/tasks.py:328, in Task.__wakeup(self, future)\r\n    326 def __wakeup(self, future):\r\n    327     try:\r\n--> 328         future.result()\r\n    329     except BaseException as exc:\r\n    330         # This may also be a cancellation.\r\n    331         self.__step(exc)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/asyncio/tasks.py:256, in Task.__step(***failed resolving arguments***)\r\n    252 try:\r\n    253     if exc is None:\r\n    254         # We use the `send` method directly, because coroutines\r\n    255         # don't have `__iter__` and `__next__` methods.\r\n--> 256         result = coro.send(None)\r\n    257     else:\r\n    258         result = coro.throw(exc)\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/llm_predictor/base.py:324, in LLMPredictor.apredict(self, prompt, **prompt_args)\r\n    314 \"\"\"Async predict the answer to a query.\r\n    315 \r\n    316 Args:\r\n   (...)\r\n    321 \r\n    322 \"\"\"\r\n    323 formatted_prompt = prompt.format(llm=self._llm, **prompt_args)\r\n--> 324 llm_prediction = await self._apredict(prompt, **prompt_args)\r\n    325 logger.debug(llm_prediction)\r\n    327 # We assume that the value of formatted_prompt is exactly the thing\r\n    328 # eventually sent to OpenAI, or whatever LLM downstream\r\n\r\nFile /mnt/batch/tasks/shared/LS_root/mounts/clusters/marco-gpu2/code/Users/marco.moresi/llama_index/llama_index/llm_predictor/base.py:310, in LLMPredictor._apredict(self, prompt, **prompt_args)\r\n    308 full_prompt_args = prompt.get_full_format_args(prompt_args)\r\n    309 # TODO: support retry on throttling\r\n--> 310 llm_prediction = await llm_chain.apredict(**full_prompt_args)\r\n    311 return llm_prediction\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/chains/llm.py:230, in LLMChain.apredict(self, callbacks, **kwargs)\r\n    215 async def apredict(self, callbacks: Callbacks = None, **kwargs: Any) -> str:\r\n    216     \"\"\"Format prompt with kwargs and pass to LLM.\r\n    217 \r\n    218     Args:\r\n   (...)\r\n    228             completion = llm.predict(adjective=\"funny\")\r\n    229     \"\"\"\r\n--> 230     return (await self.acall(kwargs, callbacks=callbacks))[self.output_key]\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/chains/base.py:178, in Chain.acall(self, inputs, return_only_outputs, callbacks)\r\n    176 except (KeyboardInterrupt, Exception) as e:\r\n    177     await run_manager.on_chain_error(e)\r\n--> 178     raise e\r\n    179 await run_manager.on_chain_end(outputs)\r\n    180 return self.prep_outputs(inputs, outputs, return_only_outputs)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/chains/base.py:172, in Chain.acall(self, inputs, return_only_outputs, callbacks)\r\n    166 run_manager = await callback_manager.on_chain_start(\r\n    167     {\"name\": self.__class__.__name__},\r\n    168     inputs,\r\n    169 )\r\n    170 try:\r\n    171     outputs = (\r\n--> 172         await self._acall(inputs, run_manager=run_manager)\r\n    173         if new_arg_supported\r\n    174         else await self._acall(inputs)\r\n    175     )\r\n    176 except (KeyboardInterrupt, Exception) as e:\r\n    177     await run_manager.on_chain_error(e)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/chains/llm.py:195, in LLMChain._acall(self, inputs, run_manager)\r\n    190 async def _acall(\r\n    191     self,\r\n    192     inputs: Dict[str, Any],\r\n    193     run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\r\n    194 ) -> Dict[str, str]:\r\n--> 195     response = await self.agenerate([inputs], run_manager=run_manager)\r\n    196     return self.create_outputs(response)[0]\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/chains/llm.py:90, in LLMChain.agenerate(self, input_list, run_manager)\r\n     88 \"\"\"Generate LLM result from inputs.\"\"\"\r\n     89 prompts, stop = await self.aprep_prompts(input_list)\r\n---> 90 return await self.llm.agenerate_prompt(\r\n     91     prompts, stop, callbacks=run_manager.get_child() if run_manager else None\r\n     92 )\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/llms/base.py:136, in BaseLLM.agenerate_prompt(self, prompts, stop, callbacks)\r\n    129 async def agenerate_prompt(\r\n    130     self,\r\n    131     prompts: List[PromptValue],\r\n    132     stop: Optional[List[str]] = None,\r\n    133     callbacks: Callbacks = None,\r\n    134 ) -> LLMResult:\r\n    135     prompt_strings = [p.to_string() for p in prompts]\r\n--> 136     return await self.agenerate(prompt_strings, stop=stop, callbacks=callbacks)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/llms/base.py:240, in BaseLLM.agenerate(self, prompts, stop, callbacks)\r\n    238 except (KeyboardInterrupt, Exception) as e:\r\n    239     await run_manager.on_llm_error(e, verbose=self.verbose)\r\n--> 240     raise e\r\n    241 await run_manager.on_llm_end(output, verbose=self.verbose)\r\n    242 return output\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/llms/base.py:234, in BaseLLM.agenerate(self, prompts, stop, callbacks)\r\n    229 run_manager = await callback_manager.on_llm_start(\r\n    230     {\"name\": self.__class__.__name__}, prompts\r\n    231 )\r\n    232 try:\r\n    233     output = (\r\n--> 234         await self._agenerate(prompts, stop=stop, run_manager=run_manager)\r\n    235         if new_arg_supported\r\n    236         else await self._agenerate(prompts, stop=stop)\r\n    237     )\r\n    238 except (KeyboardInterrupt, Exception) as e:\r\n    239     await run_manager.on_llm_error(e, verbose=self.verbose)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/llms/openai.py:345, in BaseOpenAI._agenerate(self, prompts, stop, run_manager)\r\n    343     choices.extend(response[\"choices\"])\r\n    344 else:\r\n--> 345     response = await acompletion_with_retry(self, prompt=_prompts, **params)\r\n    346     choices.extend(response[\"choices\"])\r\n    347 if not self.streaming:\r\n    348     # Can't update token usage if streaming\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/llms/openai.py:120, in acompletion_with_retry(llm, **kwargs)\r\n    115 @retry_decorator\r\n    116 async def _completion_with_retry(**kwargs: Any) -> Any:\r\n    117     # Use OpenAI's async api https://github.com/openai/openai-python#async-api\r\n    118     return await llm.client.acreate(**kwargs)\r\n--> 120 return await _completion_with_retry(**kwargs)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/tenacity/_asyncio.py:88, in AsyncRetrying.wraps..async_wrapped(*args, **kwargs)\r\n     86 @functools.wraps(fn)\r\n     87 async def async_wrapped(*args: t.Any, **kwargs: t.Any) -> t.Any:\r\n---> 88     return await fn(*args, **kwargs)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/tenacity/_asyncio.py:47, in AsyncRetrying.__call__(self, fn, *args, **kwargs)\r\n     45 retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\r\n     46 while True:\r\n---> 47     do = self.iter(retry_state=retry_state)\r\n     48     if isinstance(do, DoAttempt):\r\n     49         try:\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/tenacity/__init__.py:314, in BaseRetrying.iter(self, retry_state)\r\n    312 is_explicit_retry = fut.failed and isinstance(fut.exception(), TryAgain)\r\n    313 if not (is_explicit_retry or self.retry(retry_state)):\r\n--> 314     return fut.result()\r\n    316 if self.after is not None:\r\n    317     self.after(retry_state)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/concurrent/futures/_base.py:439, in Future.result(self, timeout)\r\n    437     raise CancelledError()\r\n    438 elif self._state == FINISHED:\r\n--> 439     return self.__get_result()\r\n    441 self._condition.wait(timeout)\r\n    443 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/concurrent/futures/_base.py:391, in Future.__get_result(self)\r\n    389 if self._exception:\r\n    390     try:\r\n--> 391         raise self._exception\r\n    392     finally:\r\n    393         # Break a reference cycle with the exception in self._exception\r\n    394         self = None\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/tenacity/_asyncio.py:50, in AsyncRetrying.__call__(self, fn, *args, **kwargs)\r\n     48 if isinstance(do, DoAttempt):\r\n     49     try:\r\n---> 50         result = await fn(*args, **kwargs)\r\n     51     except BaseException:  # noqa: B902\r\n     52         retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/langchain/llms/openai.py:118, in acompletion_with_retry.._completion_with_retry(**kwargs)\r\n    115 @retry_decorator\r\n    116 async def _completion_with_retry(**kwargs: Any) -> Any:\r\n    117     # Use OpenAI's async api https://github.com/openai/openai-python#async-api\r\n--> 118     return await llm.client.acreate(**kwargs)\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/openai/api_resources/completion.py:45, in Completion.acreate(cls, *args, **kwargs)\r\n     43 while True:\r\n     44     try:\r\n---> 45         return await super().acreate(*args, **kwargs)\r\n     46     except TryAgain as e:\r\n     47         if timeout is not None and time.time() > start + timeout:\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:214, in EngineAPIResource.acreate(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\r\n    192 @classmethod\r\n    193 async def acreate(\r\n    194     cls,\r\n   (...)\r\n    201     **params,\r\n    202 ):\r\n    203     (\r\n    204         deployment_id,\r\n    205         engine,\r\n    206         timeout,\r\n    207         stream,\r\n    208         headers,\r\n    209         request_timeout,\r\n    210         typed_api_type,\r\n    211         requestor,\r\n    212         url,\r\n    213         params,\r\n--> 214     ) = cls.__prepare_create_request(\r\n    215         api_key, api_base, api_type, api_version, organization, **params\r\n    216     )\r\n    217     response, _, api_key = await requestor.arequest(\r\n    218         \"post\",\r\n    219         url,\r\n   (...)\r\n    224         request_timeout=request_timeout,\r\n    225     )\r\n    227     if stream:\r\n    228         # must be an iterator\r\n\r\nFile /anaconda/envs/tf/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:83, in EngineAPIResource.__prepare_create_request(cls, api_key, api_base, api_type, api_version, organization, **params)\r\n     81 if typed_api_type in (util.ApiType.AZURE, util.ApiType.AZURE_AD):\r\n     82     if deployment_id is None and engine is None:\r\n---> 83         raise error.InvalidRequestError(\r\n     84             \"Must provide an 'engine' or 'deployment_id' parameter to create a %s\"\r\n     85             % cls,\r\n     86             \"engine\",\r\n     87         )\r\n     88 else:\r\n     89     if model is None and engine is None:\r\n\r\nInvalidRequestError: Must provide an 'engine' or 'deployment_id' parameter to create a \r\n\r\n```\r\n\r\nWhen using the models deployed in Azure, I'm not usually passing engine or deployment_id. What should I pass here?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3174/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3174/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3172",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3172/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3172/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3172/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3172",
        "id": 1703726097,
        "node_id": "I_kwDOIWuq585ljMwR",
        "number": 3172,
        "title": "How to add an additional step to chat agent?",
        "user": {
            "login": "zeonn",
            "id": 786821,
            "node_id": "MDQ6VXNlcjc4NjgyMQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/786821?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zeonn",
            "html_url": "https://github.com/zeonn",
            "followers_url": "https://api.github.com/users/zeonn/followers",
            "following_url": "https://api.github.com/users/zeonn/following{/other_user}",
            "gists_url": "https://api.github.com/users/zeonn/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zeonn/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zeonn/subscriptions",
            "organizations_url": "https://api.github.com/users/zeonn/orgs",
            "repos_url": "https://api.github.com/users/zeonn/repos",
            "events_url": "https://api.github.com/users/zeonn/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zeonn/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-10T11:38:06Z",
        "updated_at": "2023-09-10T16:59:40Z",
        "closed_at": "2023-09-10T16:59:39Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I initialize the agent with create_llama_chat_agent and then call its run method.\r\nI'm currently using llama_index version 0.5.27. But I can upgrade to branch 0.6 if needed.\r\n\r\nI'm trying to solve this problem: After the agent has found the answer to the given question with or without the index, I would like to add an additional check for completeness of information in the given query in relation to the context found in the index and the answer received from the model. And in case the query wasn't complete enough (for example, it was too abstract and can be interpreted differently), the model should be forced to generate a new question instead of a response, asking for more specific information. Then, this request and the answer received from the user will be used as the context of the dialog, and the following answer will be more accurate, as the clarifying request complements the original one.\r\n\r\nUnfortunately, I haven't found the right way to do this. Could someone give me a hint?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3172/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3172/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3159",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3159/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3159/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3159/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3159",
        "id": 1703648179,
        "node_id": "I_kwDOIWuq585li5uz",
        "number": 3159,
        "title": "Possible to load a 4 bit quantized model ?",
        "user": {
            "login": "LucasBordanave",
            "id": 131307949,
            "node_id": "U_kgDOB9OZrQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/131307949?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/LucasBordanave",
            "html_url": "https://github.com/LucasBordanave",
            "followers_url": "https://api.github.com/users/LucasBordanave/followers",
            "following_url": "https://api.github.com/users/LucasBordanave/following{/other_user}",
            "gists_url": "https://api.github.com/users/LucasBordanave/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/LucasBordanave/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/LucasBordanave/subscriptions",
            "organizations_url": "https://api.github.com/users/LucasBordanave/orgs",
            "repos_url": "https://api.github.com/users/LucasBordanave/repos",
            "events_url": "https://api.github.com/users/LucasBordanave/events{/privacy}",
            "received_events_url": "https://api.github.com/users/LucasBordanave/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-10T10:50:27Z",
        "updated_at": "2023-10-14T20:09:29Z",
        "closed_at": "2023-10-14T20:09:28Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I am a begginer in all that coding stuff and I am looking to run a 7B Llama model quantized in 4 bit with llama-index.\r\n\r\nThe thing is that I need to load the .pt file of it and it's been a few days now that I am completely lost with the multiple errors... \r\n\r\nThis code is full of error and unuseful stuff that I can handle with. But I would like to know if some of you had run a 4 bit model with llama-idex, is that just possible ?\r\n\r\n`class CustomLLM(LLM):\r\n    tokenizer = LlamaTokenizer.from_pretrained(\"/home/lucas/AI/Model/LLaMA-HFv2\")\r\n    model = LlamaForCausalLM.from_pretrained(\"/home/lucas/AI/Model/LLaMA-HFv2\")\r\n\r\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\r\n        prompt_length = len(prompt)\r\n\r\n        # only return newly generated tokens\r\n        return response[prompt_length:]\r\n\r\n    @property\r\n    def _identifying_params(self) -> Mapping[str, Any]:\r\n        return {\"name_of_model\": self.llm}\r\n\r\n    @property\r\n    def _llm_type(self) -> str:\r\n        return \"custom\"`",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3159/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3159/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3158",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3158/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3158/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3158/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3158",
        "id": 1703647283,
        "node_id": "I_kwDOIWuq585li5gz",
        "number": 3158,
        "title": "last_token_usage doesn't update for Embedding calls",
        "user": {
            "login": "balasurajp",
            "id": 34001536,
            "node_id": "MDQ6VXNlcjM0MDAxNTM2",
            "avatar_url": "https://avatars.githubusercontent.com/u/34001536?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balasurajp",
            "html_url": "https://github.com/balasurajp",
            "followers_url": "https://api.github.com/users/balasurajp/followers",
            "following_url": "https://api.github.com/users/balasurajp/following{/other_user}",
            "gists_url": "https://api.github.com/users/balasurajp/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balasurajp/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balasurajp/subscriptions",
            "organizations_url": "https://api.github.com/users/balasurajp/orgs",
            "repos_url": "https://api.github.com/users/balasurajp/repos",
            "events_url": "https://api.github.com/users/balasurajp/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balasurajp/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-10T10:49:58Z",
        "updated_at": "2023-09-10T16:59:46Z",
        "closed_at": "2023-09-10T16:59:45Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "when this method calls are performed from embedding class\r\n1. get_text_embedding()\r\n2. get_query_embedding()\r\n\r\nlast_token_usage seems to be zero.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3158/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3158/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3153",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3153/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3153/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3153/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3153",
        "id": 1703582790,
        "node_id": "I_kwDOIWuq585lipxG",
        "number": 3153,
        "title": "Issue with PrevNextNodePostprocessor",
        "user": {
            "login": "sid8491",
            "id": 8565062,
            "node_id": "MDQ6VXNlcjg1NjUwNjI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/8565062?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/sid8491",
            "html_url": "https://github.com/sid8491",
            "followers_url": "https://api.github.com/users/sid8491/followers",
            "following_url": "https://api.github.com/users/sid8491/following{/other_user}",
            "gists_url": "https://api.github.com/users/sid8491/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/sid8491/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/sid8491/subscriptions",
            "organizations_url": "https://api.github.com/users/sid8491/orgs",
            "repos_url": "https://api.github.com/users/sid8491/repos",
            "events_url": "https://api.github.com/users/sid8491/events{/privacy}",
            "received_events_url": "https://api.github.com/users/sid8491/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-10T10:15:37Z",
        "updated_at": "2023-05-10T17:11:36Z",
        "closed_at": "2023-05-10T17:11:36Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I am trying to configure the postprocessor, to get more relevant responses.\r\n\r\n```\r\n# build index\r\ndata = SimpleDirectoryReader('data/').load_data()\r\nindex = GPTVectorStoreIndex.from_documents(data)\r\n\r\n# configure retriever\r\nretriever = VectorIndexRetriever(\r\n    index=index, \r\n    similarity_top_k=3,\r\n)\r\n\r\n# configure response synthesizer\r\nresponse_synthesizer = ResponseSynthesizer.from_args(\r\n    node_postprocessors=[\r\n        SimilarityPostprocessor(similarity_cutoff=0.6),\r\n        PrevNextNodePostprocessor(docstore=data, mode='previous')\r\n    ]\r\n)\r\n```\r\n\r\nError:\r\n\r\n> ValidationError: 1 validation error for PrevNextNodePostprocessor\r\n> docstore\r\n>   instance of BaseDocumentStore expected (type=type_error.arbitrary_type; expected_arbitrary_type=BaseDocumentStore)\r\n\r\nHow do I configure PrevNextNodePostprocessor ? I need to add previous node as context.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3153/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3153/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3149",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3149/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3149/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3149/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3149",
        "id": 1703500245,
        "node_id": "I_kwDOIWuq585liVnV",
        "number": 3149,
        "title": "Issue from ./llama_index/examples/paul_graham_essay/TestEssay.ipynb",
        "user": {
            "login": "dglee-rbrain",
            "id": 132996108,
            "node_id": "U_kgDOB-1cDA",
            "avatar_url": "https://avatars.githubusercontent.com/u/132996108?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dglee-rbrain",
            "html_url": "https://github.com/dglee-rbrain",
            "followers_url": "https://api.github.com/users/dglee-rbrain/followers",
            "following_url": "https://api.github.com/users/dglee-rbrain/following{/other_user}",
            "gists_url": "https://api.github.com/users/dglee-rbrain/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dglee-rbrain/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dglee-rbrain/subscriptions",
            "organizations_url": "https://api.github.com/users/dglee-rbrain/orgs",
            "repos_url": "https://api.github.com/users/dglee-rbrain/repos",
            "events_url": "https://api.github.com/users/dglee-rbrain/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dglee-rbrain/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-10T09:30:10Z",
        "updated_at": "2023-09-10T16:59:51Z",
        "closed_at": "2023-09-10T16:59:50Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "At '[Demo] Build Tree Index during Query-Time',\r\n\r\nquery_engine = index_light.as_query_engine(\r\n    retriever_mode=\"all_leaf\",\r\n    response_mode='tree_summarize',\r\n)\r\nquery_engine.query(\"What did the author do after his time at Y Combinator?\")\r\n\r\n**TypeError: TreeAllLeafRetriever.__init__() got an unexpected keyword argument 'response_mode'**\r\n\r\nIt is saying 'response_mode' is an unexpected argument. Why does this happen?",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3149/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3149/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3148",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3148/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3148/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3148/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3148",
        "id": 1703413213,
        "node_id": "I_kwDOIWuq585liAXd",
        "number": 3148,
        "title": "follow the guide, but still get [stream is only supported for OpenAI LLMs]",
        "user": {
            "login": "xingcici",
            "id": 38656144,
            "node_id": "MDQ6VXNlcjM4NjU2MTQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/38656144?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xingcici",
            "html_url": "https://github.com/xingcici",
            "followers_url": "https://api.github.com/users/xingcici/followers",
            "following_url": "https://api.github.com/users/xingcici/following{/other_user}",
            "gists_url": "https://api.github.com/users/xingcici/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xingcici/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xingcici/subscriptions",
            "organizations_url": "https://api.github.com/users/xingcici/orgs",
            "repos_url": "https://api.github.com/users/xingcici/repos",
            "events_url": "https://api.github.com/users/xingcici/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xingcici/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-10T08:46:36Z",
        "updated_at": "2023-05-11T01:59:32Z",
        "closed_at": "2023-05-11T01:59:32Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "llama-index version = 0.6.4\r\n\r\nfollow guide :\r\n<img width=\"823\" alt=\"image\" src=\"https://github.com/jerryjliu/llama_index/assets/38656144/443dbc01-a236-48c9-90a3-2e9a4962002c\">\r\n\r\n\r\n\r\nand this is my code : \r\n![image](https://github.com/jerryjliu/llama_index/assets/38656144/336507e4-e894-4580-9c4b-9d65682b824c)\r\n\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3148/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3148/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3147",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3147/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3147/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3147/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3147",
        "id": 1703356328,
        "node_id": "I_kwDOIWuq585lhyeo",
        "number": 3147,
        "title": "Persisting a Pinecone Index",
        "user": {
            "login": "raghav-menon",
            "id": 38752811,
            "node_id": "MDQ6VXNlcjM4NzUyODEx",
            "avatar_url": "https://avatars.githubusercontent.com/u/38752811?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/raghav-menon",
            "html_url": "https://github.com/raghav-menon",
            "followers_url": "https://api.github.com/users/raghav-menon/followers",
            "following_url": "https://api.github.com/users/raghav-menon/following{/other_user}",
            "gists_url": "https://api.github.com/users/raghav-menon/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/raghav-menon/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/raghav-menon/subscriptions",
            "organizations_url": "https://api.github.com/users/raghav-menon/orgs",
            "repos_url": "https://api.github.com/users/raghav-menon/repos",
            "events_url": "https://api.github.com/users/raghav-menon/events{/privacy}",
            "received_events_url": "https://api.github.com/users/raghav-menon/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-05-10T08:11:53Z",
        "updated_at": "2023-06-11T06:58:52Z",
        "closed_at": "2023-06-11T06:58:42Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hello, \r\n\r\nCan you please let me know the procedure to persist and load the index in case I am storing the vectors in Pinecone. Would be grateful if you could point me to an example.\r\n\r\nThanks\r\nRaghav",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3147/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3147/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3146",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3146/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3146/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3146/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3146",
        "id": 1703266483,
        "node_id": "PR_kwDOIWuq585QKJOH",
        "number": 3146,
        "title": "on_event_end fix ",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-10T07:08:21Z",
        "updated_at": "2023-05-10T07:15:46Z",
        "closed_at": "2023-05-10T07:15:45Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3146",
            "html_url": "https://github.com/run-llama/llama_index/pull/3146",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3146.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3146.patch",
            "merged_at": "2023-05-10T07:15:45Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3146/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3146/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3142",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3142/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3142/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3142/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3142",
        "id": 1703144370,
        "node_id": "I_kwDOIWuq585lg-uy",
        "number": 3142,
        "title": "How to stream response of llamaindex like chatgpt",
        "user": {
            "login": "vishalp-simplecrm",
            "id": 115548851,
            "node_id": "U_kgDOBuMisw",
            "avatar_url": "https://avatars.githubusercontent.com/u/115548851?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vishalp-simplecrm",
            "html_url": "https://github.com/vishalp-simplecrm",
            "followers_url": "https://api.github.com/users/vishalp-simplecrm/followers",
            "following_url": "https://api.github.com/users/vishalp-simplecrm/following{/other_user}",
            "gists_url": "https://api.github.com/users/vishalp-simplecrm/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vishalp-simplecrm/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vishalp-simplecrm/subscriptions",
            "organizations_url": "https://api.github.com/users/vishalp-simplecrm/orgs",
            "repos_url": "https://api.github.com/users/vishalp-simplecrm/repos",
            "events_url": "https://api.github.com/users/vishalp-simplecrm/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vishalp-simplecrm/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318865,
                "node_id": "LA_kwDOIWuq588AAAABGzNfUQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5274485211,
                "node_id": "LA_kwDOIWuq588AAAABOmJB2w",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/discord",
                "name": "discord",
                "color": "E2208D",
                "default": false,
                "description": ""
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 8,
        "created_at": "2023-05-10T05:18:39Z",
        "updated_at": "2023-07-07T14:45:15Z",
        "closed_at": "2023-05-11T05:38:27Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I have created the custom bot on my own documents available in google drive , \r\nFollowing is the complete code currently getting complete response at once but my question here is how I can stream the response as like what we generally get in chatgpt \r\n\r\nPlease help thanks in advance \r\n\r\nGoogleDriveReader = download_loader('GoogleDriveReader')\r\nfolder_id = '1AuhkobVmt0Et0lIrEU0swvwavwXRtJYi'\r\nloader = GoogleDriveReader()\r\ndocuments = loader.load_data(folder_id=folder_id)\r\n\r\n#parsing documents into Nodes\r\nparser = SimpleNodeParser()\r\nnodes = parser.get_nodes_from_documents(documents)\r\n\r\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\",stream=True))\r\n\r\n# Define prompt helper\r\nmax_input_size = 3500\r\nnum_output = 500\r\nmax_chunk_overlap = 20\r\n\r\nprompt_helper = PromptHelper(max_input_size, num_output,max_chunk_overlap)\r\n# Define LLM\r\n\r\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\r\npath = 'C:\\\\Users\\\\Lenovo\\\\Desktop\\\\DocumentBot'\r\n\r\n# storage_context = StorageContext.from_defaults(persist_dir=path)\r\n# index = GPTVectorStoreIndex.from_documents(nodes, service_context=service_context )\r\n\r\n#persisting index\r\n# index.storage_context.persist()\r\n\r\n# rebuild storage context\r\nstorage_context = StorageContext.from_defaults(persist_dir=path)\r\n\r\n# load index\r\nindex = load_index_from_storage(storage_context)\r\n\r\nquery_engine = index.as_query_engine()\r\nprompt = input(\"Ask a question: \")\r\nresponse = query_engine.query(prompt)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3142/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3142/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3141",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3141/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3141/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3141/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3141",
        "id": 1703130670,
        "node_id": "PR_kwDOIWuq585QJsOY",
        "number": 3141,
        "title": "Move Redis notebook",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-10T05:03:28Z",
        "updated_at": "2023-05-10T05:03:41Z",
        "closed_at": "2023-05-10T05:03:40Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3141",
            "html_url": "https://github.com/run-llama/llama_index/pull/3141",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3141.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3141.patch",
            "merged_at": "2023-05-10T05:03:40Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3141/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3141/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3140",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3140/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3140/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3140/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3140",
        "id": 1703105567,
        "node_id": "PR_kwDOIWuq585QJm4y",
        "number": 3140,
        "title": "Fix Azure embedding issue",
        "user": {
            "login": "Hironsan",
            "id": 6737785,
            "node_id": "MDQ6VXNlcjY3Mzc3ODU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6737785?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Hironsan",
            "html_url": "https://github.com/Hironsan",
            "followers_url": "https://api.github.com/users/Hironsan/followers",
            "following_url": "https://api.github.com/users/Hironsan/following{/other_user}",
            "gists_url": "https://api.github.com/users/Hironsan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Hironsan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Hironsan/subscriptions",
            "organizations_url": "https://api.github.com/users/Hironsan/orgs",
            "repos_url": "https://api.github.com/users/Hironsan/repos",
            "events_url": "https://api.github.com/users/Hironsan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Hironsan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-10T04:30:49Z",
        "updated_at": "2023-05-10T10:21:35Z",
        "closed_at": "2023-05-10T05:20:25Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3140",
            "html_url": "https://github.com/run-llama/llama_index/pull/3140",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3140.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3140.patch",
            "merged_at": "2023-05-10T05:20:25Z"
        },
        "body": "- Fix the problems related to Azure OpenAI Embedding\r\n- Refactor `OpenAIEmbedding` class\r\n\r\nFix #3044\r\nFix #3055\r\nFix #2129 ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3140/reactions",
            "total_count": 4,
            "+1": 4,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3140/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3139",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3139/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3139/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3139/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3139",
        "id": 1703103548,
        "node_id": "PR_kwDOIWuq585QJmd1",
        "number": 3139,
        "title": "Index loading: allow setting ServiceContext other than default",
        "user": {
            "login": "xmfan",
            "id": 9547562,
            "node_id": "MDQ6VXNlcjk1NDc1NjI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/9547562?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xmfan",
            "html_url": "https://github.com/xmfan",
            "followers_url": "https://api.github.com/users/xmfan/followers",
            "following_url": "https://api.github.com/users/xmfan/following{/other_user}",
            "gists_url": "https://api.github.com/users/xmfan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xmfan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xmfan/subscriptions",
            "organizations_url": "https://api.github.com/users/xmfan/orgs",
            "repos_url": "https://api.github.com/users/xmfan/repos",
            "events_url": "https://api.github.com/users/xmfan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xmfan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-10T04:27:56Z",
        "updated_at": "2023-05-10T17:02:37Z",
        "closed_at": "2023-05-10T17:02:37Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3139",
            "html_url": "https://github.com/run-llama/llama_index/pull/3139",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3139.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3139.patch",
            "merged_at": null
        },
        "body": "ServiceContext is default constructed during Index loading, and the default constructed service is OpenAI. Currently, it is not possible to load indices that were saved from other predictors e.g. llamacpp.\r\n\r\nAlthough BaseGPTIndex does take ServiceContext as an argument: [llama_index/indices/base.py](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/base.py#LL40C15-L40C15), `load_index_from_storage` does not: [llama_index/indices/loading.py](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/loading.py#L12). \r\n\r\nProposed fix is to expose ServiceContext from `load_index_from_storage` API.\r\n\r\n## Test plan\r\n`make format`\r\n`make lint`\r\nRun existing unit test:\r\n```\r\n~/projects/llama_index loading_service_context llama_index \u276f pytest tests/indices/test_loading.py\r\n============================================================================================================ test session starts =============================================================================================================\r\nplatform darwin -- Python 3.10.11, pytest-7.2.1, pluggy-1.0.0\r\nrootdir: /Users/xmfan/projects/llama_index\r\nplugins: dotenv-0.5.2\r\ncollected 4 items\r\n\r\ntests/indices/test_loading.py ....                                                                                                                                                                                                     [100%]\r\n\r\n============================================================================================================== warnings summary ==============================================================================================================\r\ntests/indices/test_loading.py::test_load_index_from_storage_faiss_vector_store\r\n  /Users/xmfan/projects/llama_index/.venv/lib/python3.10/site-packages/faiss/loader.py:28: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\r\n    if LooseVersion(numpy.__version__) >= \"1.19\":\r\n\r\ntests/indices/test_loading.py::test_load_index_from_storage_faiss_vector_store\r\n  /Users/xmfan/projects/llama_index/.venv/lib/python3.10/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\r\n    other = LooseVersion(other)\r\n\r\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\r\n======================================================================================================= 4 passed, 2 warnings in 0.22s ========================================================================================================\r\n~/projects/llama_index loading_service_context llama_index \u276f\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3139/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3139/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3137",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3137/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3137/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3137/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3137",
        "id": 1703042517,
        "node_id": "PR_kwDOIWuq585QJZoh",
        "number": 3137,
        "title": "Fix lint for Redis",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-10T03:06:43Z",
        "updated_at": "2023-05-10T05:04:20Z",
        "closed_at": "2023-05-10T03:10:09Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3137",
            "html_url": "https://github.com/run-llama/llama_index/pull/3137",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3137.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3137.patch",
            "merged_at": "2023-05-10T03:10:09Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3137/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3137/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3136",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3136/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3136/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3136/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3136",
        "id": 1702994275,
        "node_id": "PR_kwDOIWuq585QJPPW",
        "number": 3136,
        "title": "bugfix issue 3135",
        "user": {
            "login": "Ryanglambert",
            "id": 3768892,
            "node_id": "MDQ6VXNlcjM3Njg4OTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3768892?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Ryanglambert",
            "html_url": "https://github.com/Ryanglambert",
            "followers_url": "https://api.github.com/users/Ryanglambert/followers",
            "following_url": "https://api.github.com/users/Ryanglambert/following{/other_user}",
            "gists_url": "https://api.github.com/users/Ryanglambert/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Ryanglambert/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Ryanglambert/subscriptions",
            "organizations_url": "https://api.github.com/users/Ryanglambert/orgs",
            "repos_url": "https://api.github.com/users/Ryanglambert/repos",
            "events_url": "https://api.github.com/users/Ryanglambert/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Ryanglambert/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-10T02:09:57Z",
        "updated_at": "2023-05-12T06:05:42Z",
        "closed_at": "2023-05-12T06:05:42Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3136",
            "html_url": "https://github.com/run-llama/llama_index/pull/3136",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3136.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3136.patch",
            "merged_at": "2023-05-12T06:05:42Z"
        },
        "body": "fixes issue (includes test)",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3136/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3136/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3135",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3135/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3135/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3135/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3135",
        "id": 1702975391,
        "node_id": "I_kwDOIWuq585lgVef",
        "number": 3135,
        "title": "SelectionOutputParser.parse sometimes encounters non-JSON friendly `output` value",
        "user": {
            "login": "Ryanglambert",
            "id": 3768892,
            "node_id": "MDQ6VXNlcjM3Njg4OTI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3768892?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Ryanglambert",
            "html_url": "https://github.com/Ryanglambert",
            "followers_url": "https://api.github.com/users/Ryanglambert/followers",
            "following_url": "https://api.github.com/users/Ryanglambert/following{/other_user}",
            "gists_url": "https://api.github.com/users/Ryanglambert/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Ryanglambert/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Ryanglambert/subscriptions",
            "organizations_url": "https://api.github.com/users/Ryanglambert/orgs",
            "repos_url": "https://api.github.com/users/Ryanglambert/repos",
            "events_url": "https://api.github.com/users/Ryanglambert/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Ryanglambert/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-05-10T01:43:53Z",
        "updated_at": "2023-09-10T16:59:56Z",
        "closed_at": "2023-09-10T16:59:55Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "```\r\n'\\nOutput:\\n[\\n  {\\n    \"choice\": 1,\\n    \"reason\": \"Useful for questions that seek to gain a general understanding of something.\"\\n  }\\n]'\r\n```\r\n\r\n^^^ is not json decoder friendly because there's no opening '['\r\n\r\nInstead you see \"\\nOutput:\\n\"...\r\n\r\nI could fix this within `SelectionOutputParser.parse` but maybe it should be fixed elsewhere? I'll submit PR",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3135/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3135/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3128",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3128/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3128/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3128/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3128",
        "id": 1702903782,
        "node_id": "PR_kwDOIWuq585QI8kP",
        "number": 3128,
        "title": "[version] bump version to 0.6.4",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-10T00:07:45Z",
        "updated_at": "2023-05-10T00:13:18Z",
        "closed_at": "2023-05-10T00:13:17Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3128",
            "html_url": "https://github.com/run-llama/llama_index/pull/3128",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3128.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3128.patch",
            "merged_at": "2023-05-10T00:13:17Z"
        },
        "body": "messed up the previous version tagging, figured it was better just to bump ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3128/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3128/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3127",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3127/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3127/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3127/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3127",
        "id": 1702898697,
        "node_id": "PR_kwDOIWuq585QI7dr",
        "number": 3127,
        "title": "[version] bump version to 0.6.3 ",
        "user": {
            "login": "jerryjliu",
            "id": 4858925,
            "node_id": "MDQ6VXNlcjQ4NTg5MjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4858925?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jerryjliu",
            "html_url": "https://github.com/jerryjliu",
            "followers_url": "https://api.github.com/users/jerryjliu/followers",
            "following_url": "https://api.github.com/users/jerryjliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/jerryjliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jerryjliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jerryjliu/subscriptions",
            "organizations_url": "https://api.github.com/users/jerryjliu/orgs",
            "repos_url": "https://api.github.com/users/jerryjliu/repos",
            "events_url": "https://api.github.com/users/jerryjliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jerryjliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-10T00:00:45Z",
        "updated_at": "2023-05-10T00:06:53Z",
        "closed_at": "2023-05-10T00:06:52Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3127",
            "html_url": "https://github.com/run-llama/llama_index/pull/3127",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3127.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3127.patch",
            "merged_at": "2023-05-10T00:06:52Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3127/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3127/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3126",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3126/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3126/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3126/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3126",
        "id": 1702873505,
        "node_id": "I_kwDOIWuq585lf8mh",
        "number": 3126,
        "title": "Support for Document Metadata in Query Responses ",
        "user": {
            "login": "yaameenc",
            "id": 105908864,
            "node_id": "U_kgDOBlAKgA",
            "avatar_url": "https://avatars.githubusercontent.com/u/105908864?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yaameenc",
            "html_url": "https://github.com/yaameenc",
            "followers_url": "https://api.github.com/users/yaameenc/followers",
            "following_url": "https://api.github.com/users/yaameenc/following{/other_user}",
            "gists_url": "https://api.github.com/users/yaameenc/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yaameenc/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yaameenc/subscriptions",
            "organizations_url": "https://api.github.com/users/yaameenc/orgs",
            "repos_url": "https://api.github.com/users/yaameenc/repos",
            "events_url": "https://api.github.com/users/yaameenc/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yaameenc/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4751318877,
                "node_id": "LA_kwDOIWuq588AAAABGzNfXQ",
                "url": "https://api.github.com/repos/run-llama/llama_index/labels/question",
                "name": "question",
                "color": "d876e3",
                "default": true,
                "description": "Further information is requested"
            }
        ],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-05-09T23:18:33Z",
        "updated_at": "2023-06-11T19:45:54Z",
        "closed_at": "2023-06-11T19:45:54Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hello,\r\n\r\nI'm currently using the Llama library for document search in my project. My documents are sourced from multiple files, and when I receive a response to a query, I need to know which file the response was sourced from. Essentially, I want to associate metadata (the source file) with each document and have that metadata returned with the query response.\r\n\r\nI attempted to subclass the Document class to include this metadata and used the subclass when adding my documents. However, I found that the Response object doesn't include the original Document object that matched the query.\r\n\r\nCould you advise on whether there's a built-in way to achieve this functionality? If not, would you consider adding support for document metadata in the future?\r\n\r\nAlternatively, if you could guide me on how to modify the library to suit my needs, I would greatly appreciate it.\r\n\r\nThanks in advance for your assistance!",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3126/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3126/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3125",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3125/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3125/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3125/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3125",
        "id": 1702831321,
        "node_id": "PR_kwDOIWuq585QItWJ",
        "number": 3125,
        "title": "Fix streaming",
        "user": {
            "login": "Disiok",
            "id": 5567282,
            "node_id": "MDQ6VXNlcjU1NjcyODI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5567282?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Disiok",
            "html_url": "https://github.com/Disiok",
            "followers_url": "https://api.github.com/users/Disiok/followers",
            "following_url": "https://api.github.com/users/Disiok/following{/other_user}",
            "gists_url": "https://api.github.com/users/Disiok/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Disiok/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Disiok/subscriptions",
            "organizations_url": "https://api.github.com/users/Disiok/orgs",
            "repos_url": "https://api.github.com/users/Disiok/repos",
            "events_url": "https://api.github.com/users/Disiok/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Disiok/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-09T22:22:35Z",
        "updated_at": "2023-05-10T05:04:27Z",
        "closed_at": "2023-05-10T05:04:26Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3125",
            "html_url": "https://github.com/run-llama/llama_index/pull/3125",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3125.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3125.patch",
            "merged_at": "2023-05-10T05:04:26Z"
        },
        "body": "### Summary\r\n* Fix streaming support for langchain LLMs\r\n\r\n### Details\r\n* previously we rely on `model.stream()`. This is now deprecated (and broken) in langchain, also lack chat model support.\r\n* now we adapt to langchain new callback mechanism by:\r\n  1. running the model predict in separate thread (to avoid blocking)\r\n  2. use a callback handler to pull tokens into a queue\r\n  3. return a generator to allow downstream users to iterate over tokens as they arrive\r\n\r\n### Dirty laundry\r\n* we override `__deepcopy__` for our callback handler to avoid being deepcopied in langchain callback manager code\r\n* we silently attach streaming callback handler to the LLM ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3125/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3125/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3124",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3124/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3124/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3124/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3124",
        "id": 1702797796,
        "node_id": "I_kwDOIWuq585lfqHk",
        "number": 3124,
        "title": "Fix notebook references in documentation of query transformations",
        "user": {
            "login": "cdagnino",
            "id": 3705969,
            "node_id": "MDQ6VXNlcjM3MDU5Njk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3705969?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/cdagnino",
            "html_url": "https://github.com/cdagnino",
            "followers_url": "https://api.github.com/users/cdagnino/followers",
            "following_url": "https://api.github.com/users/cdagnino/following{/other_user}",
            "gists_url": "https://api.github.com/users/cdagnino/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/cdagnino/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/cdagnino/subscriptions",
            "organizations_url": "https://api.github.com/users/cdagnino/orgs",
            "repos_url": "https://api.github.com/users/cdagnino/repos",
            "events_url": "https://api.github.com/users/cdagnino/events{/privacy}",
            "received_events_url": "https://api.github.com/users/cdagnino/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-09T21:46:58Z",
        "updated_at": "2023-09-10T17:00:00Z",
        "closed_at": "2023-09-10T16:59:59Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "https://gpt-index.readthedocs.io/en/latest/how_to/query/query_transformations.html references two notebooks that **don't exist** (triggers a 404 error)\r\n\r\n1. https://github.com/jerryjliu/llama_index/blob/main/examples/query_transformations/HyDEQueryTransformDemo.ipynb\r\n2. https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-multistep.ipynb\r\n\r\nI took a look at the examples folder in the main branch on GitHub, but there was nothing similar.\r\nI can fix the links if someone can point me to the correct notebooks (if they still exist)",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3124/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3124/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3115",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3115/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3115/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3115/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3115",
        "id": 1702305160,
        "node_id": "I_kwDOIWuq585ldx2I",
        "number": 3115,
        "title": "Migrating from v0.5 to v0.6",
        "user": {
            "login": "MarioCodetria",
            "id": 123121052,
            "node_id": "U_kgDOB1atnA",
            "avatar_url": "https://avatars.githubusercontent.com/u/123121052?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/MarioCodetria",
            "html_url": "https://github.com/MarioCodetria",
            "followers_url": "https://api.github.com/users/MarioCodetria/followers",
            "following_url": "https://api.github.com/users/MarioCodetria/following{/other_user}",
            "gists_url": "https://api.github.com/users/MarioCodetria/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/MarioCodetria/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/MarioCodetria/subscriptions",
            "organizations_url": "https://api.github.com/users/MarioCodetria/orgs",
            "repos_url": "https://api.github.com/users/MarioCodetria/repos",
            "events_url": "https://api.github.com/users/MarioCodetria/events{/privacy}",
            "received_events_url": "https://api.github.com/users/MarioCodetria/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-09T15:48:54Z",
        "updated_at": "2023-06-07T19:47:12Z",
        "closed_at": "2023-06-07T19:47:12Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi people. I'm having problems trying to migrate my scripts from version 0.5 to the latest 0.6. This is my original code for generating the indexes:\r\n\r\n```python\r\ndef construct_index(directory_path):\r\n    num_outputs = 512\r\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\", max_tokens=num_outputs))\r\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\n    docs = SimpleDirectoryReader(directory_path).load_data()\r\n    index = GPTSimpleVectorIndex.from_documents(docs, service_context=service_context)\r\n    index.save_to_disk('index-doc1.json')\r\n\r\n    return index\r\n```\r\n\r\nThis allowed me to have different files for each index. This seems to have been changed in the latest version, and now all indexes are stored in the same 3 files. I changed my code like this:\r\n\r\n```python\r\ndef construct_index(index_id, directory_path, file_list):\r\n    num_outputs = 512\r\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\", max_tokens=num_outputs))\r\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\n    docs = SimpleDirectoryReader(input_dir=directory_path, input_files=file_list).load_data()\r\n    index = GPTVectorStoreIndex.from_documents(docs, service_context=service_context)\r\n    index.set_index_id(index_id)\r\n    index.storage_context.persist('../indexes')\r\n\r\n    return index\r\n```\r\n\r\n~~But I can't find a way to have different indexes. Even if I change the index_id, every time I run \"persist\", **it overrides my previous index** with the new one. **Is there a way to have multiple indexes persisted**?~~ See EDIT2\r\n\r\nOn the other hand, I don't know how to load the indexes in the latest version. This was my original code:\r\n\r\n```python\r\ndef bot(index_path, input_text):\r\n    num_outputs = 512\r\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\", max_tokens=num_outputs))\r\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\n    index = GPTSimpleVectorIndex.load_from_disk(index_path, service_context=service_context)\r\n    response = index.query(input_text, response_mode=\"compact\")\r\n    return response.response\r\n```\r\n\r\n~~However, in the latest version I need to change \"GPTSimpleVectorIndex.load_from_disk\" with \"load_index_from_storage\", but I don't know where to specify the persist folder. **StorageContext has an argument for the persist folder, but ServiceContext doesn't**. Also, **how do I specify the index id**?~~ See EDIT1\r\n\r\n**EDIT1:** I ended up changing my query code to this:\r\n```python\r\ndef bot(input_text):\r\n    num_outputs = 512\r\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\", max_tokens=num_outputs))\r\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\n    storage_context = StorageContext.from_defaults(persist_dir='../indexs')\r\n    index = load_index_from_storage(storage_context, service_context = service_context)\r\n    query_engine = index.as_query_engine(\r\n        optimizer=SentenceEmbeddingOptimizer(percentile_cutoff=0.5),\r\n        response_mode=\"compact\"\r\n    )\r\n    response = query_engine.query(input_text)\r\n    return response.response\r\n```\r\nI added an optimizer, although it didn't improve token number or response time. ~~**I still need to find a way to have multiple indexes without overriding**~~.\r\n\r\n**EDIT2:** I added a StorageContext and now it doesn't overwrites the index anymore. I guess I had to load the existing index file first before doing persist.\r\n\r\n```python\r\ndef construct_index(index_id, directory_path, file_list):\r\n    num_outputs = 512\r\n\r\n    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\", max_tokens=num_outputs))\r\n\r\n    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\r\n    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\r\n\r\n    docs = SimpleDirectoryReader(input_dir=directory_path, input_files=file_list).load_data()\r\n    index = GPTVectorStoreIndex.from_documents(docs, service_context=service_context, storage_context=storage_context)\r\n\r\n    index.set_index_id(index_id)\r\n    index.storage_context.persist(persist_dir)\r\n\r\n    return index\r\n```\r\n\r\nSo I guess this is solved.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3115/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3115/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3103",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3103/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3103/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3103/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3103",
        "id": 1702066336,
        "node_id": "I_kwDOIWuq585lc3ig",
        "number": 3103,
        "title": "Getting an error when trying to get the root_id of the graph created",
        "user": {
            "login": "juancalric",
            "id": 113144170,
            "node_id": "U_kgDOBr5xag",
            "avatar_url": "https://avatars.githubusercontent.com/u/113144170?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/juancalric",
            "html_url": "https://github.com/juancalric",
            "followers_url": "https://api.github.com/users/juancalric/followers",
            "following_url": "https://api.github.com/users/juancalric/following{/other_user}",
            "gists_url": "https://api.github.com/users/juancalric/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/juancalric/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/juancalric/subscriptions",
            "organizations_url": "https://api.github.com/users/juancalric/orgs",
            "repos_url": "https://api.github.com/users/juancalric/repos",
            "events_url": "https://api.github.com/users/juancalric/events{/privacy}",
            "received_events_url": "https://api.github.com/users/juancalric/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-05-09T13:41:29Z",
        "updated_at": "2023-09-10T17:00:05Z",
        "closed_at": "2023-09-10T17:00:05Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "```\r\nroot_index = graph.get_index(graph.index_struct.root_id, GPTSimpleKeywordTableIndex)\r\nroot_index.index_struct.index_id = \"compare_contrast\"\r\n```\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n[<ipython-input-21-51a2b9ac45fd>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 root_index = graph.get_index(graph.index_struct.root_id, GPTSimpleKeywordTableIndex)\r\n      2 root_index.index_struct.index_id = \"compare_contrast\"\r\n\r\nAttributeError: 'KeywordTable' object has no attribute 'root_id'\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3103/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3103/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3071",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3071/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3071/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3071/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3071",
        "id": 1701535990,
        "node_id": "PR_kwDOIWuq585QEW_u",
        "number": 3071,
        "title": "Update Azure OpenAI notebook",
        "user": {
            "login": "zioproto",
            "id": 789701,
            "node_id": "MDQ6VXNlcjc4OTcwMQ==",
            "avatar_url": "https://avatars.githubusercontent.com/u/789701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zioproto",
            "html_url": "https://github.com/zioproto",
            "followers_url": "https://api.github.com/users/zioproto/followers",
            "following_url": "https://api.github.com/users/zioproto/following{/other_user}",
            "gists_url": "https://api.github.com/users/zioproto/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zioproto/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zioproto/subscriptions",
            "organizations_url": "https://api.github.com/users/zioproto/orgs",
            "repos_url": "https://api.github.com/users/zioproto/repos",
            "events_url": "https://api.github.com/users/zioproto/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zioproto/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-09T07:44:44Z",
        "updated_at": "2023-05-10T00:02:21Z",
        "closed_at": "2023-05-10T00:02:21Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3071",
            "html_url": "https://github.com/run-llama/llama_index/pull/3071",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3071.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3071.patch",
            "merged_at": "2023-05-10T00:02:21Z"
        },
        "body": "Update Azure OpenAI notebook\r\nFixes #947\r\n\r\ntested with:\r\n* langchain 0.0.161\r\n* llama_index 0.6.2\r\n* openai 0.27.5\r\n\r\nUsing `embed_batch_size=1` as discussed in #947",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3071/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3071/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3070",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3070/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3070/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3070/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3070",
        "id": 1701535967,
        "node_id": "PR_kwDOIWuq585QEW_e",
        "number": 3070,
        "title": "GPTTreeindex: limit # of active async llm calls, balance chunks per node, remove non-utf8 chars from doc_text",
        "user": {
            "login": "mesa-ai",
            "id": 24790644,
            "node_id": "MDQ6VXNlcjI0NzkwNjQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/24790644?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mesa-ai",
            "html_url": "https://github.com/mesa-ai",
            "followers_url": "https://api.github.com/users/mesa-ai/followers",
            "following_url": "https://api.github.com/users/mesa-ai/following{/other_user}",
            "gists_url": "https://api.github.com/users/mesa-ai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mesa-ai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mesa-ai/subscriptions",
            "organizations_url": "https://api.github.com/users/mesa-ai/orgs",
            "repos_url": "https://api.github.com/users/mesa-ai/repos",
            "events_url": "https://api.github.com/users/mesa-ai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mesa-ai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-05-09T07:44:42Z",
        "updated_at": "2023-06-27T18:22:22Z",
        "closed_at": "2023-06-27T18:22:21Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3070",
            "html_url": "https://github.com/run-llama/llama_index/pull/3070",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3070.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3070.patch",
            "merged_at": null
        },
        "body": "suggest two tiny improvement for building GPTTrees, for problems I ran into:\r\n\r\n- problem 1: with num_children 10 and 11 nodes on level 1, tree would build root node 1 with 10 childs, root node 2 with 1 child; performs better if balanced: root node 1 and 2 with 5 children each.\r\n- problem 2: when having many chunks, e.g. ingesting whole books, async is important for indexing performance, but hundreds of async llm api calls mean chaos (rate limits, retries, timeouts), so idea: limiting number of active async jobs as possible fix.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3070/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3070/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3064",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3064/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3064/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3064/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3064",
        "id": 1701499170,
        "node_id": "PR_kwDOIWuq585QEPTW",
        "number": 3064,
        "title": "GPTTreeindex: limit # of active async llm calls, balance chunks per node",
        "user": {
            "login": "mesa-ai",
            "id": 24790644,
            "node_id": "MDQ6VXNlcjI0NzkwNjQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/24790644?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mesa-ai",
            "html_url": "https://github.com/mesa-ai",
            "followers_url": "https://api.github.com/users/mesa-ai/followers",
            "following_url": "https://api.github.com/users/mesa-ai/following{/other_user}",
            "gists_url": "https://api.github.com/users/mesa-ai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mesa-ai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mesa-ai/subscriptions",
            "organizations_url": "https://api.github.com/users/mesa-ai/orgs",
            "repos_url": "https://api.github.com/users/mesa-ai/repos",
            "events_url": "https://api.github.com/users/mesa-ai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mesa-ai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-09T07:17:29Z",
        "updated_at": "2023-05-09T07:40:31Z",
        "closed_at": "2023-05-09T07:40:31Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3064",
            "html_url": "https://github.com/run-llama/llama_index/pull/3064",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3064.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3064.patch",
            "merged_at": null
        },
        "body": "suggest two tiny improvement for building GPTTrees, for problems I ran into:\r\n\r\n- problem 1: with num_children 10 and 11 nodes on level 1, tree would build root node 1 with 10 childs, root node 2 with 1 child; performs better if balanced: root node 1 and 2 with 5 children each.\r\n- problem 2: when having many chunks, e.g. ingesting whole books, async is important for indexing performance, but hundreds of async llm api calls mean chaos (rate limits, retries, timeouts), so idea: limiting number of active async jobs as possible fix.",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3064/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3064/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3062",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3062/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3062/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3062/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3062",
        "id": 1701478345,
        "node_id": "PR_kwDOIWuq585QEK1h",
        "number": 3062,
        "title": "build GPTTreeIndex: balance number of chunks per node, ",
        "user": {
            "login": "mesa-ai",
            "id": 24790644,
            "node_id": "MDQ6VXNlcjI0NzkwNjQ0",
            "avatar_url": "https://avatars.githubusercontent.com/u/24790644?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mesa-ai",
            "html_url": "https://github.com/mesa-ai",
            "followers_url": "https://api.github.com/users/mesa-ai/followers",
            "following_url": "https://api.github.com/users/mesa-ai/following{/other_user}",
            "gists_url": "https://api.github.com/users/mesa-ai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mesa-ai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mesa-ai/subscriptions",
            "organizations_url": "https://api.github.com/users/mesa-ai/orgs",
            "repos_url": "https://api.github.com/users/mesa-ai/repos",
            "events_url": "https://api.github.com/users/mesa-ai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mesa-ai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-09T07:02:11Z",
        "updated_at": "2023-05-09T07:09:57Z",
        "closed_at": "2023-05-09T07:09:57Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3062",
            "html_url": "https://github.com/run-llama/llama_index/pull/3062",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3062.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3062.patch",
            "merged_at": null
        },
        "body": "suggest two tiny improvement for building GPTTrees, for problems I ran into:\r\n- problem 1: with num_children 10 and 11 nodes on level 1, tree would build root node #1 with 10 childs, root node #2 with 1 child; performs better if balanced: root node 1 and 2 with 5 children each.\r\n-problem 2: when having many chunks, e.g. ingesting whole books, async is important for indexing performance, but hundreds of async llm api calls mean chaos (rate limits, retries, timeouts), so idea: limiting number of active async jobs as possible fix. ",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3062/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3062/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3055",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3055/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3055/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3055/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3055",
        "id": 1701340967,
        "node_id": "PR_kwDOIWuq585QDs0g",
        "number": 3055,
        "title": "Workaround for issue with Azure embedding requiring deployment_id or engine ",
        "user": {
            "login": "p2c2e",
            "id": 16713484,
            "node_id": "MDQ6VXNlcjE2NzEzNDg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/16713484?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/p2c2e",
            "html_url": "https://github.com/p2c2e",
            "followers_url": "https://api.github.com/users/p2c2e/followers",
            "following_url": "https://api.github.com/users/p2c2e/following{/other_user}",
            "gists_url": "https://api.github.com/users/p2c2e/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/p2c2e/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/p2c2e/subscriptions",
            "organizations_url": "https://api.github.com/users/p2c2e/orgs",
            "repos_url": "https://api.github.com/users/p2c2e/repos",
            "events_url": "https://api.github.com/users/p2c2e/events{/privacy}",
            "received_events_url": "https://api.github.com/users/p2c2e/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-09T05:08:17Z",
        "updated_at": "2023-05-10T05:20:27Z",
        "closed_at": "2023-05-10T05:20:27Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3055",
            "html_url": "https://github.com/run-llama/llama_index/pull/3055",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3055.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3055.patch",
            "merged_at": null
        },
        "body": "fixes #2129 - This issue specifically happens for Azure implementation of OpenAI.\r\n\r\nAzure OpenAI APIs work a bit differently (using deployment_name vs engine/mode/model_type for OpenAI) though they share some common logic. \r\nFix passes an 'engine' parameter (in additional to the current 'model' param) to satisfy openai expectation. For some reason the llama-index code is not passing through the engine param as it is and instead uses it as 'model' when calling \r\n`openai.Embedding.create()`",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3055/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3055/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3054",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3054/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3054/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3054/events",
        "html_url": "https://github.com/run-llama/llama_index/issues/3054",
        "id": 1701311141,
        "node_id": "I_kwDOIWuq585lZ_Kl",
        "number": 3054,
        "title": "DB-GPT: which use llama_index + langchain + vicuna build project, very cool",
        "user": {
            "login": "csunny",
            "id": 17919400,
            "node_id": "MDQ6VXNlcjE3OTE5NDAw",
            "avatar_url": "https://avatars.githubusercontent.com/u/17919400?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/csunny",
            "html_url": "https://github.com/csunny",
            "followers_url": "https://api.github.com/users/csunny/followers",
            "following_url": "https://api.github.com/users/csunny/following{/other_user}",
            "gists_url": "https://api.github.com/users/csunny/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/csunny/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/csunny/subscriptions",
            "organizations_url": "https://api.github.com/users/csunny/orgs",
            "repos_url": "https://api.github.com/users/csunny/repos",
            "events_url": "https://api.github.com/users/csunny/events{/privacy}",
            "received_events_url": "https://api.github.com/users/csunny/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-09T04:35:55Z",
        "updated_at": "2023-06-06T05:00:12Z",
        "closed_at": "2023-06-06T05:00:11Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "My project is here: https://github.com/csunny/DB-GPT\r\n\r\nSQL Generate\r\n![image](https://user-images.githubusercontent.com/17919400/236994991-9259d516-3c1e-4b8c-b652-9f1ec2cd280b.png)\r\n\r\nKnownledge based qa\r\n\r\n![image](https://user-images.githubusercontent.com/17919400/236995032-6a76b7da-4e1a-46a3-a083-95030066d98e.png)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3054/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3054/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3052",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3052/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3052/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3052/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3052",
        "id": 1701300616,
        "node_id": "PR_kwDOIWuq585QDkN6",
        "number": 3052,
        "title": "added regex to ignore in github reader",
        "user": {
            "login": "Vaunorage",
            "id": 10731763,
            "node_id": "MDQ6VXNlcjEwNzMxNzYz",
            "avatar_url": "https://avatars.githubusercontent.com/u/10731763?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Vaunorage",
            "html_url": "https://github.com/Vaunorage",
            "followers_url": "https://api.github.com/users/Vaunorage/followers",
            "following_url": "https://api.github.com/users/Vaunorage/following{/other_user}",
            "gists_url": "https://api.github.com/users/Vaunorage/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Vaunorage/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Vaunorage/subscriptions",
            "organizations_url": "https://api.github.com/users/Vaunorage/orgs",
            "repos_url": "https://api.github.com/users/Vaunorage/repos",
            "events_url": "https://api.github.com/users/Vaunorage/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Vaunorage/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-05-09T04:22:35Z",
        "updated_at": "2023-05-11T23:53:17Z",
        "closed_at": "2023-05-11T23:53:17Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3052",
            "html_url": "https://github.com/run-llama/llama_index/pull/3052",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3052.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3052.patch",
            "merged_at": null
        },
        "body": "Added simple support for ignoring files and directories based on a regex instead of exact matching to enable more flexibility in ignore files or directories",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3052/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3052/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/run-llama/llama_index/issues/3051",
        "repository_url": "https://api.github.com/repos/run-llama/llama_index",
        "labels_url": "https://api.github.com/repos/run-llama/llama_index/issues/3051/labels{/name}",
        "comments_url": "https://api.github.com/repos/run-llama/llama_index/issues/3051/comments",
        "events_url": "https://api.github.com/repos/run-llama/llama_index/issues/3051/events",
        "html_url": "https://github.com/run-llama/llama_index/pull/3051",
        "id": 1701241408,
        "node_id": "PR_kwDOIWuq585QDXL7",
        "number": 3051,
        "title": "Add DocumentSummaryRetriever",
        "user": {
            "login": "oaosman84",
            "id": 1301465,
            "node_id": "MDQ6VXNlcjEzMDE0NjU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1301465?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/oaosman84",
            "html_url": "https://github.com/oaosman84",
            "followers_url": "https://api.github.com/users/oaosman84/followers",
            "following_url": "https://api.github.com/users/oaosman84/following{/other_user}",
            "gists_url": "https://api.github.com/users/oaosman84/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/oaosman84/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/oaosman84/subscriptions",
            "organizations_url": "https://api.github.com/users/oaosman84/orgs",
            "repos_url": "https://api.github.com/users/oaosman84/repos",
            "events_url": "https://api.github.com/users/oaosman84/events{/privacy}",
            "received_events_url": "https://api.github.com/users/oaosman84/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-09T03:19:22Z",
        "updated_at": "2023-05-11T01:48:22Z",
        "closed_at": "2023-05-11T01:48:21Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": true,
        "pull_request": {
            "url": "https://api.github.com/repos/run-llama/llama_index/pulls/3051",
            "html_url": "https://github.com/run-llama/llama_index/pull/3051",
            "diff_url": "https://github.com/run-llama/llama_index/pull/3051.diff",
            "patch_url": "https://github.com/run-llama/llama_index/pull/3051.patch",
            "merged_at": null
        },
        "body": "Adds a DocumentSummaryRetriever, which takes a list of summary `Node`s, and uses the LLM to filter/score those `Node`s, returning the summaries that match the `query`.\r\n\r\nIf the interface/usage looks OK, can add examples. Here's from local notebook\r\n\r\n<img width=\"848\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1301465/236985309-aba88404-23c1-4ae5-9e5e-d2038dc1cac4.png\">\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/run-llama/llama_index/issues/3051/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/run-llama/llama_index/issues/3051/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    }
]