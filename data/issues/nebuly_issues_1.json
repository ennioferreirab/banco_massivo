[
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/355",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/355/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/355/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/355/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/355",
        "id": 1927823345,
        "node_id": "I_kwDOG1WDQc5y6D_x",
        "number": 355,
        "title": "nebullvm LICENSE and commercial use?",
        "user": {
            "login": "betogulliver",
            "id": 6104721,
            "node_id": "MDQ6VXNlcjYxMDQ3MjE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/6104721?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/betogulliver",
            "html_url": "https://github.com/betogulliver",
            "followers_url": "https://api.github.com/users/betogulliver/followers",
            "following_url": "https://api.github.com/users/betogulliver/following{/other_user}",
            "gists_url": "https://api.github.com/users/betogulliver/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/betogulliver/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/betogulliver/subscriptions",
            "organizations_url": "https://api.github.com/users/betogulliver/orgs",
            "repos_url": "https://api.github.com/users/betogulliver/repos",
            "events_url": "https://api.github.com/users/betogulliver/events{/privacy}",
            "received_events_url": "https://api.github.com/users/betogulliver/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-10-05T09:34:24Z",
        "updated_at": "2023-10-05T09:34:24Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "\r\nI could not find any LICENSE information here (github repo).\r\n\r\nWhat is the nebullvm LICENSE and how it relates to commercial use?\r\n\r\nI'm now working in a project to uses 'speedster' to find the optimal compiler/compression for a DeepLearning(DL) model (pytorch).\r\nI have some questions:\r\n\r\nQ1. can I distribute the optimized models to our client without any LICENSE restrictions for me/my-client?\r\n\r\nQ2. how does our client can run the optimized models? \r\n - can my client also use 'speedster' to 'load' the 'saved' optimized files without any LICENSE restrictions? \r\n - or should my client use the open source API and runtime libs of for example TorchRT, OpenVINO, etc directly?\r\n\r\nany help on this would be much appreciated.\r\n\r\nBTW, thank you so much  for 'nebullvm/speedster/et al'. your tools makes our optimization life a breeze!!!\r\nkeep the good work\r\n\r\n \r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/355/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/355/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/354",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/354/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/354/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/354/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/354",
        "id": 1895578047,
        "node_id": "I_kwDOG1WDQc5w_Dm_",
        "number": 354,
        "title": "[Speedster] optimize_model took 10 hours, and it's not over yet",
        "user": {
            "login": "wanglongwork",
            "id": 4540335,
            "node_id": "MDQ6VXNlcjQ1NDAzMzU=",
            "avatar_url": "https://avatars.githubusercontent.com/u/4540335?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wanglongwork",
            "html_url": "https://github.com/wanglongwork",
            "followers_url": "https://api.github.com/users/wanglongwork/followers",
            "following_url": "https://api.github.com/users/wanglongwork/following{/other_user}",
            "gists_url": "https://api.github.com/users/wanglongwork/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wanglongwork/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wanglongwork/subscriptions",
            "organizations_url": "https://api.github.com/users/wanglongwork/orgs",
            "repos_url": "https://api.github.com/users/wanglongwork/repos",
            "events_url": "https://api.github.com/users/wanglongwork/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wanglongwork/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-09-14T03:08:21Z",
        "updated_at": "2023-09-14T03:16:47Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "      class SmilingEncode(nn.Module):\r\n          \r\n          def __init__(self, model):\r\n              super().__init__()\r\n              self.model = model\r\n      \r\n          def forward(self, xT, cond2):\r\n              img = self.model.render_speedster(xT, cond2)   \r\n              return img \r\n              \r\n      xT_model = SmilingXT(model).to(device).eval()\r\n      \r\n      input_data = [((torch.randn(1, 3, 256, 256), torch.randn(1, 512)), torch.tensor([0])) for _ in range(100)]\r\n      \r\n      xT_optimized_model = optimized_model = optimize_model(\r\n        xT_model, input_data=input_data, optimization_time=\"unconstrained\", device=device)   \r\n      \r\n      save_model(xT_optimized_model, \"speedster/xT_optimized_model\")\r\n\r\n      \r\n2023-09-13 18:19:45 | INFO     | Running Speedster on GPU:1\r\n2023-09-13 18:23:05 | INFO     | Benchmark performance of original model\r\n2023-09-13 18:27:29 | INFO     | Original model latency: 2.0304924035072327 sec/iter\r\n============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\r\nverbose: False, log level: Level.ERROR\r\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\r\n\r\n2023-09-13 19:07:56 | INFO     | [1/2] Running PyTorch Optimization Pipeline\r\n2023-09-13 19:07:57 | INFO     | Optimizing with TorchScriptCompiler and q_type: None.\r\n2023-09-13 19:07:58 | WARNING  | Unable to trace model with torch.fx\r\n2023-09-13 19:07:59 | WARNING  | Optimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error LitModel is not attached to a `Trainer`.. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\r\n2023-09-13 19:08:00 | INFO     | Optimizing with TorchScriptCompiler and q_type: QuantizationType.HALF.\r\n2023-09-13 19:08:00 | WARNING  | Unable to trace model with torch.fx\r\n2023-09-13 19:08:02 | WARNING  | Optimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error LitModel is not attached to a `Trainer`.. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\r\n2023-09-13 19:08:02 | INFO     | Optimizing with PyTorchTensorRTCompiler and q_type: None.\r\n2023-09-13 19:08:04 | WARNING  | Optimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TENSOR_RT_TORCH. Got error LitModel is not attached to a `Trainer`.. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\r\n2023-09-13 19:08:05 | INFO     | Optimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.HALF.\r\n2023-09-13 19:08:07 | WARNING  | Optimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TENSOR_RT_TORCH. Got error LitModel is not attached to a `Trainer`.. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\r\n2023-09-13 19:08:08 | INFO     | [2/2] Running ONNX Optimization Pipeline\r\n2023-09-13 19:08:08 | INFO     | Optimizing with ONNXCompiler and q_type: None.\r\n2023-09-13 19:08:10 | WARNING  | TensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \r\n2023-09-13 19:12:14 | INFO     | Optimized model latency: 1.540417194366455 sec/iter\r\n2023-09-13 19:12:14 | INFO     | Optimizing with ONNXCompiler and q_type: QuantizationType.HALF.\r\n\r\n\r\n\r\n\r\n\r\n\r\nIt took 10 hours, and it's not over yet\uff0cwhy\uff1f\r\n\r\n        ",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/354/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/354/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/353",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/353/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/353/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/353/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/353",
        "id": 1860710296,
        "node_id": "I_kwDOG1WDQc5u6C-Y",
        "number": 353,
        "title": "[Speedster] TensorRt OSError: [WinError 127] The specified procedure could not be found",
        "user": {
            "login": "nemeziz69",
            "id": 54268858,
            "node_id": "MDQ6VXNlcjU0MjY4ODU4",
            "avatar_url": "https://avatars.githubusercontent.com/u/54268858?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nemeziz69",
            "html_url": "https://github.com/nemeziz69",
            "followers_url": "https://api.github.com/users/nemeziz69/followers",
            "following_url": "https://api.github.com/users/nemeziz69/following{/other_user}",
            "gists_url": "https://api.github.com/users/nemeziz69/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nemeziz69/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nemeziz69/subscriptions",
            "organizations_url": "https://api.github.com/users/nemeziz69/orgs",
            "repos_url": "https://api.github.com/users/nemeziz69/repos",
            "events_url": "https://api.github.com/users/nemeziz69/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nemeziz69/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-08-22T06:43:36Z",
        "updated_at": "2023-08-22T06:49:01Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I've installed the TensorRt manually follow this [https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-zip](url). However, when run script use `from spedster import optimize_model`, got following error:\r\n\r\n![image](https://github.com/nebuly-ai/nebuly/assets/54268858/1f015e70-cd8b-40cd-9c7c-d57a7a62da78)\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"optimize_yolo7_model.py\", line 4, in <module>\r\n    from speedster import optimize_model\r\n  File \"D:\\NAVIS_ODKI3_Nebullvm\\env\\lib\\site-packages\\speedster\\__init__.py\", line 1, in <module>\r\n    from speedster.api.functions import optimize_model  # noqa: F401\r\n  File \"D:\\NAVIS_ODKI3_Nebullvm\\env\\lib\\site-packages\\speedster\\api\\functions.py\", line 17, in <module>\r\n    from speedster.root_op import SpeedsterRootOp\r\n  File \"D:\\NAVIS_ODKI3_Nebullvm\\env\\lib\\site-packages\\speedster\\root_op.py\", line 18, in <module>\r\n    from nebullvm.operations.base import Operation\r\n  File \"D:\\NAVIS_ODKI3_Nebullvm\\env\\lib\\site-packages\\nebullvm\\operations\\base.py\", line 8, in <module>\r\n    from nebullvm.tools.utils import check_device\r\n  File \"D:\\NAVIS_ODKI3_Nebullvm\\env\\lib\\site-packages\\nebullvm\\tools\\utils.py\", line 36, in <module>\r\n    from nebullvm.tools.pytorch import (\r\n  File \"D:\\NAVIS_ODKI3_Nebullvm\\env\\lib\\site-packages\\nebullvm\\tools\\pytorch.py\", line 9, in <module>\r\n    from nebullvm.tools.diffusers import get_default_dynamic_info\r\n  File \"D:\\NAVIS_ODKI3_Nebullvm\\env\\lib\\site-packages\\nebullvm\\tools\\diffusers.py\", line 33, in <module>\r\n    from nebullvm.optional_modules.tensor_rt import fold_constants\r\n  File \"D:\\NAVIS_ODKI3_Nebullvm\\env\\lib\\site-packages\\nebullvm\\optional_modules\\tensor_rt.py\", line 4, in <module>\r\n    import tensorrt\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorrt\\__init__.py\", line 129, in <module>\r\n    ctypes.CDLL(find_lib(lib))\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\ctypes\\__init__.py\", line 373, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 127] The specified procedure could not be found\r\n```\r\n\r\nFYI, I run on CUDA version 11.8, cudNN 8.9.0, tensorrt 8.6.1\r\n\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/353/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/353/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/352",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/352/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/352/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/352/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/352",
        "id": 1858011296,
        "node_id": "I_kwDOG1WDQc5uvwCg",
        "number": 352,
        "title": "Forward Forward Algorithm Questions",
        "user": {
            "login": "and-rewsmith",
            "id": 19913741,
            "node_id": "MDQ6VXNlcjE5OTEzNzQx",
            "avatar_url": "https://avatars.githubusercontent.com/u/19913741?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/and-rewsmith",
            "html_url": "https://github.com/and-rewsmith",
            "followers_url": "https://api.github.com/users/and-rewsmith/followers",
            "following_url": "https://api.github.com/users/and-rewsmith/following{/other_user}",
            "gists_url": "https://api.github.com/users/and-rewsmith/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/and-rewsmith/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/and-rewsmith/subscriptions",
            "organizations_url": "https://api.github.com/users/and-rewsmith/orgs",
            "repos_url": "https://api.github.com/users/and-rewsmith/repos",
            "events_url": "https://api.github.com/users/and-rewsmith/events{/privacy}",
            "received_events_url": "https://api.github.com/users/and-rewsmith/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-08-20T07:24:31Z",
        "updated_at": "2023-08-23T21:27:12Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi @diegofiori, I am conducting some research for the Allen Institute on the recurrent Forward Forward model based on Hinton\u2019s approach. I am attempting to extend his work with the following:\r\n1. Inverting the objective function to be more biologically plausible, and to show more similarity with predictive coding.\r\n2. Hiding the label for the first few timesteps, playing into the concept of predictive coding. (i.e. high activations initially, followed by low activations in case of successfully predicted samples)\r\n3. Supporting sparse connectivity between layers, playing into concept of modularity / biological plausibility.\r\n4. It was unclear if Hinton actually implemented the recurrent connections, as the network diagram he provided was copied from his GLOM paper. But I did implement these connections.\r\n\r\nMy architecture performs on MNIST at about 94% test accuracy. Hinton reports that he got 99%+. I am curious, did you achieve SOTA performance on the recurrent model? If so I have some follow up questions.\r\n\r\nMy project is here:\r\nhttps://github.com/and-rewsmith/RecurrentForwardForward",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/352/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/352/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/351",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/351/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/351/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/351/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/351",
        "id": 1845670522,
        "node_id": "PR_kwDOG1WDQc5Xqlat",
        "number": 351,
        "title": "[DOCS] Fix broken links",
        "user": {
            "login": "SuperSecureHuman",
            "id": 88489071,
            "node_id": "MDQ6VXNlcjg4NDg5MDcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/88489071?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/SuperSecureHuman",
            "html_url": "https://github.com/SuperSecureHuman",
            "followers_url": "https://api.github.com/users/SuperSecureHuman/followers",
            "following_url": "https://api.github.com/users/SuperSecureHuman/following{/other_user}",
            "gists_url": "https://api.github.com/users/SuperSecureHuman/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/SuperSecureHuman/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/SuperSecureHuman/subscriptions",
            "organizations_url": "https://api.github.com/users/SuperSecureHuman/orgs",
            "repos_url": "https://api.github.com/users/SuperSecureHuman/repos",
            "events_url": "https://api.github.com/users/SuperSecureHuman/events{/privacy}",
            "received_events_url": "https://api.github.com/users/SuperSecureHuman/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-08-10T18:16:20Z",
        "updated_at": "2023-08-10T18:27:37Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/351",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/351",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/351.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/351.patch",
            "merged_at": null
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/351/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/351/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/350",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/350/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/350/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/350/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/350",
        "id": 1812107515,
        "node_id": "I_kwDOG1WDQc5sApD7",
        "number": 350,
        "title": "How to generate and perform inference for an ONNX model",
        "user": {
            "login": "dneemuth",
            "id": 51821556,
            "node_id": "MDQ6VXNlcjUxODIxNTU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/51821556?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/dneemuth",
            "html_url": "https://github.com/dneemuth",
            "followers_url": "https://api.github.com/users/dneemuth/followers",
            "following_url": "https://api.github.com/users/dneemuth/following{/other_user}",
            "gists_url": "https://api.github.com/users/dneemuth/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/dneemuth/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/dneemuth/subscriptions",
            "organizations_url": "https://api.github.com/users/dneemuth/orgs",
            "repos_url": "https://api.github.com/users/dneemuth/repos",
            "events_url": "https://api.github.com/users/dneemuth/events{/privacy}",
            "received_events_url": "https://api.github.com/users/dneemuth/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-07-19T14:40:20Z",
        "updated_at": "2023-07-22T18:42:59Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Thanks for the awesome work!\r\ncurrently, I've been struggling with an issue while working with speedster which I will lay out below:\r\n**1. I've been able to optimize onnx model ( from HuggingFace, and is based on Donut https://github.com/clovaai/donut )**\r\n\r\n**code used:** \r\n\r\n```\r\nimport numpy as np\r\nfrom speedster import optimize_model\r\nfrom speedster import save_model\r\nimport numpy as np\r\nimport torch\r\nimport os\r\n\r\nProvide input data for the model\r\ninput_data = [((np.array(torch.randn(5, 3),dtype=np.int64), np.array(torch.randn(5, 3, 1024),dtype=np.float32), ), torch.tensor([0, 1, 0, 1, 1])) for _ in range(100)]\r\n\r\nRun Speedster optimization\r\noptimized_model = optimize_model(\r\n    \"./models/onnx/decoder_model.onnx\",\r\n    input_data=input_data,\r\n    optimization_time=\"unconstrained\",\r\n    device=\"gpu:0\",\r\n    metric_drop_ths=0.8\r\n)\r\n\r\nsave_model(optimized_model, \"./models/speedster\")\r\n```\r\n\r\n**output:**\r\n```\r\n2023-07-19 14:22:43 | INFO     | Running Speedster on GPU:0\r\n2023-07-19 14:25:33 | INFO     | Benchmark performance of original model\r\n2023-07-19 14:26:10 | INFO     | Original model latency: 0.023933820724487305 sec/iter\r\n2023-07-19 14:26:11 | INFO     | [1/1] Running ONNX Optimization Pipeline\r\n2023-07-19 14:26:11 | INFO     | Optimizing with ONNXCompiler and q_type: None.\r\n2023-07-19 14:26:14 | WARNING  | TensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \r\n2023-07-19 14:26:16 | INFO     | Optimized model latency: 0.02505326271057129 sec/iter\r\n2023-07-19 14:26:16 | INFO     | Optimizing with ONNXCompiler and q_type: QuantizationType.HALF.\r\n2023-07-19 14:26:44 | INFO     | Optimized model latency: 0.3438906669616699 sec/iter\r\n2023-07-19 14:26:44 | INFO     | Optimizing with ONNXTensorRTCompiler and q_type: None.\r\n2023-07-19 14:28:18 | INFO     | Optimized model latency: 0.004456996917724609 sec/iter\r\n2023-07-19 14:28:18 | INFO     | Optimizing with ONNXTensorRTCompiler and q_type: QuantizationType.HALF.\r\n2023-07-19 14:28:51 | INFO     | Optimized model latency: 0.003861665725708008 sec/iter\r\n2023-07-19 14:28:51 | INFO     | Optimizing with ONNXTensorRTCompiler and q_type: QuantizationType.STATIC.\r\n2023-07-19 14:33:56 | INFO     | Optimized model latency: 0.004480838775634766 sec/iter\r\n\r\n[Speedster results on Tesla V100-SXM2-16GB]\r\nMetric       Original Model    Optimized Model    Improvement\r\n-----------  ----------------  -----------------  -------------\r\nbackend      NUMPY             TensorRT\r\nlatency      0.0239 sec/batch  0.0039 sec/batch   6.20x\r\nthroughput   208.91 data/sec   1294.78 data/sec   6.20x\r\nmodel size   743.98 MB         254.43 MB          -65%\r\nmetric drop                    0.5291\r\ntechniques                     fp16\r\n```\r\n\r\n2.  I am just hitting a wall when trying to perform inference. \r\n**code used:**\r\n\r\n```\r\nfrom speedster import load_model\r\nfrom nebullvm.tools.benchmark import benchmark\r\nimport numpy\r\nimport tensorflow as tf\r\n\r\noptimized_model = load_model(\"../opt/models/speedster/\")\r\nprint('speedster onnx model loaded')\r\n\r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\ndummy_input = torch.randn(1, 3, 300, 400, dtype=torch.float).to(device)\r\nprint(type(dummy_input))\r\n\r\noutput = optimized_model(dummy_input)\r\nprint(output)\r\n\r\n```\r\n**observation:**\r\n\r\n```\r\n2023-07-19 14:35:43 | WARNING  | Debug: Got extra keywords in NvidiaInferenceLearner::from_engine_path: {'class_name': 'NumpyONNXTensorRTInferenceLearner', 'module_name': 'nebullvm.operations.inference_learners.tensor_rt'}\r\nspeedster onnx model loaded\r\n<class 'torch.Tensor'>\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-9-ea33d0034b2d>](https://localhost:8080/#) in <cell line: 20>()\r\n     18 \r\n     19 # Use the accelerated version of your ONNX model in production\r\n---> 20 output = optimized_model(dummy_input)\r\n     21 print(output)\r\n\r\n5 frames\r\n[/usr/local/lib/python3.10/dist-packages/polygraphy/cuda/cuda.py](https://localhost:8080/#) in dtype(self, new)\r\n    296     def dtype(self, new):\r\n    297         self._dtype = new\r\n--> 298         self.itemsize = np.dtype(new).itemsize\r\n    299 \r\n    300     @property\r\n\r\nTypeError: Cannot interpret 'torch.float32' as a data type\r\n```\r\n\r\n**So my question would be what are the types of parameters I did to include for optimized_model() method here .  Previously, I've been passing the following to original model to get it working** \r\n\r\n```\r\ndef run_prediction(test_sample, model=model, processor=processor):\r\n    pixel_values = processor(test_sample, return_tensors=\"pt\").pixel_values\r\n    task_prompt = \"<s>\"\r\n    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\r\n    outputs = model.generate(\r\n        pixel_values.to(device),\r\n        decoder_input_ids=decoder_input_ids.to(device),\r\n        max_length=model.decoder.config.max_position_embeddings,\r\n        early_stopping=True,\r\n        pad_token_id=processor.tokenizer.pad_token_id,\r\n        eos_token_id=processor.tokenizer.eos_token_id,\r\n        use_cache=False,\r\n        num_beams=1,\r\n        bad_words_ids=[[processor.tokenizer.unk_token_id]],\r\n        return_dict_in_generate=True,\r\n    )\r\n    prediction = processor.batch_decode(outputs.sequences)[0]\r\n    prediction = processor.token2json(prediction)\r\n    return prediction \r\n```\r\n\r\nPlease let me know if you require additional information. thanks.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/350/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/350/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/349",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/349/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/349/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/349/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/349",
        "id": 1767146505,
        "node_id": "I_kwDOG1WDQc5pVIQJ",
        "number": 349,
        "title": "[ Speedster] With Hugging Face notebook code on nebulydocker/nebullvm container: RuntimeError: Expected all tensors to be on the same device",
        "user": {
            "login": "trent-s",
            "id": 32449003,
            "node_id": "MDQ6VXNlcjMyNDQ5MDAz",
            "avatar_url": "https://avatars.githubusercontent.com/u/32449003?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/trent-s",
            "html_url": "https://github.com/trent-s",
            "followers_url": "https://api.github.com/users/trent-s/followers",
            "following_url": "https://api.github.com/users/trent-s/following{/other_user}",
            "gists_url": "https://api.github.com/users/trent-s/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/trent-s/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/trent-s/subscriptions",
            "organizations_url": "https://api.github.com/users/trent-s/orgs",
            "repos_url": "https://api.github.com/users/trent-s/repos",
            "events_url": "https://api.github.com/users/trent-s/events{/privacy}",
            "received_events_url": "https://api.github.com/users/trent-s/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-06-21T09:10:29Z",
        "updated_at": "2023-07-27T07:40:45Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi!  Thank you for your continued work with this project! I would like to report a possible TensorFlow GPU configuration issue with the documented nebulydocker/nebullvm container that appears to prevent notebook code from running.\r\n\r\nI am trying to use code in the Hugging Face notebook found at\r\nhttps://github.com/nebuly-ai/nebuly/blob/main/optimization/speedster/notebooks/huggingface/Accelerate_Hugging_Face_PyTorch_BERT_with_Speedster.ipynb\r\n\r\nAnd am running in the current nebulydocker/nebullvm docker container documented at\r\nhttps://docs.nebuly.com/Speedster/installation/#optional-download-docker-images-with-frameworks-and-optimizers\r\n\r\nHere is exact Python code I am trying to run (essentially code from the notebook with a couple of diagnostic lines added.):\r\n```\r\n#!/usr/bin/python\r\nimport os\r\nimport torch\r\nfrom transformers import BertTokenizer, BertModel\r\nimport random\r\nfrom speedster import optimize_model\r\n\r\ntensorrt_path = \"/usr/local/lib/python3.8/dist-packages/tensorrt\"\r\n\r\nif os.path.exists(tensorrt_path):\r\n    os.environ['LD_LIBRARY_PATH'] += f\":{tensorrt_path}\"\r\nelse:\r\n    print(\"Unable to find TensorRT path. ONNXRuntime won't use TensorrtExecutionProvider.\")\r\n\r\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\nmodel = BertModel.from_pretrained('bert-base-uncased', torchscript=True)\r\n\r\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\nmodel.to(device).eval()\r\n\r\nsentences = [\r\n    \"Mars is the fourth planet from the Sun.\",\r\n    \"has a crust primarily composed of elements\",\r\n    \"However, it is unknown\",\r\n    \"can be viewed from Earth\",\r\n    \"It was the Romans\",\r\n]\r\n\r\nlen_dataset = 100\r\n\r\ntexts = []\r\nfor _ in range(len_dataset):\r\n    n_times = random.randint(1, 30)\r\n    texts.append(\" \".join(random.choice(sentences) for _ in range(n_times)))\r\n\r\nencoded_inputs = [tokenizer(text, return_tensors=\"pt\") for text in texts]\r\n\r\ndynamic_info = {\r\n    \"inputs\": [\r\n        {0: 'batch', 1: 'num_tokens'},\r\n        {0: 'batch', 1: 'num_tokens'},\r\n        {0: 'batch', 1: 'num_tokens'},\r\n    ],\r\n    \"outputs\": [\r\n        {0: 'batch', 1: 'num_tokens'},\r\n        {0: 'batch'},\r\n    ]\r\n}\r\n\r\noptimized_model = optimize_model(\r\n    model=model,\r\n    input_data=encoded_inputs,\r\n    optimization_time=\"constrained\",\r\n    ignore_compilers=[\"onnx_tensor_rt\",\"onnx_tvm\",\"onnxruntime\",\"tensor_rt\", \"tvm\"],\r\n    device=str(device),\r\n    dynamic_info=dynamic_info,\r\n)\r\n\r\nprint (\"Type of optimized model: \"+str(type(optimized_model)) + \" on device: \"+str(optimized_model.device))\r\n\r\nencoded_inputs = [tokenizer(text, return_tensors=\"pt\").to(device) for text in texts]\r\n\r\n# Warmup for 30 iterations\r\nfor encoded_input in encoded_inputs[:30]:\r\n    with torch.no_grad():\r\n        final_out = model(**encoded_input)\r\n\r\nprint (final_out)\r\n```\r\n\r\nJust in case it is useful, starting up the container looks like this:\r\n```\r\n$ docker run -ti --rm -v ~/data:/data -v ~/src:/src --gpus=all nebulydocker/nebullvm:latest\r\n\r\n=====================\r\n== NVIDIA TensorRT ==\r\n=====================\r\n\r\nNVIDIA Release 23.03 (build 54538654)\r\nNVIDIA TensorRT Version 8.5.3\r\nCopyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\r\n\r\nContainer image Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\r\n\r\nhttps://developer.nvidia.com/tensorrt\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nTo install Python sample dependencies, run /opt/tensorrt/python/python_setup.sh\r\n\r\nTo install the open-source samples corresponding to this TensorRT release version\r\nrun /opt/tensorrt/install_opensource.sh.  To build the open source parsers,\r\nplugins, and samples for current top-of-tree on master or a different branch,\r\nrun /opt/tensorrt/install_opensource.sh -b <branch>\r\nSee https://github.com/NVIDIA/TensorRT for more information.\r\n```\r\n\r\nAnd, this is the output that I get running the above code:\r\n```\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n2023-06-21 07:44:32.387780: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point r\r\nound-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-21 07:44:32.437353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-cri\r\ntical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-21 07:44:34.329062: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned a\r\nbove are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries\r\n for your platform.\r\nSkipping registering GPU devices...\r\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.b\r\nias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', '\r\ncls.seq_relationship.weight', 'cls.predictions.bias']\r\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequ\r\nenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassifica\r\ntion model from a BertForSequenceClassification model).\r\n2023-06-21 07:44:42 | INFO     | Running Speedster on GPU:0\r\n2023-06-21 07:44:46 | INFO     | Benchmark performance of original model\r\n2023-06-21 07:44:47 | INFO     | Original model latency: 0.011019186973571777 sec/iter\r\n============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\r\nverbose: False, log level: Level.ERROR\r\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\r\n\r\n2023-06-21 07:44:53 | INFO     | [1/2] Running PyTorch Optimization Pipeline\r\n2023-06-21 07:44:53 | INFO     | Optimizing with PytorchBackendCompiler and q_type: None.\r\n2023-06-21 07:44:54 | WARNING  | Unable to trace model with torch.fx\r\n2023-06-21 07:46:04 | INFO     | Optimized model latency: 0.007783412933349609 sec/iter\r\n2023-06-21 07:46:04 | INFO     | Optimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\r\n2023-06-21 07:46:04 | WARNING  | Unable to trace model with torch.fx\r\n2023-06-21 07:47:44 | INFO     | Optimized model latency: 0.007919073104858398 sec/iter\r\n2023-06-21 07:47:44 | INFO     | [2/2] Running ONNX Optimization Pipeline\r\n\r\n[Speedster results on Tesla V100-PCIE-32GB]\r\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\r\n\u2503 Metric      \u2503 Original Model   \u2503 Optimized Model   \u2503 Improvement   \u2503\r\n\u2523\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252b\r\n\u2503 backend     \u2503 PYTORCH          \u2503 TorchScript       \u2503               \u2503\r\n\u2503 latency     \u2503 0.0110 sec/batch \u2503 0.0078 sec/batch  \u2503 1.42x         \u2503\r\n\u2503 throughput  \u2503 90.75 data/sec   \u2503 128.48 data/sec   \u2503 1.42x         \u2503\r\n\u2503 model size  \u2503 438.03 MB        \u2503 438.35 MB         \u2503 0%            \u2503\r\n\u2503 metric drop \u2503                  \u2503 0                 \u2503               \u2503\r\n\u2503 techniques  \u2503                  \u2503 fp32              \u2503               \u2503\r\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\r\n\r\nMax speed-up with your input parameters is 1.42x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/Speedster/advanced_options/#acceleration-suggestions\r\n\r\nType of optimized model: <class 'nebullvm.operations.inference_learners.huggingface.HuggingFaceInferenceLearner'> on device: None\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /src/./sample.py:68 in <module>                                                                  \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   65 # Warmup for 30 iterations                                                                  \u2502\r\n\u2502   66 for encoded_input in encoded_inputs[:30]:                                                   \u2502\r\n\u2502   67 \u2502   with torch.no_grad():                                                                   \u2502\r\n\u2502 \u2771 68 \u2502   \u2502   final_out = model(**encoded_input)                                                  \u2502\r\n\u2502   69                                                                                             \u2502\r\n\u2502   70 print (final_out)                                                                           \u2502\r\n\u2502   71                                                                                             \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501 in _call_impl             \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:1013 in forward \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1010 \u2502   \u2502   # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x s  \u2502\r\n\u2502   1011 \u2502   \u2502   head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)          \u2502\r\n\u2502   1012 \u2502   \u2502                                                                                     \u2502\r\n\u2502 \u2771 1013 \u2502   \u2502   embedding_output = self.embeddings(                                               \u2502\r\n\u2502   1014 \u2502   \u2502   \u2502   input_ids=input_ids,                                                          \u2502\r\n\u2502   1015 \u2502   \u2502   \u2502   position_ids=position_ids,                                                    \u2502\r\n\u2502   1016 \u2502   \u2502   \u2502   token_type_ids=token_type_ids,                                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501 in _call_impl             \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:230 in forward  \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    227 \u2502   \u2502   \u2502   \u2502   token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.  \u2502\r\n\u2502    228 \u2502   \u2502                                                                                     \u2502\r\n\u2502    229 \u2502   \u2502   if inputs_embeds is None:                                                         \u2502\r\n\u2502 \u2771  230 \u2502   \u2502   \u2502   inputs_embeds = self.word_embeddings(input_ids)                               \u2502\r\n\u2502    231 \u2502   \u2502   token_type_embeddings = self.token_type_embeddings(token_type_ids)                \u2502\r\n\u2502    232 \u2502   \u2502                                                                                     \u2502\r\n\u2502    233 \u2502   \u2502   embeddings = inputs_embeds + token_type_embeddings                                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501 in _call_impl             \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1498 \u2502   \u2502   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   \u2502\r\n\u2502   1499 \u2502   \u2502   \u2502   \u2502   or _global_backward_pre_hooks or _global_backward_hooks                   \u2502\r\n\u2502   1500 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):                   \u2502\r\n\u2502 \u2771 1501 \u2502   \u2502   \u2502   return forward_call(*args, **kwargs)                                          \u2502\r\n\u2502   1502 \u2502   \u2502   # Do not call functions when jit is used                                          \u2502\r\n\u2502   1503 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                             \u2502\r\n\u2502   1504 \u2502   \u2502   backward_pre_hooks = []                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py:162 in forward                 \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   159 \u2502   \u2502   \u2502   \u2502   self.weight[self.padding_idx].fill_(0)                                     \u2502\r\n\u2502   160 \u2502                                                                                          \u2502\r\n\u2502   161 \u2502   def forward(self, input: Tensor) -> Tensor:                                            \u2502\r\n\u2502 \u2771 162 \u2502   \u2502   return F.embedding(                                                                \u2502\r\n\u2502   163 \u2502   \u2502   \u2502   input, self.weight, self.padding_idx, self.max_norm,                           \u2502\r\n\u2502   164 \u2502   \u2502   \u2502   self.norm_type, self.scale_grad_by_freq, self.sparse)                          \u2502\r\n\u2502   165                                                                                            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2210 in embedding                  \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   2207 \u2502   \u2502   #   torch.embedding_renorm_                                                       \u2502\r\n\u2502   2208 \u2502   \u2502   # remove once script supports set_grad_enabled                                    \u2502\r\n\u2502   2209 \u2502   \u2502   _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)                    \u2502\r\n\u2502 \u2771 2210 \u2502   return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)        \u2502\r\n\u2502   2211                                                                                           \u2502\r\n\u2502   2212                                                                                           \u2502\r\n\u2502   2213 def embedding_bag(                                                                        \u2502\r\n\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\r\n```\r\n\r\nAttempting to call the model appears to cause the final `RuntimeError`\r\n```\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)\r\n```\r\n\r\nThis seems like it may be related to `optimized_model.device` being `none`.\r\n\r\nJust FYI, GPU seems to be accessible on this container:\r\n```\r\n# nvidia-smi\r\nWed Jun 21 09:05:31 2023\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  Tesla V100-PCIE-32GB            Off| 00000000:AF:00.0 Off |                    0 |\r\n| N/A   33C    P0               23W / 250W|      5MiB / 32768MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-PCIE-32GB            Off| 00000000:D8:00.0 Off |                    0 |\r\n| N/A   32C    P0               24W / 250W|      5MiB / 32768MiB |      0%      Default |\r\n|                                         |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n\r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n+---------------------------------------------------------------------------------------+\r\n# python -c \"import torch; print(torch.cuda.is_available())\"\r\nTrue\r\n```\r\n\r\nThank you for looking at this.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/349/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/349/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/348",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/348/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/348/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/348/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/348",
        "id": 1766975573,
        "node_id": "I_kwDOG1WDQc5pUehV",
        "number": 348,
        "title": "Yolov8-Pose Model",
        "user": {
            "login": "saim212",
            "id": 127379892,
            "node_id": "U_kgDOB5eptA",
            "avatar_url": "https://avatars.githubusercontent.com/u/127379892?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/saim212",
            "html_url": "https://github.com/saim212",
            "followers_url": "https://api.github.com/users/saim212/followers",
            "following_url": "https://api.github.com/users/saim212/following{/other_user}",
            "gists_url": "https://api.github.com/users/saim212/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/saim212/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/saim212/subscriptions",
            "organizations_url": "https://api.github.com/users/saim212/orgs",
            "repos_url": "https://api.github.com/users/saim212/repos",
            "events_url": "https://api.github.com/users/saim212/events{/privacy}",
            "received_events_url": "https://api.github.com/users/saim212/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-06-21T07:50:32Z",
        "updated_at": "2023-06-21T07:50:32Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I successfully optimized the yolo-pose model but getting an error when I try to save the model.\r\n\r\nhere is the output of optimized model:``\r\nPytorchONNXTensorRTInferenceLearner(network_parameters=ModelParams(batch_size=1, input_infos=[<nebullvm.core.models.InputInfo object at 0x7fdc7d807370>], output_sizes=[(1, 17, 33600), (1, 65, 160, 160), (1, 65, 80, 80), (1, 65, 40, 40)], output_types=[<DataType.FLOAT32: 'float32'>, <DataType.FLOAT32: 'float32'>, <DataType.FLOAT32: 'float32'>, <DataType.FLOAT32: 'float32'>], dynamic_info=None), input_tfms=<nebullvm.tools.transformations.MultiStageTransformation object at 0x7fdc7b448cd0>, device=<nebullvm.core.models.Device object at 0x7fdc7d804fd0>, quantization_type=None)``\r\n\r\nerror while save:\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-19-c1d99eee9fe5> in <cell line: 3>()\r\n      1 from speedster import optimize_model, save_model, load_model\r\n      2 from ultralytics import YOLO\r\n----> 3 save_model(optimized_model, path=\"/content/drive/MyDrive/yolov8\")\r\n\r\n1 frames\r\n/usr/local/lib/python3.10/dist-packages/nebullvm/operations/inference_learners/tensor_rt.py in save(self, path, **kwargs)\r\n    218         serialized_engine = self.engine.serialize()\r\n    219         print(\"*********\", serialized_engine)\r\n--> 220         with open(path / NVIDIA_FILENAMES[\"engine\"], \"wb\") as fout:\r\n    221             fout.write(serialized_engine)\r\n    222         metadata = self._get_metadata(**kwargs)\r\n\r\nTypeError: a bytes-like object is required, not 'NoneType'\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/348/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/348/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/347",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/347/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/347/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/347/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/347",
        "id": 1764803865,
        "node_id": "I_kwDOG1WDQc5pMMUZ",
        "number": 347,
        "title": "torch2.0 support on speedster",
        "user": {
            "login": "lucasjinreal",
            "id": 21303438,
            "node_id": "MDQ6VXNlcjIxMzAzNDM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/21303438?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lucasjinreal",
            "html_url": "https://github.com/lucasjinreal",
            "followers_url": "https://api.github.com/users/lucasjinreal/followers",
            "following_url": "https://api.github.com/users/lucasjinreal/following{/other_user}",
            "gists_url": "https://api.github.com/users/lucasjinreal/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lucasjinreal/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lucasjinreal/subscriptions",
            "organizations_url": "https://api.github.com/users/lucasjinreal/orgs",
            "repos_url": "https://api.github.com/users/lucasjinreal/repos",
            "events_url": "https://api.github.com/users/lucasjinreal/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lucasjinreal/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-06-20T07:19:49Z",
        "updated_at": "2023-06-21T03:04:03Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/347/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/347/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/346",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/346/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/346/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/346/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/346",
        "id": 1764518810,
        "node_id": "I_kwDOG1WDQc5pLGua",
        "number": 346,
        "title": "[speedster] _dl_check_map_versions assertion error with optimize_model and ONNX compilers",
        "user": {
            "login": "trent-s",
            "id": 32449003,
            "node_id": "MDQ6VXNlcjMyNDQ5MDAz",
            "avatar_url": "https://avatars.githubusercontent.com/u/32449003?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/trent-s",
            "html_url": "https://github.com/trent-s",
            "followers_url": "https://api.github.com/users/trent-s/followers",
            "following_url": "https://api.github.com/users/trent-s/following{/other_user}",
            "gists_url": "https://api.github.com/users/trent-s/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/trent-s/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/trent-s/subscriptions",
            "organizations_url": "https://api.github.com/users/trent-s/orgs",
            "repos_url": "https://api.github.com/users/trent-s/repos",
            "events_url": "https://api.github.com/users/trent-s/events{/privacy}",
            "received_events_url": "https://api.github.com/users/trent-s/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-06-20T02:36:01Z",
        "updated_at": "2023-07-05T10:22:29Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi. Thank you for your useful work with Speedster. I would like to report an assertion error I encountered following the quickstart documentation for PyTorch.\r\n\r\nFirst off, I used the container documented at https://docs.nebuly.com/Speedster/installation/#optional-download-docker-images-with-frameworks-and-optimizers .\r\nThen I followed the PyTorch quick start code documented at https://github.com/nebuly-ai/nebuly/tree/main/optimization/speedster\r\n\r\nTest code from above URL:\r\n```\r\nimport torch\r\nimport torchvision.models as models\r\nfrom speedster import optimize_model\r\n\r\nmodel = models.resnet50()  \r\ninput_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0])) for _ in range(100)]\r\n\r\noptimized_model = optimize_model(\r\n    model, \r\n    input_data=input_data, \r\n    optimization_time=\"constrained\",\r\n    metric_drop_ths=0.05\r\n)\r\n```\r\n\r\nThis log output demonstrates the assertion error:\r\n```\r\nContainer image Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\r\n\r\nhttps://developer.nvidia.com/tensorrt\r\n\r\nVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n\r\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\nBy pulling and using the container, you accept the terms and conditions of this license:\r\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n\r\nTo install Python sample dependencies, run /opt/tensorrt/python/python_setup.sh\r\n\r\nTo install the open-source samples corresponding to this TensorRT release version\r\nrun /opt/tensorrt/install_opensource.sh.  To build the open source parsers,\r\nplugins, and samples for current top-of-tree on master or a different branch,\r\nrun /opt/tensorrt/install_opensource.sh -b <branch>\r\nSee https://github.com/NVIDIA/TensorRT for more information.\r\n\r\nroot@dc82462885fe:/# python\r\nPython 3.8.10 (default, Mar 13 2023, 10:26:41)\r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n\r\n>>> import torchvision.models as models\r\n>>> from speedster import optimize_model\r\n2023-06-20 01:35:55.736961: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-20 01:35:55.795719: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-20 01:35:57.801761: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n>>>\r\n>>> model = models.resnet50()\r\n>>> input_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0])) for _ in range(100)]\r\n>>>\r\n>>> optimized_model = optimize_model(\r\n...     model,\r\n...     input_data=input_data,\r\n...     optimization_time=\"constrained\",\r\n...     metric_drop_ths=0.05\r\n... )\r\n2023-06-20 01:36:18 | INFO     | Running Speedster on GPU:0\r\n\r\n2023-06-20 01:36:22 | INFO     | Benchmark performance of original model\r\n2023-06-20 01:36:23 | INFO     | Original model latency: 0.0042613792419433595 sec/iter\r\n============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\r\nverbose: False, log level: Level.ERROR\r\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\r\n\r\n2023-06-20 01:36:24 | INFO     | [1/2] Running PyTorch Optimization Pipeline\r\n2023-06-20 01:36:24 | INFO     | Optimizing with PytorchBackendCompiler and q_type: None.\r\n2023-06-20 01:36:34 | INFO     | Optimized model latency: 0.0032427310943603516 sec/iter\r\n2023-06-20 01:36:34 | INFO     | Optimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\r\n2023-06-20 01:36:45 | INFO     | Optimized model latency: 0.004563808441162109 sec/iter\r\n2023-06-20 01:36:45 | INFO     | Optimizing with PyTorchApacheTVMCompiler and q_type: None.\r\n2023-06-20 01:43:58 | INFO     | Optimized model latency: 0.007601022720336914 sec/iter\r\n2023-06-20 01:43:58 | INFO     | Optimizing with PyTorchApacheTVMCompiler and q_type: QuantizationType.HALF.\r\n2023-06-20 01:53:28 | INFO     | Optimized model latency: 0.008110284805297852 sec/iter\r\n2023-06-20 01:53:28 | INFO     | Optimizing with PyTorchApacheTVMCompiler and q_type: QuantizationType.DYNAMIC.\r\n2023-06-20 02:02:03 | WARNING  | The optimized model will be discarded due to poor results obtained with the given metric.\r\n2023-06-20 02:02:03 | INFO     | [2/2] Running ONNX Optimization Pipeline\r\n2023-06-20 02:02:03 | INFO     | Optimizing with ONNXCompiler and q_type: None.\r\nInconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!\r\n```\r\n\r\nI have found that if I skip ONNX related compilers that it seems to work fine:\r\n\r\ne.g. adding `ignore_compilers=[\"onnx_tensor_rt\",\"onnx_tvm\",\"onnxruntime\",\"tensor_rt\", \"tvm\"]` to `optimize_model` as shown below:\r\n```\r\nimport torch\r\nimport torchvision.models as models\r\nfrom speedster import optimize_model\r\n\r\nmodel = models.resnet50()  \r\ninput_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0])) for _ in range(100)]\r\n\r\noptimized_model = optimize_model(\r\n    model, \r\n    input_data=input_data, \r\n    optimization_time=\"constrained\",\r\n    metric_drop_ths=0.05,\r\n    ignore_compilers=[\"onnx_tensor_rt\",\"onnx_tvm\",\"onnxruntime\",\"tensor_rt\", \"tvm\"],\r\n)\r\n```\r\nSuccessful log output looks like this:\r\n```\r\nroot@dc82462885fe:/# python\r\nPython 3.8.10 (default, Mar 13 2023, 10:26:41)\r\n[GCC 9.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n\r\n>>> import torchvision.models as models\r\n>>> from speedster import optimize_model\r\n2023-06-20 02:10:52.634197: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-20 02:10:52.685749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-20 02:10:54.678411: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n>>>\r\n>>> model = models.resnet50()\r\n>>> input_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0])) for _ in range(100)]\r\n>>>\r\n>>> optimized_model = optimize_model(\r\n...     model,\r\n...     input_data=input_data,\r\n...     optimization_time=\"constrained\",\r\n...     metric_drop_ths=0.05,\r\n...     ignore_compilers=[\"onnx_tensor_rt\",\"onnx_tvm\",\"onnxruntime\",\"tensor_rt\", \"tvm\"],\r\n... )\r\n2023-06-20 02:11:01 | INFO     | Running Speedster on GPU:0\r\n2023-06-20 02:11:05 | INFO     | Benchmark performance of original model\r\n2023-06-20 02:11:06 | INFO     | Original model latency: 0.004358108043670654 sec/iter\r\n============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\r\nverbose: False, log level: Level.ERROR\r\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\r\n\r\n2023-06-20 02:11:07 | INFO     | [1/2] Running PyTorch Optimization Pipeline\r\n2023-06-20 02:11:07 | INFO     | Optimizing with PytorchBackendCompiler and q_type: None.\r\n2023-06-20 02:11:17 | INFO     | Optimized model latency: 0.0031163692474365234 sec/iter\r\n2023-06-20 02:11:17 | INFO     | Optimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\r\n2023-06-20 02:11:27 | INFO     | Optimized model latency: 0.003615856170654297 sec/iter\r\n2023-06-20 02:11:27 | INFO     | [2/2] Running ONNX Optimization Pipeline\r\n\r\n[Speedster results on Tesla V100-PCIE-32GB]\r\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\r\n\u2503 Metric      \u2503 Original Model   \u2503 Optimized Model   \u2503 Improvement   \u2503\r\n\u2523\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252b\r\n\u2503 backend     \u2503 PYTORCH          \u2503 TorchScript       \u2503               \u2503\r\n\u2503 latency     \u2503 0.0044 sec/batch \u2503 0.0031 sec/batch  \u2503 1.40x         \u2503\r\n\u2503 throughput  \u2503 229.46 data/sec  \u2503 320.89 data/sec   \u2503 1.40x         \u2503\r\n\u2503 model size  \u2503 102.56 MB        \u2503 102.63 MB         \u2503 0%            \u2503\r\n\u2503 metric drop \u2503                  \u2503 0                 \u2503               \u2503\r\n\u2503 techniques  \u2503                  \u2503 fp32              \u2503               \u2503\r\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\r\n\r\nMax speed-up with your input parameters is 1.40x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/Speedster/advanced_options/#acceleration-suggestions\r\n```\r\n\r\nThank you for your work on this!",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/346/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/346/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/345",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/345/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/345/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/345/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/345",
        "id": 1741999345,
        "node_id": "PR_kwDOG1WDQc5SNOiU",
        "number": 345,
        "title": "Use new Python API of OpenVINO",
        "user": {
            "login": "jiwaszki",
            "id": 65762745,
            "node_id": "MDQ6VXNlcjY1NzYyNzQ1",
            "avatar_url": "https://avatars.githubusercontent.com/u/65762745?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jiwaszki",
            "html_url": "https://github.com/jiwaszki",
            "followers_url": "https://api.github.com/users/jiwaszki/followers",
            "following_url": "https://api.github.com/users/jiwaszki/following{/other_user}",
            "gists_url": "https://api.github.com/users/jiwaszki/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jiwaszki/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jiwaszki/subscriptions",
            "organizations_url": "https://api.github.com/users/jiwaszki/orgs",
            "repos_url": "https://api.github.com/users/jiwaszki/repos",
            "events_url": "https://api.github.com/users/jiwaszki/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jiwaszki/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-06-05T15:14:11Z",
        "updated_at": "2023-06-05T15:14:11Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/345",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/345",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/345.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/345.patch",
            "merged_at": null
        },
        "body": "Updating OpenVINO to use new capabilities of [2023.0 release](https://pypi.org/project/openvino/2023.0.0/).\r\n\r\nApplied changes:\r\n* Remove `InferRequest` use in favor of direct `CompiledModel` calls. New class behavior create `InferRequest` on demand when inference is run for the first time and holds it internally. [More info here](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_Python_API_inference.html#direct-inference-with-compiledmodel).\r\n* Use of new `shared_memory` mode to limit copies of input data. If project is implemented in a manner to run sequentially, it can be left as `True`. If not, the new mode can be exposed as additional flag to the wrapper method. [More on the feature here](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_Python_API_inference.html#shared-memory-on-inputs).\r\n(note to maintainers: requires benchmarking on project's side)\r\n* Address resulting dictionary directly with names. [More on `OVDict` here](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_Python_API_exclusives.html#inference-results-ovdict).\r\n\r\nThanks in advance for the review!",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/345/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/345/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/344",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/344/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/344/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/344/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/344",
        "id": 1730992939,
        "node_id": "PR_kwDOG1WDQc5RnoOb",
        "number": 344,
        "title": "[ChatLLaMA] Add flan-t5-xl support for local and API model to generate synthetic reward_training_data scores",
        "user": {
            "login": "Linus-J",
            "id": 74065299,
            "node_id": "MDQ6VXNlcjc0MDY1Mjk5",
            "avatar_url": "https://avatars.githubusercontent.com/u/74065299?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Linus-J",
            "html_url": "https://github.com/Linus-J",
            "followers_url": "https://api.github.com/users/Linus-J/followers",
            "following_url": "https://api.github.com/users/Linus-J/following{/other_user}",
            "gists_url": "https://api.github.com/users/Linus-J/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Linus-J/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Linus-J/subscriptions",
            "organizations_url": "https://api.github.com/users/Linus-J/orgs",
            "repos_url": "https://api.github.com/users/Linus-J/repos",
            "events_url": "https://api.github.com/users/Linus-J/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Linus-J/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-29T16:07:27Z",
        "updated_at": "2023-05-29T17:48:23Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/344",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/344",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/344.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/344.patch",
            "merged_at": null
        },
        "body": "Added an optional argument `-l` / `--local_directory` to allow the usage of locally stored google/flan-t5-xl model from HuggingFace.\r\n\r\nAdded flan-t5-xl as a valid input option for the `-m` / `--model` argument. When used as just `-m flan-t5-xl`, the model can be accessed via a HuggingFace API key similar to the davinci model. If used with the option `-l /path_to_locally_stored_flan-t5-xl/`, the local model will be used to update the reward scores instead.\r\n\r\nThis commit resolves issues #218 #221 #241.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/344/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/344/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/343",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/343/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/343/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/343/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/343",
        "id": 1729651494,
        "node_id": "I_kwDOG1WDQc5nGGMm",
        "number": 343,
        "title": "Evaluating accuracy of only the reward model",
        "user": {
            "login": "HannahKirk",
            "id": 22522221,
            "node_id": "MDQ6VXNlcjIyNTIyMjIx",
            "avatar_url": "https://avatars.githubusercontent.com/u/22522221?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/HannahKirk",
            "html_url": "https://github.com/HannahKirk",
            "followers_url": "https://api.github.com/users/HannahKirk/followers",
            "following_url": "https://api.github.com/users/HannahKirk/following{/other_user}",
            "gists_url": "https://api.github.com/users/HannahKirk/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/HannahKirk/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/HannahKirk/subscriptions",
            "organizations_url": "https://api.github.com/users/HannahKirk/orgs",
            "repos_url": "https://api.github.com/users/HannahKirk/repos",
            "events_url": "https://api.github.com/users/HannahKirk/events{/privacy}",
            "received_events_url": "https://api.github.com/users/HannahKirk/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-28T20:12:49Z",
        "updated_at": "2023-05-28T20:12:49Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I have a trained reward model at './models/reward/opt-125m.pt'. How can I load this model and use it for inference only? I only want to check the accuracy of the reward model on held-out test data.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/343/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/343/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/342",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/342/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/342/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/342/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/342",
        "id": 1718058546,
        "node_id": "I_kwDOG1WDQc5mZ34y",
        "number": 342,
        "title": "yolov8 + nebuly | AttributeError: type object 'DummyClass' has no attribute 'models'",
        "user": {
            "login": "scraus",
            "id": 91582176,
            "node_id": "U_kgDOBXVu4A",
            "avatar_url": "https://avatars.githubusercontent.com/u/91582176?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/scraus",
            "html_url": "https://github.com/scraus",
            "followers_url": "https://api.github.com/users/scraus/followers",
            "following_url": "https://api.github.com/users/scraus/following{/other_user}",
            "gists_url": "https://api.github.com/users/scraus/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/scraus/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/scraus/subscriptions",
            "organizations_url": "https://api.github.com/users/scraus/orgs",
            "repos_url": "https://api.github.com/users/scraus/repos",
            "events_url": "https://api.github.com/users/scraus/events{/privacy}",
            "received_events_url": "https://api.github.com/users/scraus/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 10,
        "created_at": "2023-05-20T05:29:30Z",
        "updated_at": "2023-06-19T12:38:00Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I tried reproducing the results in this [notebook](https://github.com/nebuly-ai/nebuly/blob/main/optimization/speedster/notebooks/pytorch/Accelerate_PyTorch_YOLOv8_with_Speedster.ipynb) but I got the following errors:\r\n\r\n![image](https://github.com/nebuly-ai/nebuly/assets/91582176/5c8eba9d-380b-45ec-8c11-28a99869d625)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/342/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/342/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/341",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/341/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/341/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/341/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/341",
        "id": 1707204237,
        "node_id": "I_kwDOG1WDQc5lwd6N",
        "number": 341,
        "title": "[Speedster] Optimization failed with PytorchBackendCompiler",
        "user": {
            "login": "nemeziz69",
            "id": 54268858,
            "node_id": "MDQ6VXNlcjU0MjY4ODU4",
            "avatar_url": "https://avatars.githubusercontent.com/u/54268858?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/nemeziz69",
            "html_url": "https://github.com/nemeziz69",
            "followers_url": "https://api.github.com/users/nemeziz69/followers",
            "following_url": "https://api.github.com/users/nemeziz69/following{/other_user}",
            "gists_url": "https://api.github.com/users/nemeziz69/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/nemeziz69/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/nemeziz69/subscriptions",
            "organizations_url": "https://api.github.com/users/nemeziz69/orgs",
            "repos_url": "https://api.github.com/users/nemeziz69/repos",
            "events_url": "https://api.github.com/users/nemeziz69/events{/privacy}",
            "received_events_url": "https://api.github.com/users/nemeziz69/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-05-12T08:36:05Z",
        "updated_at": "2023-05-16T01:51:54Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "May I know what is the reason behind the warning message?\r\n\r\nFYI, my input data is Pytorch Dataloader, which was constructed in tensor.\r\n\r\n2023-05-12 16:16:31 | INFO     | [1/2] Running PyTorch Optimization Pipeline\r\n2023-05-12 16:16:31 | INFO     | Optimizing with PytorchBackendCompiler and q_type: None.\r\n2023-05-12 16:16:33 | WARNING  | Optimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error 'list' object has no attribute 'to'. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\r\n2023-05-12 16:16:33 | INFO     | Optimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\r\n2023-05-12 16:16:35 | WARNING  | Optimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error 'list' object has no attribute 'to'. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/341/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/341/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/340",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/340/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/340/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/340/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/340",
        "id": 1691073739,
        "node_id": "PR_kwDOG1WDQc5Pg-hr",
        "number": 340,
        "title": "Standford SHP reward dataset",
        "user": {
            "login": "MattiaSangermano",
            "id": 43407984,
            "node_id": "MDQ6VXNlcjQzNDA3OTg0",
            "avatar_url": "https://avatars.githubusercontent.com/u/43407984?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/MattiaSangermano",
            "html_url": "https://github.com/MattiaSangermano",
            "followers_url": "https://api.github.com/users/MattiaSangermano/followers",
            "following_url": "https://api.github.com/users/MattiaSangermano/following{/other_user}",
            "gists_url": "https://api.github.com/users/MattiaSangermano/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/MattiaSangermano/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/MattiaSangermano/subscriptions",
            "organizations_url": "https://api.github.com/users/MattiaSangermano/orgs",
            "repos_url": "https://api.github.com/users/MattiaSangermano/repos",
            "events_url": "https://api.github.com/users/MattiaSangermano/events{/privacy}",
            "received_events_url": "https://api.github.com/users/MattiaSangermano/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-05-01T17:06:21Z",
        "updated_at": "2023-05-01T17:06:21Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/340",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/340",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/340.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/340.patch",
            "merged_at": null
        },
        "body": "Following #224, I wrote a first draft of the metrics to compute the rewards of the Standford SHP dataset. If it's fine with you for now I would focus on the effectiveness of the metrics and once we find a suitable one I will tailor the code to be in line with the rest of the codebase. The code is consistent with the rest of the repo, so to get the rewards just run the command:    \r\n```python\r\npython artifacts/download_dataset.py SHPReward --path <path_to_folder_for_download> --number_of_samples <N>\r\n```\r\n\r\n\r\nThe reward distribution:\r\n\r\n0:   0.336162 %\r\n1:    0.194715 %\r\n2:   0.149592 %\r\n3:    0.083746 % \r\n4:    0.037445 %\r\n5:    0.198339 %\r\n\r\nSome samples:\r\n```txt\r\n\"user_input\": \"[DC] Is the Flash able to run forever without getting tired, or would he eventually reach a point of physical and/or mental \r\n\"completion\": \"The only version of Flash that never stops running is the one in Kingdom Come as far as I know.\",\r\n\"reward\": 0\r\n\r\n\"user_input\": \"I can partially understand Middle English readings of Chaucer, while Old English sounds entirely foreign to me. How easily would a speaker of Middle English (14th century) be able to understand an Old English reading of Beowulf (9th century)?\",\r\n\"completion\": \"Can you link me to your middle english reading? I'd like to try it\",\r\n\"reward\": 1\r\n\r\n\"user_input\": \"What have you learned from experience that a culinary school probably cannot teach? I created this account for a school project and would like to hear what everyone here has learned, that a culinary class cannot simulate or teach.\",\r\n\"completion\": \"Timing, and getting the feel for the rhythm in the kitchen. Especially when it comes to working as a team.\",\r\n\"reward\": 2\r\n\r\n\"user_input\": \"Why can't mute people speak? Can they make oral sounds? (Like screaming, humming, moaning) I don't mean to be rude.\",\r\n\"completion\": \"I didn't see it posted here yet, but there is also a form called selective mutism. You see it often w autism. It's basically an extreme anxiety of people hearing your voice. These individuals can talk, just choose not to. Applied Behavior Analysis can intervene can help these individuals overcome this.\",\r\n\"reward\": 3\r\n\r\n\"user_input\": \"CMV: God didn't stop humans from sinning, and so Christians have no business using God's will to justify trying to do the same According to the Bible, God didn't prevent Adam and Eve from taking the fruit from the Tree of the knowledge of good and evil, nor he prevented the serpent from entering the Garden of Eden. While humans were told not to sin, there were no obstacles to the sin. This was probably to allow the humans to exercise their freedom of will, which is an important idea in Christianity.  It should therefore be wrong to appeal to God's will while attempting to restrict people from sinning, e.g. by banning abortion. It just goes against the \\u201cspirit\\u201d of Genesis. CMV!  P.S. Don't get me wrong, nothing prevents us from outlawing bad things per se, there are other sources of law.  P.P.S. An obvious counterargument to this would be many examples of God punishing people for their sins. I honestly don't know what to make of this, but most Christians I talked to still consider this \\u201cfree will\\u201d\\u2014after all, you still can sin? Anyways, this doesn't seem to be voiding the idea of free will, so unless there's more to it, I'll leave this aside for the time being.\",\r\n\"completion\": \"We can still believe that murder should be punished/prevented, right?  Why not abortion for the same reasons since many christians view it as murder?  Christians don't keep their secular morality in one box and say, \\\"Okay these things are objectively immoral and can be punished\\\" and their religious morality in another box.  Their religion informs their morality.  You don't even have to be christian to believe abortion is wrong, so I wouldn't even say it must be in the religious morality box anyway.  There are religious moralities that are personal, such as Jewish people eating Kosher foods with no desire to push other people into eating Kosher foods, but that just doesn't apply to things where people are being mistreated, such as murdering fetuses.  >  It just goes against the \\u201cspirit\\u201d of Genesis.  Is jailing people for homicide against the spirit of Genesis?  Not murdering is one of the 10 commandments and is absolutely a sin too.\",\r\n\"reward\": 4\r\n\r\n\"user_input\": \"How can I avoid taking antibiotics so often when seeing several different kinds of doctors (dermatologist, rheumatologist, gynecologist, hematologist)?  [28][F] Height: 5'2 Weight: 119lbs Race/Ethnicity: Hispanic  Primary Concern: concerned about the number of antibiotics I've taken and unsure how to coordinate between doctors  Existing Medical Issues:  Slight anemia [Low Ferritin (6ng/mL 01/23/20) hemoglobin (10.5g/dL 01/23/20)], undifferentiated connective tissue disease, celiac, and PCOS  Current Medication:  Doxcycline 100mg 2x a day and clindamycin 1% topical (started 06/12/20), Zoloft 100mg 1x a day, and Adderall 20mg 2x a day, gummy multivitamin 1x a day, Zyrtec 1x a day,   Drugs / Alcohol: alcohol (1-2x a month), nicotine 3mg daily (vape)   I've taken antibiotics 6-8 times a year for different infections (UTI, BV, skin infections, strep, etc). So the gynecologist will prescribe levofloxacin for reoccurring UTI's  hoping to knock them out for good about a week after the dermatologist prescribes doxycycline and topical clindamycin for cysts.  This is after taking minocycline (sp) for a month last year (for the same cysts) and two years of taking either nitrofurantoin or cephalexin and later levofloxacin (depending on whether it's klebsiella, staph, or e. coli) for UTI's.  What happens when I get a serious infection and nothing helps because I've taken everything twice over?  I don't want to appear difficult or give off the impression that I don't trust them by questioning their recommended treatment (no matter how chill I feel, I get the impression my questions are viewed as dramatic or ridiculous). No offense, but I'm hoping to have fewer appointments and see fewer specialists.   It feels as though I am adding risk every time I step into an office or complete a round of antibiotics.   I don't want to take two different antibiotics if one would work just fine.  **How can I avoid taking too many antibiotics when seeing all of these different doctors?**\",\r\n\"completion\": \"I assume you are in the USA. Your primary care provider should be monitoring all this but, many do not.  Pharmacists often have a better knowledge of medications than doctors.  It might be a good idea to talk to your pharmacist -- be aware that not everyone who works in the pharmacy is not a pharmacist: ask to speak to a pharmacist.  You can also see if your insurance will cover a 'patient advocate' or 'patient co-ordinator' or 'patient consierge': they position has different names at different hospitals.  This person should help you deal with communication between doctors including worries about over medication.\",\r\n\"reward\": 5\r\n```\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/340/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/340/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/339",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/339/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/339/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/339/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/339",
        "id": 1688501510,
        "node_id": "PR_kwDOG1WDQc5PYi_q",
        "number": 339,
        "title": "Fix wrong imports",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-04-28T12:50:08Z",
        "updated_at": "2023-06-18T10:37:27Z",
        "closed_at": "2023-06-18T10:37:27Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/339",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/339",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/339.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/339.patch",
            "merged_at": "2023-06-18T10:37:26Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/339/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/339/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/338",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/338/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/338/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/338/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/338",
        "id": 1688064074,
        "node_id": "I_kwDOG1WDQc5kndBK",
        "number": 338,
        "title": "[chatllama]Puzzled about the update of the critic model",
        "user": {
            "login": "zhuweipg99",
            "id": 24365664,
            "node_id": "MDQ6VXNlcjI0MzY1NjY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/24365664?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zhuweipg99",
            "html_url": "https://github.com/zhuweipg99",
            "followers_url": "https://api.github.com/users/zhuweipg99/followers",
            "following_url": "https://api.github.com/users/zhuweipg99/following{/other_user}",
            "gists_url": "https://api.github.com/users/zhuweipg99/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zhuweipg99/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zhuweipg99/subscriptions",
            "organizations_url": "https://api.github.com/users/zhuweipg99/orgs",
            "repos_url": "https://api.github.com/users/zhuweipg99/repos",
            "events_url": "https://api.github.com/users/zhuweipg99/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zhuweipg99/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-04-28T07:41:44Z",
        "updated_at": "2023-04-28T09:30:32Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "When I looked into the **compute the value loss** in trainer.py line [1012-1017](https://github.com/nebuly-ai/nebuly/blob/e104539a3355dc5ef3b62de23d756d44e9d02026/apps/accelerate/chatllama/chatllama/rlhf/trainer.py#L1012),\r\n```\r\nvalue_loss_clipped = old_values + (values - old_values).clamp(-critic_eps_clip, critic_eps_clip)\r\nvalue_loss1 = (value_loss_clipped - rewards) ** 2\r\nvalue_loss2 = (values - rewards) ** 2\r\nvalue_loss = torch.max(value_loss1, value_loss2).mean()\r\n```\r\nI think the **values and rewards** are equal to the **old_values**, cause they use the **same model** to compute the score.\r\nI will be very grateful if you guys can answer my confuse.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/338/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/338/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/337",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/337/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/337/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/337/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/337",
        "id": 1673204326,
        "node_id": "I_kwDOG1WDQc5juxJm",
        "number": 337,
        "title": "[Chatllama] what's supposed to be in the Actor checkpoint dir?",
        "user": {
            "login": "StrangeTcy",
            "id": 2532099,
            "node_id": "MDQ6VXNlcjI1MzIwOTk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2532099?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/StrangeTcy",
            "html_url": "https://github.com/StrangeTcy",
            "followers_url": "https://api.github.com/users/StrangeTcy/followers",
            "following_url": "https://api.github.com/users/StrangeTcy/following{/other_user}",
            "gists_url": "https://api.github.com/users/StrangeTcy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/StrangeTcy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/StrangeTcy/subscriptions",
            "organizations_url": "https://api.github.com/users/StrangeTcy/orgs",
            "repos_url": "https://api.github.com/users/StrangeTcy/repos",
            "events_url": "https://api.github.com/users/StrangeTcy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/StrangeTcy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-04-18T14:02:03Z",
        "updated_at": "2023-04-19T10:16:10Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I'm following the example from your README, and it works like this:\r\nmy `config.yaml` has these line for the actor:\r\n```\r\nactor_config:\r\n  model: \"decapoda-research/llama-7b-hf\"\r\n  model_folder: \"./models\"\r\n  ```\r\nThen I run `python artifacts/main.py artifacts/config/config.yaml --type ACTOR`.\r\n`main.py` calls `ActorTrainer` from `actor.py`, which calls `load_model` from `llama_model.py`, which calls `load_checkpoints`, which expects to find `*.pth` files and a `params.json` file in my `ckpt_dir` which is `./models`. My `models` folder has neither . I wonder where these file have to come from.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/337/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/337/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/336",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/336/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/336/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/336/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/336",
        "id": 1669310240,
        "node_id": "PR_kwDOG1WDQc5OYdZJ",
        "number": 336,
        "title": "pegasus speedster notebook added",
        "user": {
            "login": "VinishUchiha",
            "id": 25880763,
            "node_id": "MDQ6VXNlcjI1ODgwNzYz",
            "avatar_url": "https://avatars.githubusercontent.com/u/25880763?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/VinishUchiha",
            "html_url": "https://github.com/VinishUchiha",
            "followers_url": "https://api.github.com/users/VinishUchiha/followers",
            "following_url": "https://api.github.com/users/VinishUchiha/following{/other_user}",
            "gists_url": "https://api.github.com/users/VinishUchiha/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/VinishUchiha/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/VinishUchiha/subscriptions",
            "organizations_url": "https://api.github.com/users/VinishUchiha/orgs",
            "repos_url": "https://api.github.com/users/VinishUchiha/repos",
            "events_url": "https://api.github.com/users/VinishUchiha/events{/privacy}",
            "received_events_url": "https://api.github.com/users/VinishUchiha/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-04-15T11:00:16Z",
        "updated_at": "2023-04-15T11:00:16Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/336",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/336",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/336.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/336.patch",
            "merged_at": null
        },
        "body": "I added Google's PEGASUS(an Encoder-Decoder Architecture) model speedster optimization notebook",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/336/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/336/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/335",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/335/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/335/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/335/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/335",
        "id": 1665925015,
        "node_id": "I_kwDOG1WDQc5jS_-X",
        "number": 335,
        "title": "[chatllama]How models enable inference",
        "user": {
            "login": "yangzhipeng1108",
            "id": 33448441,
            "node_id": "MDQ6VXNlcjMzNDQ4NDQx",
            "avatar_url": "https://avatars.githubusercontent.com/u/33448441?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yangzhipeng1108",
            "html_url": "https://github.com/yangzhipeng1108",
            "followers_url": "https://api.github.com/users/yangzhipeng1108/followers",
            "following_url": "https://api.github.com/users/yangzhipeng1108/following{/other_user}",
            "gists_url": "https://api.github.com/users/yangzhipeng1108/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yangzhipeng1108/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yangzhipeng1108/subscriptions",
            "organizations_url": "https://api.github.com/users/yangzhipeng1108/orgs",
            "repos_url": "https://api.github.com/users/yangzhipeng1108/repos",
            "events_url": "https://api.github.com/users/yangzhipeng1108/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yangzhipeng1108/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-04-13T08:03:47Z",
        "updated_at": "2023-04-14T13:46:20Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/335/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/335/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/334",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/334/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/334/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/334/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/334",
        "id": 1664604403,
        "node_id": "PR_kwDOG1WDQc5OIuvg",
        "number": 334,
        "title": "Speedster refactor, add support for TPUs and AWS Inferentia chips",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-04-12T13:46:31Z",
        "updated_at": "2023-04-24T11:34:43Z",
        "closed_at": "2023-04-24T11:34:43Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/334",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/334",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/334.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/334.patch",
            "merged_at": "2023-04-24T11:34:43Z"
        },
        "body": "This PR brings the following changes:\r\n\r\nadd support for Inferentia chips and TPUs for PyTorch models.\r\nadd support for PyTorch 2.0 and TensorFlow 2.12\r\nspeedster refactor to be used from cloud_surfer",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/334/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/334/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/333",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/333/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/333/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/333/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/333",
        "id": 1664576950,
        "node_id": "PR_kwDOG1WDQc5OIozr",
        "number": 333,
        "title": "Speedster refactor, add support for TPUs and AWS Inferentia chips",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-04-12T13:30:55Z",
        "updated_at": "2023-04-12T13:45:34Z",
        "closed_at": "2023-04-12T13:45:34Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/333",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/333",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/333.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/333.patch",
            "merged_at": null
        },
        "body": "This PR brings the following changes: \r\n- add support for Inferentia chips and TPUs for PyTorch models.\r\n- add support for PyTorch 2.0 and TensorFlow 2.12\r\n- speedster refactor to be used from cloud_surfer",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/333/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/333/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/332",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/332/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/332/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/332/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/332",
        "id": 1659589286,
        "node_id": "PR_kwDOG1WDQc5N4ET0",
        "number": 332,
        "title": "ViT pytorch speedster notebook added",
        "user": {
            "login": "VinishUchiha",
            "id": 25880763,
            "node_id": "MDQ6VXNlcjI1ODgwNzYz",
            "avatar_url": "https://avatars.githubusercontent.com/u/25880763?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/VinishUchiha",
            "html_url": "https://github.com/VinishUchiha",
            "followers_url": "https://api.github.com/users/VinishUchiha/followers",
            "following_url": "https://api.github.com/users/VinishUchiha/following{/other_user}",
            "gists_url": "https://api.github.com/users/VinishUchiha/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/VinishUchiha/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/VinishUchiha/subscriptions",
            "organizations_url": "https://api.github.com/users/VinishUchiha/orgs",
            "repos_url": "https://api.github.com/users/VinishUchiha/repos",
            "events_url": "https://api.github.com/users/VinishUchiha/events{/privacy}",
            "received_events_url": "https://api.github.com/users/VinishUchiha/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-04-08T17:26:32Z",
        "updated_at": "2023-04-12T07:45:42Z",
        "closed_at": "2023-04-12T07:45:42Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/332",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/332",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/332.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/332.patch",
            "merged_at": "2023-04-12T07:45:42Z"
        },
        "body": "I added a new notebook demonstrating how to optimize PyTorch VisionTransformer using Speedster.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/332/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/332/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/331",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/331/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/331/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/331/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/331",
        "id": 1656677506,
        "node_id": "I_kwDOG1WDQc5ivuSC",
        "number": 331,
        "title": "Issues with accelerate and deepspeed training",
        "user": {
            "login": "swang99",
            "id": 35492791,
            "node_id": "MDQ6VXNlcjM1NDkyNzkx",
            "avatar_url": "https://avatars.githubusercontent.com/u/35492791?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/swang99",
            "html_url": "https://github.com/swang99",
            "followers_url": "https://api.github.com/users/swang99/followers",
            "following_url": "https://api.github.com/users/swang99/following{/other_user}",
            "gists_url": "https://api.github.com/users/swang99/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/swang99/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/swang99/subscriptions",
            "organizations_url": "https://api.github.com/users/swang99/orgs",
            "repos_url": "https://api.github.com/users/swang99/repos",
            "events_url": "https://api.github.com/users/swang99/events{/privacy}",
            "received_events_url": "https://api.github.com/users/swang99/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-04-06T05:12:21Z",
        "updated_at": "2023-04-21T08:09:30Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I've been having issues trying to distribute training onto multiple GPUs. Even after following this pull request https://github.com/nebuly-ai/nebullvm/pull/316, I check the nvidia-smi log and it still shows all the load being on one GPU, whether using deepspeed or accelerate.\r\n\r\nHere are the commands I tried to train the actor models:\r\n`deepspeed artifacts/main.py artifacts/config/config.yaml --type RL`\r\n`accelerate launch artifacts/main.py artifacts/config/config.yaml --type RL`\r\nEverything else I simply kept as default configuration and did not touch anything else.\r\nAny ideas? Thank you. \r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/331/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/331/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/330",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/330/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/330/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/330/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/330",
        "id": 1656657129,
        "node_id": "PR_kwDOG1WDQc5Nuwwt",
        "number": 330,
        "title": "[ChatLLaMA] Reward Dataset Score Calculation",
        "user": {
            "login": "shrinath-suresh",
            "id": 63862647,
            "node_id": "MDQ6VXNlcjYzODYyNjQ3",
            "avatar_url": "https://avatars.githubusercontent.com/u/63862647?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shrinath-suresh",
            "html_url": "https://github.com/shrinath-suresh",
            "followers_url": "https://api.github.com/users/shrinath-suresh/followers",
            "following_url": "https://api.github.com/users/shrinath-suresh/following{/other_user}",
            "gists_url": "https://api.github.com/users/shrinath-suresh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shrinath-suresh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shrinath-suresh/subscriptions",
            "organizations_url": "https://api.github.com/users/shrinath-suresh/orgs",
            "repos_url": "https://api.github.com/users/shrinath-suresh/repos",
            "events_url": "https://api.github.com/users/shrinath-suresh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shrinath-suresh/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-04-06T04:49:14Z",
        "updated_at": "2023-04-13T08:06:01Z",
        "closed_at": "2023-04-13T08:06:00Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/330",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/330",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/330.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/330.patch",
            "merged_at": "2023-04-13T08:06:00Z"
        },
        "body": "While training the reward model using the dataset given in the README instructions, i observed that , some of the scores are getting passed to `RewardDataset` as `None`\r\n\r\nDue to this, the training fails while converting the score to float - [line](https://github.com/nebuly-ai/nebullvm/blob/87c6d132a37deea52aacb5e90324e74742970739/apps/accelerate/chatllama/chatllama/rlhf/reward.py#L226)\r\n\r\nShould we set the score to number  ( 0 or 2.5 ) or should we raise an error that the score should be between 0 and 5. @PierpaoloSorbellini @diegofiori  Need your inputs on the same\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/330/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/330/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/329",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/329/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/329/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/329/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/329",
        "id": 1656641559,
        "node_id": "PR_kwDOG1WDQc5Nuthd",
        "number": 329,
        "title": "[ChatLLaMA] RL Trainer - is_deepspeed_init variable initialization",
        "user": {
            "login": "shrinath-suresh",
            "id": 63862647,
            "node_id": "MDQ6VXNlcjYzODYyNjQ3",
            "avatar_url": "https://avatars.githubusercontent.com/u/63862647?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shrinath-suresh",
            "html_url": "https://github.com/shrinath-suresh",
            "followers_url": "https://api.github.com/users/shrinath-suresh/followers",
            "following_url": "https://api.github.com/users/shrinath-suresh/following{/other_user}",
            "gists_url": "https://api.github.com/users/shrinath-suresh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shrinath-suresh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shrinath-suresh/subscriptions",
            "organizations_url": "https://api.github.com/users/shrinath-suresh/orgs",
            "repos_url": "https://api.github.com/users/shrinath-suresh/repos",
            "events_url": "https://api.github.com/users/shrinath-suresh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shrinath-suresh/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-04-06T04:23:46Z",
        "updated_at": "2023-04-06T10:42:43Z",
        "closed_at": "2023-04-06T10:42:42Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/329",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/329",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/329.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/329.patch",
            "merged_at": "2023-04-06T10:42:42Z"
        },
        "body": "The variable `self.is_deepspeed_init` is initialized only if deepspeed is enabled in the actor or critic - [line](https://github.com/nebuly-ai/nebullvm/blob/87c6d132a37deea52aacb5e90324e74742970739/apps/accelerate/chatllama/chatllama/rlhf/trainer.py#L561)\r\n\r\nIn my case, i trained both actor and reward without enabling Deepspeed. Hence, the while training the RL model, i received error on this [line](https://github.com/nebuly-ai/nebullvm/blob/87c6d132a37deea52aacb5e90324e74742970739/apps/accelerate/chatllama/chatllama/rlhf/trainer.py#L773).\r\n\r\nInitializing the variable to `None` solves the problem.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/329/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/329/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/328",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/328/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/328/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/328/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/328",
        "id": 1655115911,
        "node_id": "PR_kwDOG1WDQc5NpmpO",
        "number": 328,
        "title": "[Nebullvm] Add models and utils to Nebullvm SDK",
        "user": {
            "login": "Telemaco019",
            "id": 24567368,
            "node_id": "MDQ6VXNlcjI0NTY3MzY4",
            "avatar_url": "https://avatars.githubusercontent.com/u/24567368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Telemaco019",
            "html_url": "https://github.com/Telemaco019",
            "followers_url": "https://api.github.com/users/Telemaco019/followers",
            "following_url": "https://api.github.com/users/Telemaco019/following{/other_user}",
            "gists_url": "https://api.github.com/users/Telemaco019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Telemaco019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Telemaco019/subscriptions",
            "organizations_url": "https://api.github.com/users/Telemaco019/orgs",
            "repos_url": "https://api.github.com/users/Telemaco019/repos",
            "events_url": "https://api.github.com/users/Telemaco019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Telemaco019/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-04-05T07:50:53Z",
        "updated_at": "2023-04-14T07:52:17Z",
        "closed_at": "2023-04-14T07:52:17Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/328",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/328",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/328.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/328.patch",
            "merged_at": null
        },
        "body": "* Add model classes to `core` package\r\n* Add StableDiffusionInferenceLearner (TODO: we still have to implement some of its methods)\r\n* Add model adapters for handling data and model conversion for HF/SD\r\n* Extract utility functions",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/328/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/328/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/327",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/327/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/327/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/327/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/327",
        "id": 1654315240,
        "node_id": "PR_kwDOG1WDQc5Nm7J2",
        "number": 327,
        "title": "[ChatLLaMA] DocFix - RL training command in README.md missing filename",
        "user": {
            "login": "shrinath-suresh",
            "id": 63862647,
            "node_id": "MDQ6VXNlcjYzODYyNjQ3",
            "avatar_url": "https://avatars.githubusercontent.com/u/63862647?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shrinath-suresh",
            "html_url": "https://github.com/shrinath-suresh",
            "followers_url": "https://api.github.com/users/shrinath-suresh/followers",
            "following_url": "https://api.github.com/users/shrinath-suresh/following{/other_user}",
            "gists_url": "https://api.github.com/users/shrinath-suresh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shrinath-suresh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shrinath-suresh/subscriptions",
            "organizations_url": "https://api.github.com/users/shrinath-suresh/orgs",
            "repos_url": "https://api.github.com/users/shrinath-suresh/repos",
            "events_url": "https://api.github.com/users/shrinath-suresh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shrinath-suresh/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-04-04T17:51:38Z",
        "updated_at": "2023-04-06T10:32:10Z",
        "closed_at": "2023-04-06T10:32:10Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/327",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/327",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/327.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/327.patch",
            "merged_at": "2023-04-06T10:32:10Z"
        },
        "body": "RL command in main readme file - https://github.com/nebuly-ai/nebullvm/blob/main/apps/accelerate/chatllama/README.md missing filename `main.py`\r\n\r\n```\r\n    python artifacts/\r\n    artifacts/config/config.yaml --type RL\r\n```\r\n\r\nto\r\n\r\n```\r\npython artifacts/main.py artifacts/config/config.yaml --type RL\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/327/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/327/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/326",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/326/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/326/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/326/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/326",
        "id": 1654163245,
        "node_id": "PR_kwDOG1WDQc5NmadH",
        "number": 326,
        "title": "Fix deepspeed multi gpu model saving",
        "user": {
            "login": "EikeKohl",
            "id": 57795397,
            "node_id": "MDQ6VXNlcjU3Nzk1Mzk3",
            "avatar_url": "https://avatars.githubusercontent.com/u/57795397?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/EikeKohl",
            "html_url": "https://github.com/EikeKohl",
            "followers_url": "https://api.github.com/users/EikeKohl/followers",
            "following_url": "https://api.github.com/users/EikeKohl/following{/other_user}",
            "gists_url": "https://api.github.com/users/EikeKohl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/EikeKohl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/EikeKohl/subscriptions",
            "organizations_url": "https://api.github.com/users/EikeKohl/orgs",
            "repos_url": "https://api.github.com/users/EikeKohl/repos",
            "events_url": "https://api.github.com/users/EikeKohl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/EikeKohl/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-04-04T16:00:39Z",
        "updated_at": "2023-04-13T08:09:35Z",
        "closed_at": "2023-04-13T08:09:35Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/326",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/326",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/326.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/326.patch",
            "merged_at": "2023-04-13T08:09:35Z"
        },
        "body": "In this PR, I introduced a new method `save_deepspeed` for the `ActorCritic` model. I also changed the checkpoint saving logic a little. These changes were necessary to ensure checkpointing for deepspeed multi GPU training. Please note that the custom state_dicts for `torch.save` cannot be saved if you use deepspeed's zero optimization, because the client_state parameter of the `DeepSpeedEngine.save_checkpoint` method will not be used in zero optimization. Unfortunately, this means that checkpoint loading will also have to be adjusted to be able to load zero checkpoints. The user can however convert the zero checkpoints to a regular state_dict using this method: https://deepspeed.readthedocs.io/en/latest/model-checkpointing.html#deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/326/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/326/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/325",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/325/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/325/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/325/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/325",
        "id": 1652436620,
        "node_id": "I_kwDOG1WDQc5ifi6M",
        "number": 325,
        "title": "Support for torch 2.0",
        "user": {
            "login": "lminer",
            "id": 5126549,
            "node_id": "MDQ6VXNlcjUxMjY1NDk=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5126549?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/lminer",
            "html_url": "https://github.com/lminer",
            "followers_url": "https://api.github.com/users/lminer/followers",
            "following_url": "https://api.github.com/users/lminer/following{/other_user}",
            "gists_url": "https://api.github.com/users/lminer/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/lminer/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/lminer/subscriptions",
            "organizations_url": "https://api.github.com/users/lminer/orgs",
            "repos_url": "https://api.github.com/users/lminer/repos",
            "events_url": "https://api.github.com/users/lminer/events{/privacy}",
            "received_events_url": "https://api.github.com/users/lminer/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4981513383,
                "node_id": "LA_kwDOG1WDQc8AAAABKOvcpw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/speedster",
                "name": "speedster",
                "color": "bfdadc",
                "default": false,
                "description": "Issue related to the Speedster App"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-04-03T16:59:38Z",
        "updated_at": "2023-04-04T07:20:30Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Is support for torch 2.0 on the roadmap anytime soon?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/325/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/325/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/324",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/324/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/324/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/324/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/324",
        "id": 1651376869,
        "node_id": "I_kwDOG1WDQc5ibgLl",
        "number": 324,
        "title": "module not found:chatllama.rlhf.dataset",
        "user": {
            "login": "MuffinC",
            "id": 56317821,
            "node_id": "MDQ6VXNlcjU2MzE3ODIx",
            "avatar_url": "https://avatars.githubusercontent.com/u/56317821?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/MuffinC",
            "html_url": "https://github.com/MuffinC",
            "followers_url": "https://api.github.com/users/MuffinC/followers",
            "following_url": "https://api.github.com/users/MuffinC/following{/other_user}",
            "gists_url": "https://api.github.com/users/MuffinC/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/MuffinC/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/MuffinC/subscriptions",
            "organizations_url": "https://api.github.com/users/MuffinC/orgs",
            "repos_url": "https://api.github.com/users/MuffinC/repos",
            "events_url": "https://api.github.com/users/MuffinC/events{/privacy}",
            "received_events_url": "https://api.github.com/users/MuffinC/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-04-03T05:47:32Z",
        "updated_at": "2023-04-03T09:39:39Z",
        "closed_at": "2023-04-03T05:56:42Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi! i've already ran the setup.py, requirements.txt and also installed chatllama-py. But when i run python artifacts/main.py artifacts/config/config.yaml --type ALL. It gives the error\r\nModuleNotFoundError: No module named 'chatllama.rlhf.dataset'\r\nPlease advise on what exactly am i missing! Also have all the datasets downloaded",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/324/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/324/timeline",
        "performed_via_github_app": null,
        "state_reason": "not_planned"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/323",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/323/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/323/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/323/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/323",
        "id": 1650914036,
        "node_id": "I_kwDOG1WDQc5iZvL0",
        "number": 323,
        "title": "[Chatllama] facebook/opt-350m is missing in rlhf/model_list.py",
        "user": {
            "login": "tengxiaoliu",
            "id": 61669825,
            "node_id": "MDQ6VXNlcjYxNjY5ODI1",
            "avatar_url": "https://avatars.githubusercontent.com/u/61669825?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/tengxiaoliu",
            "html_url": "https://github.com/tengxiaoliu",
            "followers_url": "https://api.github.com/users/tengxiaoliu/followers",
            "following_url": "https://api.github.com/users/tengxiaoliu/following{/other_user}",
            "gists_url": "https://api.github.com/users/tengxiaoliu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/tengxiaoliu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/tengxiaoliu/subscriptions",
            "organizations_url": "https://api.github.com/users/tengxiaoliu/orgs",
            "repos_url": "https://api.github.com/users/tengxiaoliu/repos",
            "events_url": "https://api.github.com/users/tengxiaoliu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/tengxiaoliu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-04-02T11:40:13Z",
        "updated_at": "2023-04-03T16:20:18Z",
        "closed_at": "2023-04-03T16:20:17Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi, thanks for the great repo!\r\n\r\nIn rlhf/model_list.py, \"facebook/opt-350m\" is missing in hf_models_causal_lm. Setting \"model=facebook/opt-350m\" in actor_config will cause \"UnboundLocalError: local variable 'tokenizer' referenced before assignment\" at rlhf/actor.py:176. \r\n\r\nOr maybe simply add a warning for unsupported models when loading tokenizer at Line176 in chatllama/rlhf/actor.py.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/323/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/323/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/322",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/322/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/322/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/322/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/322",
        "id": 1649872632,
        "node_id": "I_kwDOG1WDQc5iVw74",
        "number": 322,
        "title": "[chatllama]Do I need to split the llama model manully?",
        "user": {
            "login": "balcklive",
            "id": 57667856,
            "node_id": "MDQ6VXNlcjU3NjY3ODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/57667856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balcklive",
            "html_url": "https://github.com/balcklive",
            "followers_url": "https://api.github.com/users/balcklive/followers",
            "following_url": "https://api.github.com/users/balcklive/following{/other_user}",
            "gists_url": "https://api.github.com/users/balcklive/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balcklive/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balcklive/subscriptions",
            "organizations_url": "https://api.github.com/users/balcklive/orgs",
            "repos_url": "https://api.github.com/users/balcklive/repos",
            "events_url": "https://api.github.com/users/balcklive/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balcklive/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-31T19:02:21Z",
        "updated_at": "2023-04-14T13:50:42Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I downloaded a llama 7B model. It only get one model file which ends with .pth.  But as the model loading code in llama_model .py showed as below says. If I want train the model with multi gpus, I need to divide the model into the same number as the graphics card. May I ask how should do that? or is there anything I did not understand?\r\n\r\n\r\ndef load_checkpoints(\r\n    ckpt_dir: str, local_rank: int, world_size: int\r\n) -> Tuple[dict, dict]:\r\n    checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\r\n    assert world_size == len(checkpoints), (\r\n        f\"Loading a checkpoint for MP={len(checkpoints)} but world \"     # world size means numbers of gpus used right?\r\n        f\"size is {world_size}\"\r\n    )\r\n    ckpt_path = checkpoints[local_rank]\r\n    print(\"Loading\")\r\n    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\r\n    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\r\n        params = json.loads(f.read())\r\n    return checkpoint, params",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/322/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/322/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/321",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/321/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/321/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/321/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/321",
        "id": 1649362638,
        "node_id": "I_kwDOG1WDQc5iT0bO",
        "number": 321,
        "title": "[Chatllama] Merge the datasets to create more insightful training data",
        "user": {
            "login": "PierpaoloSorbellini",
            "id": 47692350,
            "node_id": "MDQ6VXNlcjQ3NjkyMzUw",
            "avatar_url": "https://avatars.githubusercontent.com/u/47692350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PierpaoloSorbellini",
            "html_url": "https://github.com/PierpaoloSorbellini",
            "followers_url": "https://api.github.com/users/PierpaoloSorbellini/followers",
            "following_url": "https://api.github.com/users/PierpaoloSorbellini/following{/other_user}",
            "gists_url": "https://api.github.com/users/PierpaoloSorbellini/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PierpaoloSorbellini/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PierpaoloSorbellini/subscriptions",
            "organizations_url": "https://api.github.com/users/PierpaoloSorbellini/orgs",
            "repos_url": "https://api.github.com/users/PierpaoloSorbellini/repos",
            "events_url": "https://api.github.com/users/PierpaoloSorbellini/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PierpaoloSorbellini/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 3825828844,
                "node_id": "LA_kwDOG1WDQc7kCYPs",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/good%20first%20issue",
                "name": "good first issue",
                "color": "7057ff",
                "default": true,
                "description": "Good for newcomers"
            },
            {
                "id": 5241046875,
                "node_id": "LA_kwDOG1WDQc8AAAABOGQHWw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/chatllama",
                "name": "chatllama",
                "color": "bfd4f2",
                "default": false,
                "description": "Issue related to the ChatLLaMA module"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": {
            "login": "mohsinmahmood12",
            "id": 55545648,
            "node_id": "MDQ6VXNlcjU1NTQ1NjQ4",
            "avatar_url": "https://avatars.githubusercontent.com/u/55545648?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mohsinmahmood12",
            "html_url": "https://github.com/mohsinmahmood12",
            "followers_url": "https://api.github.com/users/mohsinmahmood12/followers",
            "following_url": "https://api.github.com/users/mohsinmahmood12/following{/other_user}",
            "gists_url": "https://api.github.com/users/mohsinmahmood12/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mohsinmahmood12/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mohsinmahmood12/subscriptions",
            "organizations_url": "https://api.github.com/users/mohsinmahmood12/orgs",
            "repos_url": "https://api.github.com/users/mohsinmahmood12/repos",
            "events_url": "https://api.github.com/users/mohsinmahmood12/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mohsinmahmood12/received_events",
            "type": "User",
            "site_admin": false
        },
        "assignees": [
            {
                "login": "mohsinmahmood12",
                "id": 55545648,
                "node_id": "MDQ6VXNlcjU1NTQ1NjQ4",
                "avatar_url": "https://avatars.githubusercontent.com/u/55545648?v=4",
                "gravatar_id": "",
                "url": "https://api.github.com/users/mohsinmahmood12",
                "html_url": "https://github.com/mohsinmahmood12",
                "followers_url": "https://api.github.com/users/mohsinmahmood12/followers",
                "following_url": "https://api.github.com/users/mohsinmahmood12/following{/other_user}",
                "gists_url": "https://api.github.com/users/mohsinmahmood12/gists{/gist_id}",
                "starred_url": "https://api.github.com/users/mohsinmahmood12/starred{/owner}{/repo}",
                "subscriptions_url": "https://api.github.com/users/mohsinmahmood12/subscriptions",
                "organizations_url": "https://api.github.com/users/mohsinmahmood12/orgs",
                "repos_url": "https://api.github.com/users/mohsinmahmood12/repos",
                "events_url": "https://api.github.com/users/mohsinmahmood12/events{/privacy}",
                "received_events_url": "https://api.github.com/users/mohsinmahmood12/received_events",
                "type": "User",
                "site_admin": false
            }
        ],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-31T13:30:52Z",
        "updated_at": "2023-04-19T06:01:16Z",
        "closed_at": null,
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "body": "# Description\r\nCurrently the dataset supported can be used alternatively to each other, \r\nIt would be nice to add diversity in the training data to select recipes to merge this dataset and create more insightful trainings. \r\n\r\n# TODO\r\n- [ ] Define what parameters needs to be specified to create a \"recipe\" for the dataset and add them to the config files\r\n- [ ] Expand the dataset class to allow parameters from the config file to generate the appropriate dataset mixture. \r\n- [ ] Evaluate the possible increase in model quality due to different \"recipes\" used. ",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/321/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/321/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/320",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/320/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/320/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/320/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/320",
        "id": 1649354835,
        "node_id": "I_kwDOG1WDQc5iTyhT",
        "number": 320,
        "title": "[Chatllama] Support Inference for trained models.",
        "user": {
            "login": "PierpaoloSorbellini",
            "id": 47692350,
            "node_id": "MDQ6VXNlcjQ3NjkyMzUw",
            "avatar_url": "https://avatars.githubusercontent.com/u/47692350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PierpaoloSorbellini",
            "html_url": "https://github.com/PierpaoloSorbellini",
            "followers_url": "https://api.github.com/users/PierpaoloSorbellini/followers",
            "following_url": "https://api.github.com/users/PierpaoloSorbellini/following{/other_user}",
            "gists_url": "https://api.github.com/users/PierpaoloSorbellini/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PierpaoloSorbellini/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PierpaoloSorbellini/subscriptions",
            "organizations_url": "https://api.github.com/users/PierpaoloSorbellini/orgs",
            "repos_url": "https://api.github.com/users/PierpaoloSorbellini/repos",
            "events_url": "https://api.github.com/users/PierpaoloSorbellini/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PierpaoloSorbellini/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 3825828844,
                "node_id": "LA_kwDOG1WDQc7kCYPs",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/good%20first%20issue",
                "name": "good first issue",
                "color": "7057ff",
                "default": true,
                "description": "Good for newcomers"
            },
            {
                "id": 5241046875,
                "node_id": "LA_kwDOG1WDQc8AAAABOGQHWw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/chatllama",
                "name": "chatllama",
                "color": "bfd4f2",
                "default": false,
                "description": "Issue related to the ChatLLaMA module"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-31T13:25:27Z",
        "updated_at": "2023-04-07T17:36:15Z",
        "closed_at": null,
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "body": "# Description\r\nCurrently to perform inference of the models generated the user needs to interact with the model generated writing a small python script accordingly to how the model is saved by library, by loading the resulting checkpoint or model saved after training.\r\n\r\nMoreover a lot of optimization can be integrated to speed-up the inference such as:\r\n- CPU Offloading.\r\n- [llama.ccp](https://github.com/ggerganov/llama.cpp) implementation \r\n- accelerate / deepspeed distributed inference. \r\n\r\n# TODO\r\n- [ ] Implement Inference Class to make inference very easy and even possible from CLI. \r\n- [ ] Implement Inference with  the optimisations available from deepspeed\r\n- [ ] Implement inference with the optimisations available from accelerate\r\n- [ ] Implement fast lama inference with known library [llama.ccp](https://github.com/ggerganov/llama.cpp) implementation ",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/320/reactions",
            "total_count": 2,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/320/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/319",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/319/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/319/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/319/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/319",
        "id": 1649344853,
        "node_id": "I_kwDOG1WDQc5iTwFV",
        "number": 319,
        "title": "[Chatllama] Evaluation Function and Loop with metrics",
        "user": {
            "login": "PierpaoloSorbellini",
            "id": 47692350,
            "node_id": "MDQ6VXNlcjQ3NjkyMzUw",
            "avatar_url": "https://avatars.githubusercontent.com/u/47692350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PierpaoloSorbellini",
            "html_url": "https://github.com/PierpaoloSorbellini",
            "followers_url": "https://api.github.com/users/PierpaoloSorbellini/followers",
            "following_url": "https://api.github.com/users/PierpaoloSorbellini/following{/other_user}",
            "gists_url": "https://api.github.com/users/PierpaoloSorbellini/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PierpaoloSorbellini/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PierpaoloSorbellini/subscriptions",
            "organizations_url": "https://api.github.com/users/PierpaoloSorbellini/orgs",
            "repos_url": "https://api.github.com/users/PierpaoloSorbellini/repos",
            "events_url": "https://api.github.com/users/PierpaoloSorbellini/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PierpaoloSorbellini/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 3825828844,
                "node_id": "LA_kwDOG1WDQc7kCYPs",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/good%20first%20issue",
                "name": "good first issue",
                "color": "7057ff",
                "default": true,
                "description": "Good for newcomers"
            },
            {
                "id": 5241046875,
                "node_id": "LA_kwDOG1WDQc8AAAABOGQHWw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/chatllama",
                "name": "chatllama",
                "color": "bfd4f2",
                "default": false,
                "description": "Issue related to the ChatLLaMA module"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-31T13:18:38Z",
        "updated_at": "2023-04-04T08:21:51Z",
        "closed_at": null,
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "body": "# Description\r\nCurrently each training loop has an evaluation loop but it is not debugged nor used so far. \r\n\r\nIt needs to be generalised to be launched also outside the training activities, and to support specific language modelling metrics. \r\nIt would be nice if a report can be generated highlighting the performance achieved also in comparison with other models.\r\n\r\n# TODO\r\n- [ ] Understand that libraries such as [openai/evals](https://github.com/openai/evals) or [FastChat](https://github.com/lm-sys/FastChat) can be adapted to be used as an evaluation tool\r\n- [ ] Debug Evaluation of the model.\r\n- [ ] Collect and Compute relevant metrics.\r\n- [ ] Launch the evaluation loop also outside the training. \r\n- [ ] Produce a meaningful report that can compare the performance of one or more models. \r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/319/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/319/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/318",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/318/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/318/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/318/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/318",
        "id": 1649314136,
        "node_id": "PR_kwDOG1WDQc5NWZYe",
        "number": 318,
        "title": "Add support for AWS Inferentia chips and Google TPUs & support latest PyTorch and TensorFlow versions",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-31T12:59:01Z",
        "updated_at": "2023-04-12T13:31:14Z",
        "closed_at": "2023-04-12T13:31:14Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/318",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/318",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/318.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/318.patch",
            "merged_at": null
        },
        "body": "This PR adds the support of Inferentia chips and TPUs for PyTorch models.\r\nMoreover, it adds support for PyTorch 2.0 and TensorFlow 2.12",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/318/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/318/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/317",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/317/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/317/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/317/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/317",
        "id": 1649300610,
        "node_id": "I_kwDOG1WDQc5iTlSC",
        "number": 317,
        "title": "[ChatLLaMA] Will there be online Demo about ChatLLaMA?",
        "user": {
            "login": "xihuai18",
            "id": 23721828,
            "node_id": "MDQ6VXNlcjIzNzIxODI4",
            "avatar_url": "https://avatars.githubusercontent.com/u/23721828?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/xihuai18",
            "html_url": "https://github.com/xihuai18",
            "followers_url": "https://api.github.com/users/xihuai18/followers",
            "following_url": "https://api.github.com/users/xihuai18/following{/other_user}",
            "gists_url": "https://api.github.com/users/xihuai18/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/xihuai18/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/xihuai18/subscriptions",
            "organizations_url": "https://api.github.com/users/xihuai18/orgs",
            "repos_url": "https://api.github.com/users/xihuai18/repos",
            "events_url": "https://api.github.com/users/xihuai18/events{/privacy}",
            "received_events_url": "https://api.github.com/users/xihuai18/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-31T12:49:41Z",
        "updated_at": "2023-04-03T14:52:52Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/317/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/317/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/316",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/316/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/316/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/316/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/316",
        "id": 1649020785,
        "node_id": "PR_kwDOG1WDQc5NVaEj",
        "number": 316,
        "title": "Implement deepspeed multi gpu for RL Resolves #307",
        "user": {
            "login": "EikeKohl",
            "id": 57795397,
            "node_id": "MDQ6VXNlcjU3Nzk1Mzk3",
            "avatar_url": "https://avatars.githubusercontent.com/u/57795397?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/EikeKohl",
            "html_url": "https://github.com/EikeKohl",
            "followers_url": "https://api.github.com/users/EikeKohl/followers",
            "following_url": "https://api.github.com/users/EikeKohl/following{/other_user}",
            "gists_url": "https://api.github.com/users/EikeKohl/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/EikeKohl/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/EikeKohl/subscriptions",
            "organizations_url": "https://api.github.com/users/EikeKohl/orgs",
            "repos_url": "https://api.github.com/users/EikeKohl/repos",
            "events_url": "https://api.github.com/users/EikeKohl/events{/privacy}",
            "received_events_url": "https://api.github.com/users/EikeKohl/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-03-31T09:27:12Z",
        "updated_at": "2023-03-31T15:17:19Z",
        "closed_at": "2023-03-31T13:59:11Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/316",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/316",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/316.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/316.patch",
            "merged_at": "2023-03-31T13:59:11Z"
        },
        "body": "This PR enables multi GPU training using deepspeed for the RL training. Please note the following:\r\n\r\n- Training has to be launched using the deepspeed command and not pyth: `deepspeed main.py <path to config> -t RL`\r\n- The saving of the conversation logs is not yet fixed and therefore disabled (TBD)\r\n- I had to apply some refactoring, in the RLTrainer, so you might want to check the implications for other PRs first!\r\n\r\nPlease feel free to give feedback and make comments.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/316/reactions",
            "total_count": 4,
            "+1": 2,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 1,
            "rocket": 1,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/316/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/315",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/315/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/315/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/315/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/315",
        "id": 1648632102,
        "node_id": "I_kwDOG1WDQc5iRCEm",
        "number": 315,
        "title": "[Chatllama] how to reduce the CUDA memory comsumption a llama7B model",
        "user": {
            "login": "balcklive",
            "id": 57667856,
            "node_id": "MDQ6VXNlcjU3NjY3ODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/57667856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balcklive",
            "html_url": "https://github.com/balcklive",
            "followers_url": "https://api.github.com/users/balcklive/followers",
            "following_url": "https://api.github.com/users/balcklive/following{/other_user}",
            "gists_url": "https://api.github.com/users/balcklive/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balcklive/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balcklive/subscriptions",
            "organizations_url": "https://api.github.com/users/balcklive/orgs",
            "repos_url": "https://api.github.com/users/balcklive/repos",
            "events_url": "https://api.github.com/users/balcklive/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balcklive/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-31T04:04:02Z",
        "updated_at": "2023-03-31T04:04:02Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "As I metioned in this Issue: #314 \r\nI am training a llama 7B model with 8 v100, totally I got 32 *8 G GPU memory. But at last, you can tell from the detail, I finally got a CUDA out of memory, fault. \r\nCan anybody tell me Is there any other way that I can reduce my model memory comsuption on GPU?\r\nmy deepspeed config.json file is as below:\r\n{\r\n  \r\n    \"gradient_accumulation_steps\": 1,\r\n    \"optimizer\": {\r\n      \"type\": \"Adam\",\r\n      \"params\": {\r\n        \"lr\": 0.00015\r\n      }},\r\n  \"zero_force_ds_cpu_optimizer\": false,\r\n  \"zero_optimization\": {\r\n  \"stage\": 3,\r\n  \"contiguous_gradients\": true,\r\n  \"stage3_max_live_parameters\": 0,\r\n  \"stage3_max_reuse_distance\": 0,\r\n  \"stage3_prefetch_bucket_size\": 0,\r\n  \"stage3_param_persistence_threshold\": 1e2,\r\n  \"reduce_bucket_size\": 1e2,\r\n  \"sub_group_size\": 1e8,\r\n  \"offload_optimizer\": {\r\n  \"device\": \"cpu\",\r\n  \"pin_memory\": true\r\n  },\r\n  \"offload_param\": {\r\n  \"device\": \"cpu\",\r\n  \"pin_memory\": true\r\n  },\r\n  \"stage3_gather_16bit_weights_on_model_save\": true\r\n  },\r\n  \"fp16\": {\r\n  \"enabled\": true,\r\n  \"auto_cast\": false,\r\n  \"loss_scale\": 0,\r\n  \"initial_scale_power\": 32,\r\n  \"loss_scale_window\": 1000,\r\n  \"hysteresis\": 2,\r\n  \"min_loss_scale\": 1\r\n  },\r\n  \"train_batch_size\": 8,\r\n  \"train_micro_batch_size_per_gpu\": 1,\r\n  \"wall_clock_breakdown\": false\r\n  }",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/315/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/315/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/314",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/314/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/314/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/314/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/314",
        "id": 1648628525,
        "node_id": "I_kwDOG1WDQc5iRBMt",
        "number": 314,
        "title": "[Chatllama] train actor model with llama7B, the loss is nan",
        "user": {
            "login": "balcklive",
            "id": 57667856,
            "node_id": "MDQ6VXNlcjU3NjY3ODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/57667856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balcklive",
            "html_url": "https://github.com/balcklive",
            "followers_url": "https://api.github.com/users/balcklive/followers",
            "following_url": "https://api.github.com/users/balcklive/following{/other_user}",
            "gists_url": "https://api.github.com/users/balcklive/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balcklive/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balcklive/subscriptions",
            "organizations_url": "https://api.github.com/users/balcklive/orgs",
            "repos_url": "https://api.github.com/users/balcklive/repos",
            "events_url": "https://api.github.com/users/balcklive/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balcklive/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-31T03:59:32Z",
        "updated_at": "2023-03-31T03:59:32Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I mannuly split the model checkpoint into 8 splits. and train the llama model with 8 V100 GPUs. but strangely, the loss is nan. I trained successfully with same data on gpt2-xl model, so I think that's not a data problem. \r\ncan anybody figure out why?\r\n\r\n\r\nubuntu@ip-172-31-10-190:~/ubuntu$ torchrun --standalone --nnodes=1 --nproc-per-node=8 artifacts/main.py artifacts/config/config.yaml --type ACTOR\r\nmaster_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\r\nWARNING:torch.distributed.run:\r\n*****************************************\r\nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n*****************************************\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\r\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\r\nCurrent device used :cuda\r\nStart cleaning the dataset for Actor\r\nCurrent device used :cuda\r\nCurrent device used :cuda\r\nStart cleaning the dataset for Actor\r\nStart cleaning the dataset for Actor\r\nCurrent device used :cuda\r\nCurrent device used :cuda\r\nStart cleaning the dataset for Actor\r\nCurrent device used :cuda\r\nCurrent device used :cuda\r\nCurrent device used :cuda\r\nStart cleaning the dataset for Actor\r\nStart cleaning the dataset for Actor\r\nStart cleaning the dataset for Actor\r\nStart cleaning the dataset for Actor\r\nDataset is already clean\r\nDataset is already clean\r\nDataset is already clean\r\nlocal_rank: 4 world_size: 8\r\nDataset is already clean\r\nDataset is already clean\r\nDataset is already clean\r\nDataset is already clean\r\nlocal_rank: 0 world_size: 8\r\nlocal_rank: 6 world_size: 8\r\nlocal_rank: 5 world_size: 8\r\nlocal_rank: 1 world_size: 8\r\nDataset is already clean\r\nlocal_rank: 2 world_size: 8\r\nlocal_rank: 7 world_size: 8\r\nlocal_rank: 3 world_size: 8\r\n> initializing model parallel with size 8\r\n> initializing ddp with size 1\r\n> initializing pipeline with size 1\r\nLoading\r\nLoading\r\nLoading\r\nLoading\r\nLoading\r\nLoading\r\nLoading\r\nLoading\r\nNo previous model found at /home/ubuntu/ubuntu/pyllama_data1/7B/actor for model llama-7B.pt\r\nNo previous model found at /home/ubuntu/ubuntu/pyllama_data1/7B/actor for model llama-7B.pt\r\nNo previous model found at /home/ubuntu/ubuntu/pyllama_data1/7B/actor for model llama-7B.pt\r\nNo previous model found at /home/ubuntu/ubuntu/pyllama_data1/7B/actor for model llama-7B.pt\r\nNo previous model found at /home/ubuntu/ubuntu/pyllama_data1/7B/actor for model llama-7B.pt\r\nNo previous model found at /home/ubuntu/ubuntu/pyllama_data1/7B/actor for model llama-7B.pt\r\nNo previous model found at /home/ubuntu/ubuntu/pyllama_data1/7B/actor for model llama-7B.pt\r\nNo previous model found at /home/ubuntu/ubuntu/pyllama_data1/7B/actor for model llama-7B.pt\r\n[2023-03-31 03:50:04,515] [INFO] [logging.py:77:log_dist] [Rank -1] DeepSpeed info: version=0.8.2, git-hash=unknown, git-branch=unknown\r\n[2023-03-31 03:50:04,533] [INFO] [logging.py:77:log_dist] [Rank -1] DeepSpeed info: version=0.8.2, git-hash=unknown, git-branch=unknown\r\n[2023-03-31 03:50:04,600] [INFO] [logging.py:77:log_dist] [Rank -1] DeepSpeed info: version=0.8.2, git-hash=unknown, git-branch=unknown\r\n[2023-03-31 03:50:04,614] [INFO] [logging.py:77:log_dist] [Rank -1] DeepSpeed info: version=0.8.2, git-hash=unknown, git-branch=unknown\r\n[2023-03-31 03:50:04,655] [INFO] [logging.py:77:log_dist] [Rank -1] DeepSpeed info: version=0.8.2, git-hash=unknown, git-branch=unknown\r\n[2023-03-31 03:50:04,685] [INFO] [logging.py:77:log_dist] [Rank -1] DeepSpeed info: version=0.8.2, git-hash=unknown, git-branch=unknown\r\n[2023-03-31 03:50:04,686] [INFO] [logging.py:77:log_dist] [Rank -1] DeepSpeed info: version=0.8.2, git-hash=unknown, git-branch=unknown\r\n[2023-03-31 03:50:04,750] [INFO] [logging.py:77:log_dist] [Rank -1] DeepSpeed info: version=0.8.2, git-hash=unknown, git-branch=unknown\r\n[2023-03-31 03:50:06,138] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\r\n[2023-03-31 03:50:06,138] [INFO] [logging.py:77:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\r\n[2023-03-31 03:50:06,138] [INFO] [logging.py:77:log_dist] [Rank 0] Using client Optimizer as basic optimizer\r\n[2023-03-31 03:50:06,152] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\r\n[2023-03-31 03:50:06,152] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\r\n[2023-03-31 03:50:06,152] [INFO] [logging.py:77:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\r\n[2023-03-31 03:50:06,306] [INFO] [utils.py:829:see_memory_usage] Stage 3 initialize beginning\r\n[2023-03-31 03:50:06,307] [INFO] [utils.py:830:see_memory_usage] MA 14.55 GB         Max_MA 14.55 GB         CA 14.57 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:06,307] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 21.76 GB, percent = 2.9%\r\n[2023-03-31 03:50:06,308] [INFO] [stage3.py:113:__init__] Reduce bucket size 100\r\n[2023-03-31 03:50:06,309] [INFO] [stage3.py:114:__init__] Prefetch bucket size 0\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nEmitting ninja build file /home/ubuntu/.cache/torch_extensions/py38_cu117/utils/build.ninja...\r\nBuilding extension module utils...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nninja: no work to do.\r\nLoading extension module utils...\r\nTime to load utils op: 0.25902438163757324 seconds\r\nLoading extension module utils...\r\nTime to load utils op: 0.10285019874572754 seconds\r\nLoading extension module utils...\r\nTime to load utils op: 0.20204687118530273 seconds\r\nLoading extension module utils...\r\nTime to load utils op: 0.20221972465515137 seconds\r\nLoading extension module utils...\r\nLoading extension module utils...\r\nTime to load utils op: 0.30254244804382324 seconds\r\nTime to load utils op: 0.3023359775543213 seconds\r\nLoading extension module utils...\r\nTime to load utils op: 0.3025219440460205 seconds\r\nLoading extension module utils...\r\nTime to load utils op: 0.30247020721435547 seconds\r\n[2023-03-31 03:50:07,142] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\r\n[2023-03-31 03:50:07,143] [INFO] [utils.py:830:see_memory_usage] MA 14.55 GB         Max_MA 14.55 GB         CA 14.57 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:07,143] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 22.86 GB, percent = 3.1%\r\nParameter Offload: Total persistent parameters: 0 in 0 params\r\n[2023-03-31 03:50:09,626] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\r\n[2023-03-31 03:50:09,627] [INFO] [utils.py:830:see_memory_usage] MA 2.0 GB         Max_MA 14.55 GB         CA 14.57 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:09,627] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 38.78 GB, percent = 5.2%\r\n[2023-03-31 03:50:09,708] [INFO] [utils.py:829:see_memory_usage] Before creating fp16 partitions\r\n[2023-03-31 03:50:09,708] [INFO] [utils.py:830:see_memory_usage] MA 2.0 GB         Max_MA 2.0 GB         CA 14.57 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:09,708] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 38.78 GB, percent = 5.2%\r\n[2023-03-31 03:50:12,637] [INFO] [utils.py:829:see_memory_usage] After creating fp16 partitions: 9\r\n[2023-03-31 03:50:12,638] [INFO] [utils.py:830:see_memory_usage] MA 2.0 GB         Max_MA 2.0 GB         CA 14.57 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:12,638] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 57.15 GB, percent = 7.6%\r\n[2023-03-31 03:50:12,756] [INFO] [utils.py:829:see_memory_usage] Before creating fp32 partitions\r\n[2023-03-31 03:50:12,757] [INFO] [utils.py:830:see_memory_usage] MA 2.0 GB         Max_MA 2.0 GB         CA 14.57 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:12,757] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 58.49 GB, percent = 7.8%\r\n[2023-03-31 03:50:15,870] [INFO] [utils.py:829:see_memory_usage] After creating fp32 partitions\r\n[2023-03-31 03:50:15,870] [INFO] [utils.py:830:see_memory_usage] MA 2.0 GB         Max_MA 2.0 GB         CA 14.57 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:15,871] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 80.31 GB, percent = 10.7%\r\n[2023-03-31 03:50:15,985] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states\r\n[2023-03-31 03:50:15,986] [INFO] [utils.py:830:see_memory_usage] MA 2.0 GB         Max_MA 2.0 GB         CA 14.57 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:15,986] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 81.74 GB, percent = 10.9%\r\n[2023-03-31 03:50:36,904] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states\r\n[2023-03-31 03:50:36,905] [INFO] [utils.py:830:see_memory_usage] MA 2.0 GB         Max_MA 2.0 GB         CA 14.57 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:36,905] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 167.03 GB, percent = 22.3%\r\n[2023-03-31 03:50:38,351] [INFO] [stage3.py:376:_setup_for_real_optimizer] optimizer state initialized\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nNo modifications detected for re-loaded extension module utils, skipping build step...\r\nLoading extension module utils...\r\nTime to load utils op: 0.0007975101470947266 seconds\r\nTraining with DeepSpeed\r\nStart Actor Model Pretraining\r\nLooking for checkpoints...\r\nNo previous checkpoint found at /home/ubuntu/ubuntu/pyllama_data1/7B/checkpoints/actor for llama-7B.pt\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nNo modifications detected for re-loaded extension module utils, skipping build step...\r\nLoading extension module utils...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nTime to load utils op: 0.0007760524749755859 seconds\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nTraining with DeepSpeed\r\nNo modifications detected for re-loaded extension module utils, skipping build step...\r\nLoading extension module utils...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nStart Actor Model Pretraining\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nLooking for checkpoints...\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...No modifications detected for re-loaded extension module utils, skipping build step...No previous checkpoint found at /home/ubuntu/ubuntu/pyllama_data1/7B/checkpoints/actor for llama-7B.pt\r\n\r\nTime to load utils op: 0.0008547306060791016 seconds\r\n\r\nLoading extension module utils...\r\nNo modifications detected for re-loaded extension module utils, skipping build step...\r\nLoading extension module utils...\r\nNo modifications detected for re-loaded extension module utils, skipping build step...\r\nLoading extension module utils...\r\nTraining with DeepSpeed\r\nStart Actor Model Pretraining\r\nTime to load utils op: 0.0008461475372314453 seconds\r\nNo modifications detected for re-loaded extension module utils, skipping build step...Looking for checkpoints...\r\n\r\nLoading extension module utils...\r\nTime to load utils op: 0.0008032321929931641 seconds\r\nTraining with DeepSpeed\r\nTime to load utils op: 0.0009899139404296875 secondsNo previous checkpoint found at /home/ubuntu/ubuntu/pyllama_data1/7B/checkpoints/actor for llama-7B.pt\r\n\r\nTraining with DeepSpeed\r\nStart Actor Model Pretraining\r\nLooking for checkpoints...\r\nStart Actor Model Pretraining\r\nTraining with DeepSpeed\r\nTime to load utils op: 0.0009548664093017578 secondsLooking for checkpoints...\r\n\r\nStart Actor Model Pretraining\r\nNo previous checkpoint found at /home/ubuntu/ubuntu/pyllama_data1/7B/checkpoints/actor for llama-7B.pt\r\nLooking for checkpoints...\r\nTraining with DeepSpeed\r\nNo previous checkpoint found at /home/ubuntu/ubuntu/pyllama_data1/7B/checkpoints/actor for llama-7B.pt\r\nStart Actor Model Pretraining\r\nLooking for checkpoints...\r\nNo previous checkpoint found at /home/ubuntu/ubuntu/pyllama_data1/7B/checkpoints/actor for llama-7B.pt\r\nNo previous checkpoint found at /home/ubuntu/ubuntu/pyllama_data1/7B/checkpoints/actor for llama-7B.pt\r\n[2023-03-31 03:50:41,801] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer\r\n[2023-03-31 03:50:41,802] [INFO] [utils.py:830:see_memory_usage] MA 2.0 GB         Max_MA 2.49 GB         CA 14.82 GB         Max_CA 15 GB \r\n[2023-03-31 03:50:41,802] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 179.5 GB, percent = 24.0%\r\n[2023-03-31 03:50:41,802] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\r\n[2023-03-31 03:50:41,802] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed using client LR scheduler\r\n[2023-03-31 03:50:41,802] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.CosineAnnealingWarmRestarts object at 0x7f812de03340>\r\n[2023-03-31 03:50:41,802] [INFO] [logging.py:77:log_dist] [Rank 0] step=0, skipped=0, lr=[9e-06], mom=[(0.9, 0.999)]\r\n[2023-03-31 03:50:41,803] [INFO] [config.py:1010:print] DeepSpeedEngine configuration:\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   activation_checkpointing_config  {\r\n    \"partition_activations\": false, \r\n    \"contiguous_memory_optimization\": false, \r\n    \"cpu_checkpointing\": false, \r\n    \"number_checkpoints\": null, \r\n    \"synchronize_checkpoint_boundary\": false, \r\n    \"profile\": false\r\n}\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   amp_enabled .................. False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   amp_params ................... False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   autotuning_config ............ {\r\n    \"enabled\": false, \r\n    \"start_step\": null, \r\n    \"end_step\": null, \r\n    \"metric_path\": null, \r\n    \"arg_mappings\": null, \r\n    \"metric\": \"throughput\", \r\n    \"model_info\": null, \r\n    \"results_dir\": \"autotuning_results\", \r\n    \"exps_dir\": \"autotuning_exps\", \r\n    \"overwrite\": true, \r\n    \"fast\": true, \r\n    \"start_profile_step\": 3, \r\n    \"end_profile_step\": 5, \r\n    \"tuner_type\": \"gridsearch\", \r\n    \"tuner_early_stopping\": 5, \r\n    \"tuner_num_trials\": 50, \r\n    \"model_info_path\": null, \r\n    \"mp_size\": 1, \r\n    \"max_train_batch_size\": null, \r\n    \"min_train_batch_size\": 1, \r\n    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \r\n    \"min_train_micro_batch_size_per_gpu\": 1, \r\n    \"num_tuning_micro_batch_sizes\": 3\r\n}\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   bfloat16_enabled ............. False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   checkpoint_parallel_write_pipeline  False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   checkpoint_tag_validation_enabled  True\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   checkpoint_tag_validation_fail  False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f812de038e0>\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   communication_data_type ...... None\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   curriculum_enabled_legacy .... False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   curriculum_params_legacy ..... False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   data_efficiency_enabled ...... False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   dataloader_drop_last ......... False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   disable_allgather ............ False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   dump_state ................... False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   eigenvalue_enabled ........... False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   eigenvalue_gas_boundary_resolution  1\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   eigenvalue_layer_name ........ bert.encoder.layer\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   eigenvalue_layer_num ......... 0\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   eigenvalue_max_iter .......... 100\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   eigenvalue_stability ......... 1e-06\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   eigenvalue_tol ............... 0.01\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   eigenvalue_verbose ........... False\r\n[2023-03-31 03:50:41,804] [INFO] [config.py:1014:print]   elasticity_enabled ........... False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   flops_profiler_config ........ {\r\n    \"enabled\": false, \r\n    \"profile_step\": 1, \r\n    \"module_depth\": -1, \r\n    \"top_modules\": 1, \r\n    \"detailed\": true, \r\n    \"output_file\": null\r\n}\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   fp16_auto_cast ............... False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   fp16_enabled ................. True\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   fp16_master_weights_and_gradients  False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   global_rank .................. 0\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   grad_accum_dtype ............. None\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   gradient_accumulation_steps .. 1\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   gradient_clipping ............ 0.0\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   gradient_predivide_factor .... 1.0\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   initial_dynamic_scale ........ 4294967296\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   load_universal_checkpoint .... False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   loss_scale ................... 0\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   memory_breakdown ............. False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   nebula_config ................ {\r\n    \"enabled\": false, \r\n    \"persistent_storage_path\": null, \r\n    \"persistent_time_interval\": 100, \r\n    \"num_of_version_in_retention\": 2, \r\n    \"enable_nebula_load\": true, \r\n    \"load_path\": null\r\n}\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   optimizer_legacy_fusion ...... False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   optimizer_name ............... adam\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   optimizer_params ............. {'lr': 0.00015}\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   pld_enabled .................. False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   pld_params ................... False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   prescale_gradients ........... False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   scheduler_name ............... None\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   scheduler_params ............. None\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   sparse_attention ............. None\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   sparse_gradients_enabled ..... False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   steps_per_print .............. 10\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   train_batch_size ............. 8\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   train_micro_batch_size_per_gpu  1\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   use_node_local_storage ....... False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   wall_clock_breakdown ......... False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   world_size ................... 8\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   zero_allow_untested_optimizer  False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=100 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=0 param_persistence_threshold=100 model_persistence_threshold=sys.maxsize max_live_parameters=0 max_reuse_distance=0 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   zero_enabled ................. True\r\n[2023-03-31 03:50:41,805] [INFO] [config.py:1014:print]   zero_optimization_stage ...... 3\r\n[2023-03-31 03:50:41,806] [INFO] [config.py:999:print_user_config]   json = {\r\n    \"gradient_accumulation_steps\": 1, \r\n    \"optimizer\": {\r\n        \"type\": \"Adam\", \r\n        \"params\": {\r\n            \"lr\": 0.00015\r\n        }\r\n    }, \r\n    \"zero_force_ds_cpu_optimizer\": false, \r\n    \"zero_optimization\": {\r\n        \"stage\": 3, \r\n        \"contiguous_gradients\": true, \r\n        \"stage3_max_live_parameters\": 0, \r\n        \"stage3_max_reuse_distance\": 0, \r\n        \"stage3_prefetch_bucket_size\": 0, \r\n        \"stage3_param_persistence_threshold\": 100, \r\n        \"reduce_bucket_size\": 100, \r\n        \"sub_group_size\": 1.000000e+08, \r\n        \"offload_optimizer\": {\r\n            \"device\": \"cpu\", \r\n            \"pin_memory\": true\r\n        }, \r\n        \"offload_param\": {\r\n            \"device\": \"cpu\", \r\n            \"pin_memory\": true\r\n        }, \r\n        \"stage3_gather_16bit_weights_on_model_save\": true\r\n    }, \r\n    \"fp16\": {\r\n        \"enabled\": true, \r\n        \"auto_cast\": false, \r\n        \"loss_scale\": 0, \r\n        \"initial_scale_power\": 32, \r\n        \"loss_scale_window\": 1000, \r\n        \"hysteresis\": 2, \r\n        \"min_loss_scale\": 1\r\n    }, \r\n    \"train_batch_size\": 8, \r\n    \"train_micro_batch_size_per_gpu\": 1, \r\n    \"wall_clock_breakdown\": false\r\n}\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nNo modifications detected for re-loaded extension module utils, skipping build step...\r\nLoading extension module utils...\r\nTime to load utils op: 0.0004968643188476562 seconds\r\nTraining with DeepSpeed\r\nStart Actor Model Pretraining\r\nLooking for checkpoints...\r\nNo previous checkpoint found at /home/ubuntu/ubuntu/pyllama_data1/7B/checkpoints/actor for llama-7B.pt\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\r\n  warnings.warn(\r\n/home/ubuntu/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\r\n  warnings.warn(\r\nEpoch: 1/1, Iteration: 1/160782, Training Loss: nanEpoch: 1/1, Iteration: 1/160782, Training Loss: nan\r\n\r\nEpoch: 1/1, Iteration: 1/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 1/160782, Training Loss: nanEpoch: 1/1, Iteration: 1/160782, Training Loss: nan\r\n\r\nEpoch: 1/1, Iteration: 1/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 1/160782, Training Loss: nan\r\n[2023-03-31 03:50:48,116] [INFO] [stage3.py:1843:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296\r\nEpoch: 1/1, Iteration: 1/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 2/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 2/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 2/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 2/160782, Training Loss: nanEpoch: 1/1, Iteration: 2/160782, Training Loss: nan\r\n\r\nEpoch: 1/1, Iteration: 2/160782, Training Loss: nan\r\n[2023-03-31 03:50:51,973] [INFO] [stage3.py:1843:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0\r\nEpoch: 1/1, Iteration: 2/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 2/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 3/160782, Training Loss: nanEpoch: 1/1, Iteration: 3/160782, Training Loss: nan\r\n\r\nEpoch: 1/1, Iteration: 3/160782, Training Loss: nanEpoch: 1/1, Iteration: 3/160782, Training Loss: nan[2023-03-31 03:50:55,374] [INFO] [stage3.py:1843:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0\r\n\r\n\r\nEpoch: 1/1, Iteration: 3/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 3/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 3/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 3/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 4/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 4/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 4/160782, Training Loss: nanEpoch: 1/1, Iteration: 4/160782, Training Loss: nan\r\n\r\nEpoch: 1/1, Iteration: 4/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 4/160782, Training Loss: nanEpoch: 1/1, Iteration: 4/160782, Training Loss: nan\r\n\r\n[2023-03-31 03:50:58,870] [INFO] [stage3.py:1843:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0\r\nEpoch: 1/1, Iteration: 4/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 5/160782, Training Loss: nanEpoch: 1/1, Iteration: 5/160782, Training Loss: nan\r\n\r\n[2023-03-31 03:51:02,331] [INFO] [stage3.py:1843:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0\r\nEpoch: 1/1, Iteration: 5/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 5/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 5/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 5/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 5/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 5/160782, Training Loss: nan\r\n[2023-03-31 03:51:05,827] [INFO] [stage3.py:1843:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0\r\nEpoch: 1/1, Iteration: 6/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 6/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 6/160782, Training Loss: nanEpoch: 1/1, Iteration: 6/160782, Training Loss: nan\r\n\r\nEpoch: 1/1, Iteration: 6/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 6/160782, Training Loss: nanEpoch: 1/1, Iteration: 6/160782, Training Loss: nan\r\n\r\nEpoch: 1/1, Iteration: 6/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 7/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 7/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 7/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 7/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 7/160782, Training Loss: nan\r\n[2023-03-31 03:51:09,498] [INFO] [stage3.py:1843:_overflow_clean_up] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0\r\nEpoch: 1/1, Iteration: 7/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 7/160782, Training Loss: nan\r\nEpoch: 1/1, Iteration: 7/160782, Training Loss: nan\r\nTraceback (most recent call last):\r\n  File \"artifacts/main.py\", line 61, in <module>\r\n    actor_trainer.train()\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/chatllama/rlhf/actor.py\", line 340, in train\r\n    est_output = self.model_engine(\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/deepspeed/utils/nvtx.py\", line 11, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/deepspeed/runtime/engine.py\", line 1832, in forward\r\n    loss = self.module(*inputs, **kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"<@beartype(chatllama.rlhf.actor.ActorModel.forward) at 0x7f8153aad550>\", line 51, in forward\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/chatllama/rlhf/actor.py\", line 43, in forward\r\n    model_output = self.model.forward(\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/chatllama/llama_model.py\", line 512, in forward\r\n    logits = self._forward(tokens, attention_mask)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/chatllama/llama_model.py\", line 552, in _forward\r\n    h, _, _ = layer(h, kv_mask, freqs_cis)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\r\n    result = forward_call(*args, **kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/chatllama/llama_model.py\", line 438, in forward\r\n    attn, cache_k, cache_v = self.attention.forward(\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/chatllama/llama_model.py\", line 330, in forward\r\n    scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 31.75 GiB total capacity; 28.66 GiB already allocated; 41.94 MiB free; 29.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/314/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/314/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/313",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/313/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/313/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/313/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/313",
        "id": 1647602360,
        "node_id": "I_kwDOG1WDQc5iNGq4",
        "number": 313,
        "title": "[Speedster] Make Speedster optimize_model() return InferenceLearner also for StableDiffusion models",
        "user": {
            "login": "Telemaco019",
            "id": 24567368,
            "node_id": "MDQ6VXNlcjI0NTY3MzY4",
            "avatar_url": "https://avatars.githubusercontent.com/u/24567368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Telemaco019",
            "html_url": "https://github.com/Telemaco019",
            "followers_url": "https://api.github.com/users/Telemaco019/followers",
            "following_url": "https://api.github.com/users/Telemaco019/following{/other_user}",
            "gists_url": "https://api.github.com/users/Telemaco019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Telemaco019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Telemaco019/subscriptions",
            "organizations_url": "https://api.github.com/users/Telemaco019/orgs",
            "repos_url": "https://api.github.com/users/Telemaco019/repos",
            "events_url": "https://api.github.com/users/Telemaco019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Telemaco019/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4981513383,
                "node_id": "LA_kwDOG1WDQc8AAAABKOvcpw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/speedster",
                "name": "speedster",
                "color": "bfdadc",
                "default": false,
                "description": "Issue related to the Speedster App"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-30T13:15:10Z",
        "updated_at": "2023-04-03T10:09:53Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "body": "Speedster `optimize_model` function should return an object of type [BaseInferenceLearner](https://github.com/nebuly-ai/nebullvm/blob/main/nebullvm/operations/inference_learners/base.py#L41). However when optimizing Stable Diffusion models, the function returns an object of type `StableDiffusionPipeline`. \r\n\r\nIt would be useful if the function returned an Inference Learner also for Stable Diffusion to have a common API and expose utility methods such as `save()` and `get_size()` on the optimized models.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/313/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/313/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/312",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/312/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/312/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/312/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/312",
        "id": 1645536294,
        "node_id": "I_kwDOG1WDQc5iFOQm",
        "number": 312,
        "title": "[ChatLLaMA] RLHF Training: dimension mismatch",
        "user": {
            "login": "BigRoddy",
            "id": 45114253,
            "node_id": "MDQ6VXNlcjQ1MTE0MjUz",
            "avatar_url": "https://avatars.githubusercontent.com/u/45114253?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BigRoddy",
            "html_url": "https://github.com/BigRoddy",
            "followers_url": "https://api.github.com/users/BigRoddy/followers",
            "following_url": "https://api.github.com/users/BigRoddy/following{/other_user}",
            "gists_url": "https://api.github.com/users/BigRoddy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BigRoddy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BigRoddy/subscriptions",
            "organizations_url": "https://api.github.com/users/BigRoddy/orgs",
            "repos_url": "https://api.github.com/users/BigRoddy/repos",
            "events_url": "https://api.github.com/users/BigRoddy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BigRoddy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-03-29T10:47:07Z",
        "updated_at": "2023-04-03T15:32:44Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I am getting the following error when doing RLHF training:\r\nTraceback (most recent call last):\r\n  File \"/code/main.py\", in \r\n    rlhf_trainer.train()\r\n  File \"/code/trainer.py\", in train\r\n    self.learn(memories)\r\n  File \"/code/trainer.py\", in learn\r\n    surr1 = advantages * ratios\r\nRuntimeError: The size of tensor a (29) must match the size of tensor b (38) at non-singleton dimension 1\r\n\r\nAnd I output some shape of the tensors:\r\nrewards shape: torch.Size([1, 29])\r\nold_values shape: torch.Size([1, 29])\r\nactions_logits shape: torch.Size([1, 38, 50272])\r\nold_actions_log_probs shape: torch.Size([1, 38])\r\nratios shape: torch.Size([1, 38])\r\nadvantages shape: torch.Size([1, 29])\r\n\r\nThis seems to be due to the fact that my actor and critic use different family models (opt-125m and gpt2)?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/312/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/312/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/311",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/311/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/311/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/311/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/311",
        "id": 1644272838,
        "node_id": "PR_kwDOG1WDQc5NFgzV",
        "number": 311,
        "title": "Add Generator and FloatTensor to torch class",
        "user": {
            "login": "Telemaco019",
            "id": 24567368,
            "node_id": "MDQ6VXNlcjI0NTY3MzY4",
            "avatar_url": "https://avatars.githubusercontent.com/u/24567368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Telemaco019",
            "html_url": "https://github.com/Telemaco019",
            "followers_url": "https://api.github.com/users/Telemaco019/followers",
            "following_url": "https://api.github.com/users/Telemaco019/following{/other_user}",
            "gists_url": "https://api.github.com/users/Telemaco019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Telemaco019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Telemaco019/subscriptions",
            "organizations_url": "https://api.github.com/users/Telemaco019/orgs",
            "repos_url": "https://api.github.com/users/Telemaco019/repos",
            "events_url": "https://api.github.com/users/Telemaco019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Telemaco019/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-28T16:12:25Z",
        "updated_at": "2023-03-29T15:45:29Z",
        "closed_at": "2023-03-29T15:45:29Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/311",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/311",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/311.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/311.patch",
            "merged_at": "2023-03-29T15:45:29Z"
        },
        "body": "Same as https://github.com/nebuly-ai/nebullvm/pull/296 - add missing modules",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/311/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/311/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/310",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/310/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/310/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/310/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/310",
        "id": 1644006850,
        "node_id": "I_kwDOG1WDQc5h_Y3C",
        "number": 310,
        "title": "[Nebullvm] Add option to Nebullvm auto-installer for installing **all** libraries",
        "user": {
            "login": "Telemaco019",
            "id": 24567368,
            "node_id": "MDQ6VXNlcjI0NTY3MzY4",
            "avatar_url": "https://avatars.githubusercontent.com/u/24567368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Telemaco019",
            "html_url": "https://github.com/Telemaco019",
            "followers_url": "https://api.github.com/users/Telemaco019/followers",
            "following_url": "https://api.github.com/users/Telemaco019/following{/other_user}",
            "gists_url": "https://api.github.com/users/Telemaco019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Telemaco019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Telemaco019/subscriptions",
            "organizations_url": "https://api.github.com/users/Telemaco019/orgs",
            "repos_url": "https://api.github.com/users/Telemaco019/repos",
            "events_url": "https://api.github.com/users/Telemaco019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Telemaco019/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4981513383,
                "node_id": "LA_kwDOG1WDQc8AAAABKOvcpw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/speedster",
                "name": "speedster",
                "color": "bfdadc",
                "default": false,
                "description": "Issue related to the Speedster App"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-28T13:52:34Z",
        "updated_at": "2023-03-28T14:05:53Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "body": "## Description\r\nThe Nebullvm auto-installer (`nebullvm.installers.auto_installer`) installs only the libraries available for the current hardware, even when `--all` options are specified. This means that if I run the installer on a machine with no GPUs available, all the libraries required for GPU optimization won't be installed.\r\n\r\nIt would be useful to have an option that allows installing *all* libraries, including GPU ones, even if no GPUs are available. Without this option, when using the installer inside a Docker image build, it is required to build the image on a GPU-enabled node. Otherwise, all the GPUs libraries won't be installed. \r\n\r\n**This prevents me from using the Nebullvm auto-installer in GitHub actions used for building Docker images**, as no GPUs are available. \r\n\r\n### Proposed solution\r\nAdd `--all` option to auto_installer CLI\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/310/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/310/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/309",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/309/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/309/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/309/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/309",
        "id": 1643738618,
        "node_id": "I_kwDOG1WDQc5h-XX6",
        "number": 309,
        "title": "[ChatLLaMA] Training process halted",
        "user": {
            "login": "MuffinC",
            "id": 56317821,
            "node_id": "MDQ6VXNlcjU2MzE3ODIx",
            "avatar_url": "https://avatars.githubusercontent.com/u/56317821?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/MuffinC",
            "html_url": "https://github.com/MuffinC",
            "followers_url": "https://api.github.com/users/MuffinC/followers",
            "following_url": "https://api.github.com/users/MuffinC/following{/other_user}",
            "gists_url": "https://api.github.com/users/MuffinC/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/MuffinC/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/MuffinC/subscriptions",
            "organizations_url": "https://api.github.com/users/MuffinC/orgs",
            "repos_url": "https://api.github.com/users/MuffinC/repos",
            "events_url": "https://api.github.com/users/MuffinC/events{/privacy}",
            "received_events_url": "https://api.github.com/users/MuffinC/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-03-28T11:24:37Z",
        "updated_at": "2023-06-17T13:00:03Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "![image](https://user-images.githubusercontent.com/56317821/228220945-c73ad45f-e57a-4ace-98b3-8edabc636bb2.png)\r\n\r\nHi, it seemed like the training process was fine unitl it hit iteration 71? ran the line: python artifacts/main.py artifacts/config/config.yaml --type ALL",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/309/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/309/timeline",
        "performed_via_github_app": null,
        "state_reason": "reopened"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/308",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/308/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/308/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/308/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/308",
        "id": 1643671501,
        "node_id": "I_kwDOG1WDQc5h-G_N",
        "number": 308,
        "title": "[Speedster] Would  nebullvm support YoloV8 segmentation soon or at all ?",
        "user": {
            "login": "flyzxm5177",
            "id": 36351639,
            "node_id": "MDQ6VXNlcjM2MzUxNjM5",
            "avatar_url": "https://avatars.githubusercontent.com/u/36351639?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/flyzxm5177",
            "html_url": "https://github.com/flyzxm5177",
            "followers_url": "https://api.github.com/users/flyzxm5177/followers",
            "following_url": "https://api.github.com/users/flyzxm5177/following{/other_user}",
            "gists_url": "https://api.github.com/users/flyzxm5177/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/flyzxm5177/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/flyzxm5177/subscriptions",
            "organizations_url": "https://api.github.com/users/flyzxm5177/orgs",
            "repos_url": "https://api.github.com/users/flyzxm5177/repos",
            "events_url": "https://api.github.com/users/flyzxm5177/events{/privacy}",
            "received_events_url": "https://api.github.com/users/flyzxm5177/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4981513383,
                "node_id": "LA_kwDOG1WDQc8AAAABKOvcpw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/speedster",
                "name": "speedster",
                "color": "bfdadc",
                "default": false,
                "description": "Issue related to the Speedster App"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-28T10:40:59Z",
        "updated_at": "2023-03-28T14:06:08Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/308/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/308/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/307",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/307/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/307/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/307/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/307",
        "id": 1643606205,
        "node_id": "I_kwDOG1WDQc5h93C9",
        "number": 307,
        "title": "[ChatLLaMA] train RL on multi-gpu",
        "user": {
            "login": "EthanChen1234",
            "id": 45474996,
            "node_id": "MDQ6VXNlcjQ1NDc0OTk2",
            "avatar_url": "https://avatars.githubusercontent.com/u/45474996?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/EthanChen1234",
            "html_url": "https://github.com/EthanChen1234",
            "followers_url": "https://api.github.com/users/EthanChen1234/followers",
            "following_url": "https://api.github.com/users/EthanChen1234/following{/other_user}",
            "gists_url": "https://api.github.com/users/EthanChen1234/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/EthanChen1234/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/EthanChen1234/subscriptions",
            "organizations_url": "https://api.github.com/users/EthanChen1234/orgs",
            "repos_url": "https://api.github.com/users/EthanChen1234/repos",
            "events_url": "https://api.github.com/users/EthanChen1234/events{/privacy}",
            "received_events_url": "https://api.github.com/users/EthanChen1234/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-03-28T10:01:11Z",
        "updated_at": "2023-03-31T13:59:15Z",
        "closed_at": "2023-03-31T13:59:15Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi, thanks for your great job.\r\n\r\nI'm training actor and reward, setting \" deepspeed_enable: True\" to enable multi-gpu training.\r\n\r\nBut, when training the RL with multi-gpu, I encountered various errors.\r\n\r\ncould you send me the  config.yaml and starting command for RL multi-gpu training?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/307/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/307/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/306",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/306/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/306/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/306/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/306",
        "id": 1642470208,
        "node_id": "PR_kwDOG1WDQc5M_bXI",
        "number": 306,
        "title": "Refactor of models and trainers with base class for common methods",
        "user": {
            "login": "PierpaoloSorbellini",
            "id": 47692350,
            "node_id": "MDQ6VXNlcjQ3NjkyMzUw",
            "avatar_url": "https://avatars.githubusercontent.com/u/47692350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PierpaoloSorbellini",
            "html_url": "https://github.com/PierpaoloSorbellini",
            "followers_url": "https://api.github.com/users/PierpaoloSorbellini/followers",
            "following_url": "https://api.github.com/users/PierpaoloSorbellini/following{/other_user}",
            "gists_url": "https://api.github.com/users/PierpaoloSorbellini/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PierpaoloSorbellini/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PierpaoloSorbellini/subscriptions",
            "organizations_url": "https://api.github.com/users/PierpaoloSorbellini/orgs",
            "repos_url": "https://api.github.com/users/PierpaoloSorbellini/repos",
            "events_url": "https://api.github.com/users/PierpaoloSorbellini/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PierpaoloSorbellini/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-27T16:58:35Z",
        "updated_at": "2023-04-20T15:49:06Z",
        "closed_at": null,
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/306",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/306",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/306.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/306.patch",
            "merged_at": null
        },
        "body": "- Refactor models and trainers to avoid code replication.\r\n- Added logs with loguru package.\r\n- Fix logs with MultiGPU trainers.\r\n- Added support for LoRA with PEFT library.\r\n- Added support for load_8bit option with HF models.\r\n- Added self-instruct dataset of HF.\r\n- Added CerebrasGPT and Decapoda LLaMA models from HF.\r\n- Added mixed-precision training to reduce GPU memory requirements.\r\n- Fixed RLHF KL divergence equation.\r\n- Added support to keep only the last n checkpoints for all training.\r\n- Added generation of negative examples when creating the reward dataset to improve the quality of the reward model.\r\n- Improved stability of MultiGPU training with both Accelerate form HF and DeepSpeed.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/306/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/306/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/305",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/305/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/305/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/305/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/305",
        "id": 1642254535,
        "node_id": "PR_kwDOG1WDQc5M-tb2",
        "number": 305,
        "title": "Update config.yaml",
        "user": {
            "login": "PierpaoloSorbellini",
            "id": 47692350,
            "node_id": "MDQ6VXNlcjQ3NjkyMzUw",
            "avatar_url": "https://avatars.githubusercontent.com/u/47692350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PierpaoloSorbellini",
            "html_url": "https://github.com/PierpaoloSorbellini",
            "followers_url": "https://api.github.com/users/PierpaoloSorbellini/followers",
            "following_url": "https://api.github.com/users/PierpaoloSorbellini/following{/other_user}",
            "gists_url": "https://api.github.com/users/PierpaoloSorbellini/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PierpaoloSorbellini/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PierpaoloSorbellini/subscriptions",
            "organizations_url": "https://api.github.com/users/PierpaoloSorbellini/orgs",
            "repos_url": "https://api.github.com/users/PierpaoloSorbellini/repos",
            "events_url": "https://api.github.com/users/PierpaoloSorbellini/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PierpaoloSorbellini/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-27T14:51:35Z",
        "updated_at": "2023-03-27T14:55:10Z",
        "closed_at": "2023-03-27T14:55:10Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/305",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/305",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/305.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/305.patch",
            "merged_at": "2023-03-27T14:55:10Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/305/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/305/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/304",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/304/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/304/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/304/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/304",
        "id": 1642247244,
        "node_id": "PR_kwDOG1WDQc5M-r46",
        "number": 304,
        "title": "Add peft_config.yaml description",
        "user": {
            "login": "PierpaoloSorbellini",
            "id": 47692350,
            "node_id": "MDQ6VXNlcjQ3NjkyMzUw",
            "avatar_url": "https://avatars.githubusercontent.com/u/47692350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PierpaoloSorbellini",
            "html_url": "https://github.com/PierpaoloSorbellini",
            "followers_url": "https://api.github.com/users/PierpaoloSorbellini/followers",
            "following_url": "https://api.github.com/users/PierpaoloSorbellini/following{/other_user}",
            "gists_url": "https://api.github.com/users/PierpaoloSorbellini/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PierpaoloSorbellini/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PierpaoloSorbellini/subscriptions",
            "organizations_url": "https://api.github.com/users/PierpaoloSorbellini/orgs",
            "repos_url": "https://api.github.com/users/PierpaoloSorbellini/repos",
            "events_url": "https://api.github.com/users/PierpaoloSorbellini/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PierpaoloSorbellini/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-27T14:47:51Z",
        "updated_at": "2023-03-27T14:54:50Z",
        "closed_at": "2023-03-27T14:54:50Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/304",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/304",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/304.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/304.patch",
            "merged_at": "2023-03-27T14:54:50Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/304/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/304/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/303",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/303/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/303/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/303/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/303",
        "id": 1642040646,
        "node_id": "PR_kwDOG1WDQc5M9-cZ",
        "number": 303,
        "title": "[ChatLLaMA] Add support for LoRA and PeFT efficient training",
        "user": {
            "login": "diegofiori",
            "id": 38586138,
            "node_id": "MDQ6VXNlcjM4NTg2MTM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/38586138?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/diegofiori",
            "html_url": "https://github.com/diegofiori",
            "followers_url": "https://api.github.com/users/diegofiori/followers",
            "following_url": "https://api.github.com/users/diegofiori/following{/other_user}",
            "gists_url": "https://api.github.com/users/diegofiori/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/diegofiori/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/diegofiori/subscriptions",
            "organizations_url": "https://api.github.com/users/diegofiori/orgs",
            "repos_url": "https://api.github.com/users/diegofiori/repos",
            "events_url": "https://api.github.com/users/diegofiori/events{/privacy}",
            "received_events_url": "https://api.github.com/users/diegofiori/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-27T13:22:45Z",
        "updated_at": "2023-03-27T13:25:35Z",
        "closed_at": "2023-03-27T13:25:35Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/303",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/303",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/303.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/303.patch",
            "merged_at": "2023-03-27T13:25:35Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/303/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/303/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/302",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/302/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/302/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/302/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/302",
        "id": 1641492874,
        "node_id": "I_kwDOG1WDQc5h1zGK",
        "number": 302,
        "title": "[ChatLLaMA] No GPU Detected issue",
        "user": {
            "login": "MuffinC",
            "id": 56317821,
            "node_id": "MDQ6VXNlcjU2MzE3ODIx",
            "avatar_url": "https://avatars.githubusercontent.com/u/56317821?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/MuffinC",
            "html_url": "https://github.com/MuffinC",
            "followers_url": "https://api.github.com/users/MuffinC/followers",
            "following_url": "https://api.github.com/users/MuffinC/following{/other_user}",
            "gists_url": "https://api.github.com/users/MuffinC/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/MuffinC/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/MuffinC/subscriptions",
            "organizations_url": "https://api.github.com/users/MuffinC/orgs",
            "repos_url": "https://api.github.com/users/MuffinC/repos",
            "events_url": "https://api.github.com/users/MuffinC/events{/privacy}",
            "received_events_url": "https://api.github.com/users/MuffinC/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-27T06:53:26Z",
        "updated_at": "2023-03-28T11:18:42Z",
        "closed_at": "2023-03-28T11:18:42Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "When trying to run the training step python artifacts/main.py artifacts/config/config.yaml --type ALL\r\nIt replies with  ValueError(\"No Gpu available\") . Is there anyone with advice on this? Currently trying to run in a azure cloud gpu vm. The gpu is NVIDIA Corporation GP100GL [Tesla P100 PCIe 16gb]. If there is any more information required please do reach out thanks! ",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/302/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/302/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/301",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/301/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/301/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/301/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/301",
        "id": 1641137833,
        "node_id": "I_kwDOG1WDQc5h0cap",
        "number": 301,
        "title": "[Speedster] Running into \"CUDA out of memory\" on an A100",
        "user": {
            "login": "michaelbogdan",
            "id": 5346098,
            "node_id": "MDQ6VXNlcjUzNDYwOTg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/5346098?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/michaelbogdan",
            "html_url": "https://github.com/michaelbogdan",
            "followers_url": "https://api.github.com/users/michaelbogdan/followers",
            "following_url": "https://api.github.com/users/michaelbogdan/following{/other_user}",
            "gists_url": "https://api.github.com/users/michaelbogdan/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/michaelbogdan/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/michaelbogdan/subscriptions",
            "organizations_url": "https://api.github.com/users/michaelbogdan/orgs",
            "repos_url": "https://api.github.com/users/michaelbogdan/repos",
            "events_url": "https://api.github.com/users/michaelbogdan/events{/privacy}",
            "received_events_url": "https://api.github.com/users/michaelbogdan/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4981513383,
                "node_id": "LA_kwDOG1WDQc8AAAABKOvcpw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/speedster",
                "name": "speedster",
                "color": "bfdadc",
                "default": false,
                "description": "Issue related to the Speedster App"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 9,
        "created_at": "2023-03-26T23:18:49Z",
        "updated_at": "2023-03-29T09:55:14Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I am trying `speedster` to optimize inference of `oasst-sft-1-pythia-12b` on a rented A100 with 40GB VRAM on Lambda Cloud. The code I use is pasted here:\r\n\r\n```\r\nfrom speedster import optimize_model, save_model\r\nfrom transformers import GPTNeoXForCausalLM, AutoTokenizer\r\n\r\ncache_directory = \"./model_cache\"\r\nmodel = GPTNeoXForCausalLM.from_pretrained(\"OpenAssistant/oasst-sft-1-pythia-12b\", cache_dir=cache_directory).half()\r\ntokenizer = AutoTokenizer.from_pretrained(\"OpenAssistant/oasst-sft-1-pythia-12b\", cache_dir=cache_directory)\r\n\r\ntext = \"<|prompter|>Answer with as many words as possible: Do horses lay eggs?<|endoftext|><|assistant|>\"\r\ninput_dict = tokenizer(text, return_tensors=\"pt\")\r\ninput_data = [input_dict for _ in range(100)]\r\n    \r\noptimized_model = optimize_model(\r\n    model, \r\n    input_data=input_data,\r\n    optimization_time=\"constrained\",\r\n    metric_drop_ths=0.05\r\n)\r\n\r\nsave_model(optimized_model, \"model_save_path\")\r\n```\r\n\r\nHowever, I always run into error messages like\r\n\r\n```\r\n2023-03-26 23:06:11 | INFO     | Running Speedster on GPU:0\r\n2023-03-26 23:06:17 | WARNING  | Dynamic shape info has not been provided for the HuggingFace model. The resulting optimized model will be usable only with a fixed input shape. To optimize the model for dynamic shapes, please look here: https://nebuly.gitbook.io/nebuly/modules/speedster/how-to-guides#using-dynamic-shape.\r\n2023-03-26 23:06:23 | INFO     | Benchmark performance of original model\r\n2023-03-26 23:06:29 | INFO     | Original model latency: 0.03949021577835083 sec/iter\r\n2023-03-26 23:06:40 | WARNING  | Exception raised during conversion from torch to onnx model. ONNX pipeline will be unavailable.\r\n2023-03-26 23:07:08 | INFO     | [1/1] Running PyTorch Optimization Pipeline\r\n2023-03-26 23:07:16 | INFO     | Optimizing with PyTorchTensorRTCompiler and q_type: None.\r\n2023-03-26 23:07:44 | WARNING  | Optimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TENSOR_RT_TORCH. Got error [Error thrown at core/conversion/conversionctx/ConversionCtx.cpp:169] Building serialized network failed in TensorRT\r\n. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\r\n```\r\n\r\nWhat are some possibilities I can try? I know that the model itself takes up about 25GiB of VRAM, so it doesn't fit into the GPU's memory twice. It seems like `speedster` is not flushing the previous model out of memory.\r\n\r\nI had the idea to just load the model into actual RAM and perform the optimizations there, but reading the docs I only found the `device` paramater, which controls the target for inference, no option for where the compilation takes place.\r\n\r\nCan you maybe help me?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/301/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/301/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/300",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/300/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/300/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/300/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/300",
        "id": 1640854179,
        "node_id": "PR_kwDOG1WDQc5M6Gxi",
        "number": 300,
        "title": "Add tokenizer path to load_tokenizer for llama models",
        "user": {
            "login": "PierpaoloSorbellini",
            "id": 47692350,
            "node_id": "MDQ6VXNlcjQ3NjkyMzUw",
            "avatar_url": "https://avatars.githubusercontent.com/u/47692350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PierpaoloSorbellini",
            "html_url": "https://github.com/PierpaoloSorbellini",
            "followers_url": "https://api.github.com/users/PierpaoloSorbellini/followers",
            "following_url": "https://api.github.com/users/PierpaoloSorbellini/following{/other_user}",
            "gists_url": "https://api.github.com/users/PierpaoloSorbellini/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PierpaoloSorbellini/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PierpaoloSorbellini/subscriptions",
            "organizations_url": "https://api.github.com/users/PierpaoloSorbellini/orgs",
            "repos_url": "https://api.github.com/users/PierpaoloSorbellini/repos",
            "events_url": "https://api.github.com/users/PierpaoloSorbellini/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PierpaoloSorbellini/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-26T10:28:46Z",
        "updated_at": "2023-03-26T12:20:27Z",
        "closed_at": "2023-03-26T12:20:26Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/300",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/300",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/300.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/300.patch",
            "merged_at": "2023-03-26T12:20:26Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/300/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/300/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/299",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/299/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/299/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/299/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/299",
        "id": 1640704865,
        "node_id": "I_kwDOG1WDQc5hyyth",
        "number": 299,
        "title": "[ChatLLaMA] RLHF Training: Prompt too long",
        "user": {
            "login": "swang99",
            "id": 35492791,
            "node_id": "MDQ6VXNlcjM1NDkyNzkx",
            "avatar_url": "https://avatars.githubusercontent.com/u/35492791?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/swang99",
            "html_url": "https://github.com/swang99",
            "followers_url": "https://api.github.com/users/swang99/followers",
            "following_url": "https://api.github.com/users/swang99/following{/other_user}",
            "gists_url": "https://api.github.com/users/swang99/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/swang99/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/swang99/subscriptions",
            "organizations_url": "https://api.github.com/users/swang99/orgs",
            "repos_url": "https://api.github.com/users/swang99/repos",
            "events_url": "https://api.github.com/users/swang99/events{/privacy}",
            "received_events_url": "https://api.github.com/users/swang99/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 3825828828,
                "node_id": "LA_kwDOG1WDQc7kCYPc",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/bug",
                "name": "bug",
                "color": "d73a4a",
                "default": true,
                "description": "Something isn't working"
            },
            {
                "id": 5241046875,
                "node_id": "LA_kwDOG1WDQc8AAAABOGQHWw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/chatllama",
                "name": "chatllama",
                "color": "bfd4f2",
                "default": false,
                "description": "Issue related to the ChatLLaMA module"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 8,
        "created_at": "2023-03-25T23:59:13Z",
        "updated_at": "2023-04-14T13:56:05Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I am getting the following error when doing RLHF training. I decreased the max_sequence_length in my actor configuration to 1024 because there were errors with training for me when set to 2048. Is my actor max_sequence_length too small, and does this mean I have to redo pre-training with a larger max sequence? There isn't a way to change the state_length to my knowledge.\r\n\r\nValueError: The prompt is too long w.r.t the model sequence length \r\nmax_sequence_length=1024\r\nstate_length=1024\r\nmin_tokens=100\r\nmax_tokens=2048\r\nmax_generation_possible=0",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/299/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/299/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/298",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/298/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/298/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/298/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/298",
        "id": 1640586658,
        "node_id": "I_kwDOG1WDQc5hyV2i",
        "number": 298,
        "title": "[Chatllama] KL Divergence equation",
        "user": {
            "login": "mountinyy",
            "id": 28773464,
            "node_id": "MDQ6VXNlcjI4NzczNDY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/28773464?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mountinyy",
            "html_url": "https://github.com/mountinyy",
            "followers_url": "https://api.github.com/users/mountinyy/followers",
            "following_url": "https://api.github.com/users/mountinyy/following{/other_user}",
            "gists_url": "https://api.github.com/users/mountinyy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mountinyy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mountinyy/subscriptions",
            "organizations_url": "https://api.github.com/users/mountinyy/orgs",
            "repos_url": "https://api.github.com/users/mountinyy/repos",
            "events_url": "https://api.github.com/users/mountinyy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mountinyy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-03-25T17:03:06Z",
        "updated_at": "2023-04-14T13:58:12Z",
        "closed_at": null,
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "Hello, I have a quick question.\r\nI know most RLHF structure use KL divergence.\r\n\r\nhttps://github.com/nebuly-ai/nebullvm/blob/aad1c09ce20946294df3ec83569bad9496f58d0e/apps/accelerate/chatllama/chatllama/rlhf/trainer.py#L871-L877\r\n\r\nHowever, when you see the InstructGPT paper([link](https://arxiv.org/pdf/2203.02155.pdf)), they are actually **dividing  (policy for RL) by (policy for SFT)**. (which is **action_log_prob - old_action_log_probs** from your code).\r\nAnd I saw some codes that actually follow this instructions like \r\n[ColossalAI](https://github.com/hpcaitech/ColossalAI/blob/1216d1e7bdf223d831895e34c01fb40df36ea9c7/applications/ChatGPT/chatgpt/models/utils.py#L9-L28)\r\n<br>\r\nBut it seems like you are doing oppositely.\r\nI know you are converting loss to negative since you're applying RL reward for deep learning loss.\r\nBut even consdiering that, you are adding those KL value, not subtracting it, whcih means it's still oppostie to **action_log_prob - old_action_log_probs**. \r\n\r\nDo you have any special reason for this?\r\n\r\nThank you for reading :)\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/298/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/298/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/297",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/297/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/297/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/297/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/297",
        "id": 1640509479,
        "node_id": "I_kwDOG1WDQc5hyDAn",
        "number": 297,
        "title": "Tokenizer Issue",
        "user": {
            "login": "yradwan147",
            "id": 54478004,
            "node_id": "MDQ6VXNlcjU0NDc4MDA0",
            "avatar_url": "https://avatars.githubusercontent.com/u/54478004?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yradwan147",
            "html_url": "https://github.com/yradwan147",
            "followers_url": "https://api.github.com/users/yradwan147/followers",
            "following_url": "https://api.github.com/users/yradwan147/following{/other_user}",
            "gists_url": "https://api.github.com/users/yradwan147/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yradwan147/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yradwan147/subscriptions",
            "organizations_url": "https://api.github.com/users/yradwan147/orgs",
            "repos_url": "https://api.github.com/users/yradwan147/repos",
            "events_url": "https://api.github.com/users/yradwan147/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yradwan147/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-03-25T13:06:40Z",
        "updated_at": "2023-03-26T12:22:32Z",
        "closed_at": "2023-03-26T12:22:31Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Whenever I try to train the model on the 7B LLaMa weights (Actor training), I get the following error:\r\n\r\nCurrent device used :cuda\r\nStart cleaning the dataset for Actor\r\nTraceback (most recent call last):\r\n  File \"artifacts/main.py\", line 59, in <module>\r\n    BaseDataset.clean_dataset(config.actor)\r\n  File \"/opt/conda/envs/newCondaEnvironment/lib/python3.8/site-packages/chatllama/rlhf/dataset.py\", line 136, in clean_dataset\r\n    a_tokenizer = ActorModel.load_tokenizer(config)\r\n  File \"/opt/conda/envs/newCondaEnvironment/lib/python3.8/site-packages/chatllama/rlhf/actor.py\", line 146, in load_tokenizer\r\n    tokenizer = load_tokenizer()\r\nTypeError: load_tokenizer() missing 1 required positional argument: 'tokenizer_path'\r\n\r\nEven though my config.yaml file includes the tokenizer_path in the actor config:\r\n\r\nactor_config:\r\n  model: \"llama-7B\"\r\n  model_folder: \"/kaggle/input/chatllamadataset/7B\"\r\n  tokenizer_path: \"/kaggle/input/chatllamadataset/tokenizer.model\"\r\n\r\n\r\nWhat is the issue here?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/297/reactions",
            "total_count": 1,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 1
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/297/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/296",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/296/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/296/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/296/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/296",
        "id": 1639793919,
        "node_id": "PR_kwDOG1WDQc5M2siX",
        "number": 296,
        "title": "[Nebullvm] Add Generator and FloatTensor classes to torch optional modules",
        "user": {
            "login": "Telemaco019",
            "id": 24567368,
            "node_id": "MDQ6VXNlcjI0NTY3MzY4",
            "avatar_url": "https://avatars.githubusercontent.com/u/24567368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Telemaco019",
            "html_url": "https://github.com/Telemaco019",
            "followers_url": "https://api.github.com/users/Telemaco019/followers",
            "following_url": "https://api.github.com/users/Telemaco019/following{/other_user}",
            "gists_url": "https://api.github.com/users/Telemaco019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Telemaco019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Telemaco019/subscriptions",
            "organizations_url": "https://api.github.com/users/Telemaco019/orgs",
            "repos_url": "https://api.github.com/users/Telemaco019/repos",
            "events_url": "https://api.github.com/users/Telemaco019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Telemaco019/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-24T17:37:01Z",
        "updated_at": "2023-03-24T18:40:12Z",
        "closed_at": "2023-03-24T18:40:12Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/296",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/296",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/296.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/296.patch",
            "merged_at": "2023-03-24T18:40:12Z"
        },
        "body": "Add Generator and FloatTensor classes to torch optional modules so that it is possible to import module `nebullvm.tools.diffusers.py` even if torch is not installed",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/296/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/296/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/295",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/295/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/295/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/295/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/295",
        "id": 1639015917,
        "node_id": "I_kwDOG1WDQc5hsWXt",
        "number": 295,
        "title": "Add Support for PEFT fine-tuning",
        "user": {
            "login": "PierpaoloSorbellini",
            "id": 47692350,
            "node_id": "MDQ6VXNlcjQ3NjkyMzUw",
            "avatar_url": "https://avatars.githubusercontent.com/u/47692350?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/PierpaoloSorbellini",
            "html_url": "https://github.com/PierpaoloSorbellini",
            "followers_url": "https://api.github.com/users/PierpaoloSorbellini/followers",
            "following_url": "https://api.github.com/users/PierpaoloSorbellini/following{/other_user}",
            "gists_url": "https://api.github.com/users/PierpaoloSorbellini/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/PierpaoloSorbellini/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/PierpaoloSorbellini/subscriptions",
            "organizations_url": "https://api.github.com/users/PierpaoloSorbellini/orgs",
            "repos_url": "https://api.github.com/users/PierpaoloSorbellini/repos",
            "events_url": "https://api.github.com/users/PierpaoloSorbellini/events{/privacy}",
            "received_events_url": "https://api.github.com/users/PierpaoloSorbellini/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 5241046875,
                "node_id": "LA_kwDOG1WDQc8AAAABOGQHWw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/chatllama",
                "name": "chatllama",
                "color": "bfd4f2",
                "default": false,
                "description": "Issue related to the ChatLLaMA module"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-24T09:18:32Z",
        "updated_at": "2023-03-31T14:11:18Z",
        "closed_at": null,
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "body": "# Description\r\n\r\nSupports for PEFT for chatllama models and trainings\r\n\r\n# TODO\r\n\r\n- [x] Add PEFT to Enable Parameter efficient fine-tuning in actor, reward and critic models.\r\n- [ ] Check RLHF stability.\r\n- [ ] Check quality of adapted models vs. fully trained vs. reduced training requirements.\r\n- [ ] Update README.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/295/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/295/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/294",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/294/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/294/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/294/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/294",
        "id": 1638964701,
        "node_id": "PR_kwDOG1WDQc5Mz5qN",
        "number": 294,
        "title": "[Speedster]\u00a0Fix bug when diffusers is not installed",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-24T08:44:07Z",
        "updated_at": "2023-03-24T10:18:01Z",
        "closed_at": "2023-03-24T10:18:01Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/294",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/294",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/294.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/294.patch",
            "merged_at": "2023-03-24T10:18:01Z"
        },
        "body": "Fixes #293",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/294/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/294/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/293",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/293/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/293/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/293/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/293",
        "id": 1638963640,
        "node_id": "I_kwDOG1WDQc5hsJm4",
        "number": 293,
        "title": "[Speedster]\u00a0type object \u2018DummyClass\u2019 has no attribute \u2018models\u2019",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-24T08:43:20Z",
        "updated_at": "2023-05-21T01:59:54Z",
        "closed_at": "2023-03-24T10:18:02Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "body": "This error appears when using the auto-installer to selectively install dependences. It can be solved by running `pip install diffusers`.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/293/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/293/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/292",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/292/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/292/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/292/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/292",
        "id": 1638692762,
        "node_id": "I_kwDOG1WDQc5hrHea",
        "number": 292,
        "title": "[ChatLlama] Train Actor with llama-7b model Error initializing torch.distributed using env:// rendezvous: ",
        "user": {
            "login": "balcklive",
            "id": 57667856,
            "node_id": "MDQ6VXNlcjU3NjY3ODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/57667856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balcklive",
            "html_url": "https://github.com/balcklive",
            "followers_url": "https://api.github.com/users/balcklive/followers",
            "following_url": "https://api.github.com/users/balcklive/following{/other_user}",
            "gists_url": "https://api.github.com/users/balcklive/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balcklive/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balcklive/subscriptions",
            "organizations_url": "https://api.github.com/users/balcklive/orgs",
            "repos_url": "https://api.github.com/users/balcklive/repos",
            "events_url": "https://api.github.com/users/balcklive/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balcklive/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-03-24T04:17:50Z",
        "updated_at": "2023-06-09T13:45:45Z",
        "closed_at": "2023-03-24T11:07:27Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "When I train actor model with llama-7b, I got this:\r\n\r\nCurrent device used :cuda\r\nlocal_rank: -1 world_size: -1\r\nTraceback (most recent call last):\r\n  File \"artifacts/main.py\", line 50, in <module>\r\n    actor_trainer = ActorTrainer(config.actor)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/chatllama/rlhf/actor.py\", line 313, in __init__\r\n    self.actor = ActorModel(config)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/chatllama/rlhf/actor.py\", line 57, in __init__\r\n    local_rank, world_size = setup_model_parallel()\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/chatllama/llama_model.py\", line 551, in setup_model_parallel\r\n    torch.distributed.init_process_group(\"nccl\")\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 888, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 235, in _env_rendezvous_handler\r\n    rank = int(_get_env_or_raise(\"RANK\"))\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/distributed/rendezvous.py\", line 220, in _get_env_or_raise\r\n    raise _env_error(env_var)\r\nValueError: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set\r\n\r\nmy config.yaml is as below:\r\ntrainer_config:\r\n  # learning rates\r\n  actor_lr: 0.00001\r\n  critic_lr: 0.00001\r\n  # PPO Hyperparameters\r\n  actor_eps_clip: 0.2\r\n  critic_eps_clip: 0.2\r\n  beta_s: 0.02\r\n  # path to examples to be sampled (training dataset) see rlhf_dataset.json\r\n  examples_path: \"./datasets/rlhf_training_data.json\"\r\n  # number of episodes and generation performed for each episode\r\n  # in the train() method\r\n  num_episodes: 5\r\n  max_timesteps: 1\r\n  # number of timesteps after which the learn() method is called \r\n  # (to update the weights)\r\n  update_timesteps: 1\r\n  # number of example sampled at each timestep\r\n  num_examples: 8\r\n  # batch and epochs for the training\r\n  batch_size: 1\r\n  epochs: 1\r\n  # number of learning steps (i.e. learn()) after which a checkpoint is saved\r\n  checkpoint_steps: 5\r\n  checkpoint_name: null\r\n\r\nactor_config:\r\n  model: \"llama-7B\"\r\n  model_folder: \"/home/zhoux/models/actor_model\"\r\n  tokenizer_path: \"/home/zhoux/models/actor_tokenizer\"\r\n  train_dataset_path: \"./datasets/actor_training_data.json\"\r\n  validation_dataset_path: null\r\n  # froze model embedding during training\r\n  froze_embeddings: True\r\n  # use fairscale layers to build the model instead of vanilla pytorch\r\n  use_fairscale: False\r\n  # max sequence length for the actor (i.e. prompt + completion) it depends on\r\n  # the model used.\r\n  max_sequence_length: 1500\r\n  # max tokens generated by the actor (completion only)\r\n  max_tokens: 2048\r\n  # minimum number of tokens generated by the actor\r\n  min_tokens: 100\r\n  # additional prompt tokens to be used for template or as safety\r\n  additonal_prompt_tokens: 20\r\n  # temperature for the actor\r\n  temperature: 0.5\r\n  batch_size: 1\r\n  # number iteration after print\r\n  iteration_per_print: 1\r\n  lr: 0.000009\r\n  epochs: 2\r\n  # number of backpropagation after saving the checkpoints\r\n  checkpoint_steps: 10000000000\r\n  # number of checkpoints to keep while removing the older \r\n  # (keep memory consumption of checkpoints reasonable)\r\n  n_checkpoints_to_keep: 5\r\n  # here specify the name of the actor checkpoint from which resume \r\n  # during actor training. If null load the last one.\r\n  checkpoint_name: null\r\n  # deepspeed settings\r\n  deepspeed_enable: False\r\n  deepspeed_config_path: \"./artifacts/config/ds_config.json\"\r\n  # accelerate settings\r\n  accelerate_enable: False",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/292/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/292/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/291",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/291/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/291/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/291/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/291",
        "id": 1637504486,
        "node_id": "PR_kwDOG1WDQc5Mu-Tj",
        "number": 291,
        "title": "[Chatllama] fix typo of discounted_rewards in PPO loss",
        "user": {
            "login": "mountinyy",
            "id": 28773464,
            "node_id": "MDQ6VXNlcjI4NzczNDY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/28773464?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mountinyy",
            "html_url": "https://github.com/mountinyy",
            "followers_url": "https://api.github.com/users/mountinyy/followers",
            "following_url": "https://api.github.com/users/mountinyy/following{/other_user}",
            "gists_url": "https://api.github.com/users/mountinyy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mountinyy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mountinyy/subscriptions",
            "organizations_url": "https://api.github.com/users/mountinyy/orgs",
            "repos_url": "https://api.github.com/users/mountinyy/repos",
            "events_url": "https://api.github.com/users/mountinyy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mountinyy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-23T13:02:17Z",
        "updated_at": "2023-03-23T13:24:37Z",
        "closed_at": "2023-03-23T13:24:37Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/291",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/291",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/291.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/291.patch",
            "merged_at": "2023-03-23T13:24:36Z"
        },
        "body": "# Issue\r\n**discounted_rewards.shape[0]** means **batch_size**, so it cannot properly calculate discounted_rewards.\r\nhttps://github.com/nebuly-ai/nebullvm/blob/8ccfadb00fa88bf0db6c605029a34bc6a1349655/apps/accelerate/chatllama/chatllama/rlhf/trainer.py#L880-L889\r\n\r\n# Solve\r\nTo fix this, I've changed to use **discounted_rewards.shape[1]** instead of discounted_rewards.shape[0].\r\n\r\nCloses #290 ",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/291/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/291/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/290",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/290/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/290/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/290/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/290",
        "id": 1637372638,
        "node_id": "I_kwDOG1WDQc5hmFLe",
        "number": 290,
        "title": "[Chatllama] discounted_rewards with PPO",
        "user": {
            "login": "mountinyy",
            "id": 28773464,
            "node_id": "MDQ6VXNlcjI4NzczNDY0",
            "avatar_url": "https://avatars.githubusercontent.com/u/28773464?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mountinyy",
            "html_url": "https://github.com/mountinyy",
            "followers_url": "https://api.github.com/users/mountinyy/followers",
            "following_url": "https://api.github.com/users/mountinyy/following{/other_user}",
            "gists_url": "https://api.github.com/users/mountinyy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mountinyy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mountinyy/subscriptions",
            "organizations_url": "https://api.github.com/users/mountinyy/orgs",
            "repos_url": "https://api.github.com/users/mountinyy/repos",
            "events_url": "https://api.github.com/users/mountinyy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mountinyy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-23T11:44:06Z",
        "updated_at": "2023-03-23T14:18:46Z",
        "closed_at": "2023-03-23T14:18:46Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "Hello.\r\nI was studying RLHF with Chatllama code, and found something the I couldn't undestand.\r\n\r\nWhen calculating **discounted_rewards** in PPO, it uses **discounted_rewards.shape[0]** for the loop.\r\nBut isn't it a batch_size?\r\nIf the size of **old_values** is [1, 1013](which is [batch_size, sequence_length]), then it only calculates for discounted_rewards[0, 0].\r\nWhy not use **discounted_rewards.shape[1]** for the loop so that it calculates all the discounted rewards according to the rewards?\r\n\r\nThank you for reading :)\r\nhttps://github.com/nebuly-ai/nebullvm/blob/8ccfadb00fa88bf0db6c605029a34bc6a1349655/apps/accelerate/chatllama/chatllama/rlhf/trainer.py#L880-L889",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/290/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/290/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/289",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/289/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/289/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/289/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/289",
        "id": 1637287890,
        "node_id": "I_kwDOG1WDQc5hlwfS",
        "number": 289,
        "title": "RL trainig RuntimeError: CUDA error: device-side assert triggered",
        "user": {
            "login": "balcklive",
            "id": 57667856,
            "node_id": "MDQ6VXNlcjU3NjY3ODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/57667856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balcklive",
            "html_url": "https://github.com/balcklive",
            "followers_url": "https://api.github.com/users/balcklive/followers",
            "following_url": "https://api.github.com/users/balcklive/following{/other_user}",
            "gists_url": "https://api.github.com/users/balcklive/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balcklive/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balcklive/subscriptions",
            "organizations_url": "https://api.github.com/users/balcklive/orgs",
            "repos_url": "https://api.github.com/users/balcklive/repos",
            "events_url": "https://api.github.com/users/balcklive/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balcklive/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-23T10:54:22Z",
        "updated_at": "2023-03-24T01:57:03Z",
        "closed_at": "2023-03-24T01:57:02Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I run my reward training and actor training successfully. when I run RL training the program paused as below:\r\n                                                                             \r\n(py38) [xx@xx~]$ accelerate launch artifacts/main.py artifacts/config/config.yaml --type RL                                                                                  \r\nCurrent device used :cuda\r\nNo previous model found at /home/zhoux/models/actor_model/actor for model gpt2-large.pt\r\nNo previous model found at /home/zhoux/models/critic_model/critic for model gpt2-large.pt\r\nInitializing Critic from Reward model...\r\nNo previous model found at /home/zhoux/models/critic_model/reward for model gpt2-large.pt\r\nCritic Model remains uninitialized\r\nNo previous model found at /home/zhoux/models/critic_model/critic for model gpt2-large.pt\r\nNo previous model found at /home/zhoux/models/reward_model/reward for model gpt2-large.pt\r\nStart RL Training\r\nLooking for checkpoints...\r\nNo previous checkpoint found at /home/zhoux/models/critic_model/checkpoints/critic for gpt2-large.pt\r\nLooking for checkpoints...\r\nNo previous checkpoint found at /home/zhoux/models/actor_model/checkpoints/actor_rl for gpt2-large.pt\r\nClearing conversations log\r\nEpisode: 1/100, Timestep: 1/1 Learning Cnt: 1/1\r\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n\r\nand then got this error:\r\nad: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1093: indexSelectSmallIndex: block: [7,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1093: indexSelectSmallIndex: block: [7,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1093: indexSelectSmallIndex: block: [7,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1093: indexSelectSmallIndex: block: [7,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1093: indexSelectSmallIndex: block: [7,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1093: indexSelectSmallIndex: block: [7,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1093: indexSelectSmallIndex: block: [7,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\nTraceback (most recent call last):\r\n  File \"artifacts/main.py\", line 48, in <module>\r\n    rlhf_trainer.train()\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/chatllama/rlhf/trainer.py\", line 1092, in train\r\n    ) = self.actorcritic.generate(\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"<@beartype(chatllama.rlhf.trainer.ActorCritic.generate) at 0x7f45ae974790>\", line 70, in generate\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/chatllama/rlhf/trainer.py\", line 310, in generate\r\n    actions, sequences_actor = self.actor.generate(\r\n  File \"<@beartype(chatllama.rlhf.actor.ActorModel.generate) at 0x7f45ae952a60>\", line 51, in generate\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/chatllama/rlhf/actor.py\", line 222, in generate\r\n    sequences = self.model.generate(\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1406, in generate\r\n    return self.greedy_search(\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/generation/utils.py\", line 2201, in greedy_search\r\n    outputs = self(\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1075, in forward\r\n    transformer_outputs = self.transformer(\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 899, in forward\r\n    outputs = block(\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 389, in forward\r\n    attn_outputs = self.attn(\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 330, in forward\r\n    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\r\n  File \"/home/zhoux/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 200, in _attn\r\n    mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\nmy GPU is A100, and got 80G memory, my config.yaml is as below:\r\n---\r\ntrainer_config:\r\n  # learning rates\r\n  actor_lr: 0.00001\r\n  critic_lr: 0.00001\r\n  # PPO Hyperparameters\r\n  actor_eps_clip: 0.2\r\n  critic_eps_clip: 0.2\r\n  beta_s: 0.02\r\n  # path to examples to be sampled (training dataset) see rlhf_dataset.json\r\n  examples_path: \"./datasets/rlhf_training_data.json\"\r\n  # number of episodes and generation performed for each episode\r\n  # in the train() method\r\n  num_episodes: 100\r\n  max_timesteps: 1\r\n  # number of timesteps after which the learn() method is called \r\n  # (to update the weights)\r\n  update_timesteps: 1\r\n  # number of example sampled at each timestep\r\n  num_examples: 8\r\n  # batch and epochs for the training\r\n  batch_size: 1\r\n  epochs: 1\r\n  # number of learning steps (i.e. learn()) after which a checkpoint is saved\r\n  checkpoint_steps: 5\r\n  checkpoint_name: null\r\n\r\nactor_config:\r\n  model: \"gpt2-large\"\r\n  model_folder: \"/home/zhoux/models/actor_model\"\r\n  tokenizer_path: \"/home/zhoux/models/actor_tokenizer\"\r\n  train_dataset_path: \"./datasets/actor_training_data.json\"\r\n  validation_dataset_path: null\r\n  # froze model embedding during training\r\n  froze_embeddings: True\r\n  # use fairscale layers to build the model instead of vanilla pytorch\r\n  use_fairscale: False\r\n  # max sequence length for the actor (i.e. prompt + completion) it depends on\r\n  # the model used.\r\n  max_sequence_length: 2048\r\n  # max tokens generated by the actor (completion only)\r\n  max_tokens: 2048\r\n  # minimum number of tokens generated by the actor\r\n  min_tokens: 100\r\n  # additional prompt tokens to be used for template or as safety\r\n  additonal_prompt_tokens: 20\r\n  # temperature for the actor\r\n  temperature: 0.5\r\n  batch_size: 1\r\n  # number iteration after print\r\n  iteration_per_print: 1\r\n  lr: 0.000009\r\n  epochs: 2\r\n  # number of backpropagation after saving the checkpoints\r\n  checkpoint_steps: 10000000000\r\n  # number of checkpoints to keep while removing the older \r\n  # (keep memory consumption of checkpoints reasonable)\r\n  n_checkpoints_to_keep: 5\r\n  # here specify the name of the actor checkpoint from which resume \r\n  # during actor training. If null load the last one.\r\n  checkpoint_name: null\r\n  # deepspeed settings\r\n  deepspeed_enable: False\r\n  deepspeed_config_path: \"./artifacts/config/ds_config.json\"\r\n  # accelerate settings\r\n  accelerate_enable: False\r\n\r\nreward_config:\r\n  # model to be chosen are gp2-large, bart-base, longformer-base-4096\r\n  # more can be simply added in the reward.py __init__()\r\n  model: \"gpt2-large\"\r\n  model_folder: \"/home/zhoux/models/reward_model\"\r\n  # hidden size of the additional ffw head to produce the scores\r\n  model_head_hidden_size: 2048\r\n  max_sequence_length: 1024\r\n  train_dataset_path: \"./datasets/reward_training_data.json\"\r\n  validation_dataset_path: null\r\n  batch_size: 1\r\n  epochs: 2\r\n  iteration_per_print: 1\r\n  # steps after which the checkpoint are saved\r\n  checkpoint_steps: 10000000000\r\n  # here specify the name of the reward checkpoint from which resume \r\n  # during reward training. If null load the last one.\r\n  checkpoint_name: null\r\n  lr: 0.000009\r\n  # deepspeed settings\r\n  deepspeed_enable: False\r\n  deepspeed_config_path: \"./artifacts/config/ds_config.json\"\r\n  # accelerate settings\r\n  accelerate_enable: False\r\n\r\ncritic_config:\r\n  # model to be chosen are gp2-large, bart-base, longformer-base-4096\r\n  # more can be simply added in the reward.py __init__()\r\n  model: \"gpt2-large\"\r\n  # hidden size of the additional ffw head to produce the scores\r\n  model_head_hidden_size: 2048\r\n  max_sequence_length: 1024\r\n  model_folder: \"/home/zhoux/models/critic_model\"\r\n  # here specify the name of the critic checkpoint from which resume \r\n  # during critic training. If null load the last one.\r\n  checkpoint_name: null\r\n\r\nwhat is wrong with it? can't figure out why...",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/289/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/289/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/288",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/288/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/288/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/288/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/288",
        "id": 1637145038,
        "node_id": "I_kwDOG1WDQc5hlNnO",
        "number": 288,
        "title": "for rlhf_accelerate branch, can't run with multiGPU",
        "user": {
            "login": "balcklive",
            "id": 57667856,
            "node_id": "MDQ6VXNlcjU3NjY3ODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/57667856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balcklive",
            "html_url": "https://github.com/balcklive",
            "followers_url": "https://api.github.com/users/balcklive/followers",
            "following_url": "https://api.github.com/users/balcklive/following{/other_user}",
            "gists_url": "https://api.github.com/users/balcklive/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balcklive/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balcklive/subscriptions",
            "organizations_url": "https://api.github.com/users/balcklive/orgs",
            "repos_url": "https://api.github.com/users/balcklive/repos",
            "events_url": "https://api.github.com/users/balcklive/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balcklive/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-23T09:21:44Z",
        "updated_at": "2023-04-03T14:46:31Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "my ~/.cache/huggingface/accelerate/default_config.yaml is:\r\ncompute_environment: LOCAL_MACHINE\r\ndeepspeed_config: {}\r\ndistributed_type: MULTI_GPU\r\ndowncast_bf16: 'no'\r\ndynamo_config: {}\r\nfsdp_config: {}\r\ngpu_ids: all\r\nmachine_rank: 0\r\nmain_training_function: main\r\nmegatron_lm_config: {}\r\nmixed_precision: 'no'\r\nnum_machines: 1\r\nnum_processes: 4\r\nrdzv_backend: static\r\nsame_network: true\r\ntpu_env: []\r\ntpu_use_cluster: false\r\ntpu_use_sudo: false\r\nuse_cpu: false\r\n\r\nmy start training command is: accelerate launch --multi_gpu /home/ubuntu/ubuntu/artifacts/main.py /home/ubuntu/ubuntu/artifacts/config/config.yaml --type REWARD   \r\n\r\nand then I got this:\r\n<img width=\"465\" alt=\"1679563172948\" src=\"https://user-images.githubusercontent.com/57667856/227157991-0b5ae998-839b-45e2-b5fd-ae3bb5a4cb4a.png\">\r\nIt seems that the program are using the same GPU, how is that happend? For the old version of main branch, I did succed run with multiGPU using this command.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/288/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/288/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/287",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/287/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/287/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/287/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/287",
        "id": 1637096708,
        "node_id": "PR_kwDOG1WDQc5MtmPL",
        "number": 287,
        "title": "[Nebullvm] Improve typing",
        "user": {
            "login": "Telemaco019",
            "id": 24567368,
            "node_id": "MDQ6VXNlcjI0NTY3MzY4",
            "avatar_url": "https://avatars.githubusercontent.com/u/24567368?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Telemaco019",
            "html_url": "https://github.com/Telemaco019",
            "followers_url": "https://api.github.com/users/Telemaco019/followers",
            "following_url": "https://api.github.com/users/Telemaco019/following{/other_user}",
            "gists_url": "https://api.github.com/users/Telemaco019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Telemaco019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Telemaco019/subscriptions",
            "organizations_url": "https://api.github.com/users/Telemaco019/orgs",
            "repos_url": "https://api.github.com/users/Telemaco019/repos",
            "events_url": "https://api.github.com/users/Telemaco019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Telemaco019/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-23T08:51:08Z",
        "updated_at": "2023-03-24T18:26:13Z",
        "closed_at": "2023-03-24T18:26:12Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/287",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/287",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/287.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/287.patch",
            "merged_at": "2023-03-24T18:26:12Z"
        },
        "body": "- Expose property \"name\" on BaseInferenceLearner\r\n- Fix postprocess_diffuser params type (DiffusionPipeline -> StableDiffusionPipeline)",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/287/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/287/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/286",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/286/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/286/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/286/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/286",
        "id": 1636997880,
        "node_id": "I_kwDOG1WDQc5hkpr4",
        "number": 286,
        "title": "How to inference?",
        "user": {
            "login": "wallon-ai",
            "id": 25277463,
            "node_id": "MDQ6VXNlcjI1Mjc3NDYz",
            "avatar_url": "https://avatars.githubusercontent.com/u/25277463?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/wallon-ai",
            "html_url": "https://github.com/wallon-ai",
            "followers_url": "https://api.github.com/users/wallon-ai/followers",
            "following_url": "https://api.github.com/users/wallon-ai/following{/other_user}",
            "gists_url": "https://api.github.com/users/wallon-ai/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/wallon-ai/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/wallon-ai/subscriptions",
            "organizations_url": "https://api.github.com/users/wallon-ai/orgs",
            "repos_url": "https://api.github.com/users/wallon-ai/repos",
            "events_url": "https://api.github.com/users/wallon-ai/events{/privacy}",
            "received_events_url": "https://api.github.com/users/wallon-ai/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-23T07:32:33Z",
        "updated_at": "2023-03-23T10:57:13Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/286/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/286/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/285",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/285/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/285/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/285/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/285",
        "id": 1636723018,
        "node_id": "I_kwDOG1WDQc5hjmlK",
        "number": 285,
        "title": "[Speedster] Speedster usage",
        "user": {
            "login": "Ludobico",
            "id": 89598307,
            "node_id": "MDQ6VXNlcjg5NTk4MzA3",
            "avatar_url": "https://avatars.githubusercontent.com/u/89598307?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/Ludobico",
            "html_url": "https://github.com/Ludobico",
            "followers_url": "https://api.github.com/users/Ludobico/followers",
            "following_url": "https://api.github.com/users/Ludobico/following{/other_user}",
            "gists_url": "https://api.github.com/users/Ludobico/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/Ludobico/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/Ludobico/subscriptions",
            "organizations_url": "https://api.github.com/users/Ludobico/orgs",
            "repos_url": "https://api.github.com/users/Ludobico/repos",
            "events_url": "https://api.github.com/users/Ludobico/events{/privacy}",
            "received_events_url": "https://api.github.com/users/Ludobico/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-23T01:52:20Z",
        "updated_at": "2023-03-27T09:39:06Z",
        "closed_at": "2023-03-27T09:39:06Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "```py\r\nfrom speedster import optimize_model\r\n\r\noptimized_model = optimize_model(model, input_data = input_data, optimization_time = \"constrained\", metric_drop_ths=0.05)\r\n```\r\n\r\nI don't know exactly what this property `metric_drop_ths` does",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/285/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/285/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/284",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/284/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/284/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/284/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/284",
        "id": 1636537458,
        "node_id": "I_kwDOG1WDQc5hi5Ry",
        "number": 284,
        "title": "[ChatLlama] Error in the start of OPT1.3B actor pre-training",
        "user": {
            "login": "swang99",
            "id": 35492791,
            "node_id": "MDQ6VXNlcjM1NDkyNzkx",
            "avatar_url": "https://avatars.githubusercontent.com/u/35492791?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/swang99",
            "html_url": "https://github.com/swang99",
            "followers_url": "https://api.github.com/users/swang99/followers",
            "following_url": "https://api.github.com/users/swang99/following{/other_user}",
            "gists_url": "https://api.github.com/users/swang99/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/swang99/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/swang99/subscriptions",
            "organizations_url": "https://api.github.com/users/swang99/orgs",
            "repos_url": "https://api.github.com/users/swang99/repos",
            "events_url": "https://api.github.com/users/swang99/events{/privacy}",
            "received_events_url": "https://api.github.com/users/swang99/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-22T21:43:46Z",
        "updated_at": "2023-03-26T00:53:12Z",
        "closed_at": "2023-03-26T00:53:12Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hello, I am trying to pre-train the actor model but around the 815-816th example, the training stops and shows this very long error message. I had already trained the reward model so I have been using separate commands instead of pipelining them. Any idea what might be causing this? Thank you.\r\n\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [80,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [188,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [188,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cuTraceback (most recent call last):\r\n:1141: indexSelectLargeIndex: block: [189,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu  File \"/content/drive/MyDrive/Colab Notebooks/llama/artifacts/main.py\", line 51, in <module>\r\n:1141: indexSelectLargeIndex: block: [189,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu    return forward_call(*input, **kwargs)\r\n:1141  File \"<@beartype(chatllama.rlhf.actor.ActorModel.forward) at 0x7f72e6137040>\", line 51, in forward\r\n: indexSelectLargeIndex: block: [146,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0  File \"/usr/local/lib/python3.9/dist-packages/chatllama/rlhf/actor.py\", line 154, in forward\r\n], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [15,0,0    model_output = self.model.forward(\r\n] Assertion `srcIndex < srcSelectDimSize  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\", line 930, in forward\r\n` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141    outputs = self.model.decoder(\r\n: indexSelectLargeIndex  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n: block: [404,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141    return forward_call(*input, **kwargs)\r\n: indexSelectLargeIndex  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\", line 696, in forward\r\n: block: [404,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [404,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize    layer_outputs = decoder_layer(\r\n` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n:1141: indexSelectLargeIndex: block: [404,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex    return forward_call(*input, **kwargs)\r\n: block: [146,0  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\", line 326, in forward\r\n,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n: block: [146,0  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146    return forward_call(*input, **kwargs)\r\n,0,0  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\", line 171, in forward\r\n], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n    query_states = self.q_proj(hidden_states) * self.scaling\r\n../aten/src/ATen/native/cuda/Indexing.cu  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\r\n:1141: indexSelectLargeIndex: block: [146,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [20,0,0    return forward_call(*input, **kwargs)\r\n] Assertion `srcIndex < srcSelectDimSize` failed.\r\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n../aten/src/ATen/native/cuda/Indexing.cu:1141: indexSelectLargeIndex: block: [146,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\r\n    return F.linear(input, self.weight, self.bias)\r\nRuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/284/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/284/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/283",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/283/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/283/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/283/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/283",
        "id": 1635721010,
        "node_id": "PR_kwDOG1WDQc5MpA1p",
        "number": 283,
        "title": "Fix issue with visualization of some jupyter notebooks on github",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-22T12:59:38Z",
        "updated_at": "2023-03-22T13:59:57Z",
        "closed_at": "2023-03-22T13:59:57Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/283",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/283",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/283.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/283.patch",
            "merged_at": "2023-03-22T13:59:57Z"
        },
        "body": "Fixes #282 ",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/283/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/283/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/282",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/282/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/282/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/282/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/282",
        "id": 1635679608,
        "node_id": "I_kwDOG1WDQc5hfn14",
        "number": 282,
        "title": "[Speedster] - Example notebooks not working ",
        "user": {
            "login": "LuigiCerone",
            "id": 24299345,
            "node_id": "MDQ6VXNlcjI0Mjk5MzQ1",
            "avatar_url": "https://avatars.githubusercontent.com/u/24299345?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/LuigiCerone",
            "html_url": "https://github.com/LuigiCerone",
            "followers_url": "https://api.github.com/users/LuigiCerone/followers",
            "following_url": "https://api.github.com/users/LuigiCerone/following{/other_user}",
            "gists_url": "https://api.github.com/users/LuigiCerone/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/LuigiCerone/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/LuigiCerone/subscriptions",
            "organizations_url": "https://api.github.com/users/LuigiCerone/orgs",
            "repos_url": "https://api.github.com/users/LuigiCerone/repos",
            "events_url": "https://api.github.com/users/LuigiCerone/events{/privacy}",
            "received_events_url": "https://api.github.com/users/LuigiCerone/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-22T12:31:02Z",
        "updated_at": "2023-03-22T13:59:58Z",
        "closed_at": "2023-03-22T13:59:58Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi! :)\r\n\r\nI've noticed that a couple of example notebooks for Speedster from [notebooks/speedster](https://github.com/nebuly-ai/nebullvm/tree/main/notebooks/speedster) are not correctly loading on Github:\r\n\r\n* https://github.com/nebuly-ai/nebullvm/blob/main/notebooks/speedster/pytorch/Accelerate_PyTorch_ResNet50_with_Speedster.ipynb\r\n* https://github.com/nebuly-ai/nebullvm/blob/main/notebooks/speedster/pytorch/Accelerate_PyTorch_YOLOv8_with_Speedster.ipynb\r\n* https://github.com/nebuly-ai/nebullvm/blob/main/notebooks/speedster/pytorch/Accelerate_fast_ai_Resnet34_with_Speedster.ipynb\r\n* https://github.com/nebuly-ai/nebullvm/blob/main/notebooks/speedster/onnx/Accelerate_ONNX_ResNet50_with_Speedster.ipynb\r\n* https://github.com/nebuly-ai/nebullvm/blob/main/notebooks/speedster/tensorflow/Accelerate_Tensorflow_ResNet50_with_Speedster.ipynb\r\n\r\nThanks for all the example materials!\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/282/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/282/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/281",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/281/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/281/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/281/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/281",
        "id": 1635269432,
        "node_id": "I_kwDOG1WDQc5heDs4",
        "number": 281,
        "title": "[Chatllama] looks like REWARD MODEL donesn't support OPT",
        "user": {
            "login": "balcklive",
            "id": 57667856,
            "node_id": "MDQ6VXNlcjU3NjY3ODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/57667856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balcklive",
            "html_url": "https://github.com/balcklive",
            "followers_url": "https://api.github.com/users/balcklive/followers",
            "following_url": "https://api.github.com/users/balcklive/following{/other_user}",
            "gists_url": "https://api.github.com/users/balcklive/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balcklive/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balcklive/subscriptions",
            "organizations_url": "https://api.github.com/users/balcklive/orgs",
            "repos_url": "https://api.github.com/users/balcklive/repos",
            "events_url": "https://api.github.com/users/balcklive/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balcklive/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-22T08:20:39Z",
        "updated_at": "2023-03-23T09:16:22Z",
        "closed_at": "2023-03-23T09:16:22Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "here's the piece of code of reward.py:\r\n`class RewardModel(torch.nn.Module):\r\n    \"\"\"Model to be trained to predict the reward for RL.\r\n    or to be used as Critic in RL.\r\n\r\n    Attributes:\r\n        model (torch.nn.Module): Model to be used for the reward model\r\n        tokenizer (torch.nn.Module): Tokenizer to be used for the reward model\r\n        head (torch.nn.Module): Head to be used for the reward model\r\n        config (ConfigReward): Config parameters for the reward model\r\n        max_model_tokens (int): Maximum sequence length for the reward model\r\n\r\n    Methods:\r\n        forward: Forward pass of the model (used by the critic)\r\n        save: Save the model\r\n        load: Load the model\r\n        get_reward: Get the reward for a given input (used by the reward model)\r\n    \"\"\"\r\n\r\n    def __init__(self, config: ConfigReward) -> None:\r\n        super().__init__()\r\n        # load the model -- add here other models\r\n        head_hidden_size = config.model_head_hidden_size\r\n        if config.model in hf_models:\r\n            self.tokenizer = AutoTokenizer.from_pretrained(\r\n                config.model,\r\n                padding_side=\"left\",\r\n                truncation_side=\"left\",\r\n            )\r\n            class RewardModel(torch.nn.Module):\r\n    \"\"\"Model to be trained to predict the reward for RL.\r\n    or to be used as Critic in RL.\r\n\r\n    Attributes:\r\n        model (torch.nn.Module): Model to be used for the reward model\r\n        tokenizer (torch.nn.Module): Tokenizer to be used for the reward model\r\n        head (torch.nn.Module): Head to be used for the reward model\r\n        config (ConfigReward): Config parameters for the reward model\r\n        max_model_tokens (int): Maximum sequence length for the reward model\r\n\r\n    Methods:\r\n        forward: Forward pass of the model (used by the critic)\r\n        save: Save the model\r\n        load: Load the model\r\n        get_reward: Get the reward for a given input (used by the reward model)\r\n    \"\"\"\r\n\r\n    def __init__(self, config: ConfigReward) -> None:\r\n        super().__init__()\r\n        # load the model -- add here other models\r\n        head_hidden_size = config.model_head_hidden_size\r\n        if config.model in hf_models:\r\n            self.tokenizer = AutoTokenizer.from_pretrained(\r\n                config.model,\r\n                padding_side=\"left\",\r\n                truncation_side=\"left\",\r\n            )\r\n            self.model = AutoModel.from_pretrained(\"gpt2-large\") ###<------watch this line!!!!!!!!!!!!!!!!!!!!!!\r\n            self.tokenizer.pad_token = self.tokenizer.eos_token\r\n            self.head = torch.nn.Sequential(\r\n                torch.nn.Linear(self.model.config.n_embd, head_hidden_size),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(head_hidden_size, 1),\r\n                Rearrange(\"... 1 -> ...\"),\r\n            )\r\n        elif config.model == \"bart-base\":\r\n            bart_config = BartConfig.from_pretrained(\"facebook/bart-base\")\r\n            bart_config.max_position_embeddings = 2048 + 1024\r\n            self.model = BartModel(bart_config)\r\n            self.tokenizer = BartTokenizer.from_pretrained(\r\n                \"facebook/bart-large\",\r\n                padding_side=\"left\",\r\n                truncation_side=\"left\",\r\n            )\r\n            self.tokenizer.pad_token = self.tokenizer.eos_token\r\n            self.head = torch.nn.Sequential(\r\n                torch.nn.Linear(768, head_hidden_size),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(head_hidden_size, 1),\r\n                Rearrange(\"... 1 -> ...\"),\r\n            )\r\n        elif config.model == \"longformer-base-4096\":\r\n            self.model = AutoModel.from_pretrained(\r\n                \"allenai/longformer-base-4096\"\r\n            )\r\n            self.tokenizer = AutoTokenizer.from_pretrained(\r\n                \"allenai/longformer-base-4096\",\r\n                padding_side=\"left\",\r\n                truncation_side=\"left\",\r\n            )\r\n            self.tokenizer.eos_token = self.tokenizer.pad_token\r\n            self.head = torch.nn.Sequential(\r\n                torch.nn.Linear(768, head_hidden_size),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(head_hidden_size, 1),\r\n                Rearrange(\"... 1 -> ...\"),\r\n            )\r\n        else:\r\n            raise ValueError(f\"model {config.model} not supported\")\r\n        # store config\r\n        self.config = config\r\n        if os.path.exists(config.model_folder) is False:\r\n            os.mkdir(config.model_folder)\r\n        else:\r\n            self.load()\r\n        # freeze model parameters (only train the head)\r\n        for param in self.model.parameters():\r\n            param.requires_grad = False\r\n        # move model to device\r\n        # self.model.to(config.device)\r\n        # self.head.to(config.device)\r\n            self.tokenizer.pad_token = self.tokenizer.eos_token\r\n            self.head = torch.nn.Sequential(\r\n                torch.nn.Linear(self.model.config.n_embd, head_hidden_size),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(head_hidden_size, 1),\r\n                Rearrange(\"... 1 -> ...\"),\r\n            )\r\n        elif config.model == \"bart-base\":\r\n            bart_config = BartConfig.from_pretrained(\"facebook/bart-base\")\r\n            bart_config.max_position_embeddings = 2048 + 1024\r\n            self.model = BartModel(bart_config)\r\n            self.tokenizer = BartTokenizer.from_pretrained(\r\n                \"facebook/bart-large\",\r\n                padding_side=\"left\",\r\n                truncation_side=\"left\",\r\n            )\r\n            self.tokenizer.pad_token = self.tokenizer.eos_token\r\n            self.head = torch.nn.Sequential(\r\n                torch.nn.Linear(768, head_hidden_size),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(head_hidden_size, 1),\r\n                Rearrange(\"... 1 -> ...\"),\r\n            )\r\n        elif config.model == \"longformer-base-4096\":\r\n            self.model = AutoModel.from_pretrained(\r\n                \"allenai/longformer-base-4096\"\r\n            )\r\n            self.tokenizer = AutoTokenizer.from_pretrained(\r\n                \"allenai/longformer-base-4096\",\r\n                padding_side=\"left\",\r\n                truncation_side=\"left\",\r\n            )\r\n            self.tokenizer.eos_token = self.tokenizer.pad_token\r\n            self.head = torch.nn.Sequential(\r\n                torch.nn.Linear(768, head_hidden_size),\r\n                torch.nn.ReLU(),\r\n                torch.nn.Linear(head_hidden_size, 1),\r\n                Rearrange(\"... 1 -> ...\"),\r\n            )\r\n        else:\r\n            raise ValueError(f\"model {config.model} not supported\")\r\n        # store config\r\n        self.config = config\r\n        if os.path.exists(config.model_folder) is False:\r\n            os.mkdir(config.model_folder)\r\n        else:\r\n            self.load()\r\n        # freeze model parameters (only train the head)\r\n        for param in self.model.parameters():\r\n            param.requires_grad = False\r\n        # move model to device\r\n        self.model.to(config.device)\r\n        self.head.to(config.device)`\r\n\r\nthe marked line says, even if you specify a reward model type like opt-1.3b, the code still gives you a gpt2 model. Is this a bug or on purpose?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/281/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/281/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/280",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/280/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/280/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/280/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/280",
        "id": 1633976327,
        "node_id": "PR_kwDOG1WDQc5MjKo1",
        "number": 280,
        "title": "Fix tensorrt installation issue",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-21T13:52:57Z",
        "updated_at": "2023-03-21T15:46:03Z",
        "closed_at": "2023-03-21T15:46:03Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/280",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/280",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/280.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/280.patch",
            "merged_at": "2023-03-21T15:46:03Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/280/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/280/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/279",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/279/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/279/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/279/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/279",
        "id": 1633643983,
        "node_id": "PR_kwDOG1WDQc5MiBrF",
        "number": 279,
        "title": "Add requirements in the docs for stable diffusion",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-21T10:47:42Z",
        "updated_at": "2023-03-21T13:30:02Z",
        "closed_at": "2023-03-21T12:43:51Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/279",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/279",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/279.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/279.patch",
            "merged_at": "2023-03-21T12:43:51Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/279/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/279/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/278",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/278/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/278/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/278/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/278",
        "id": 1632669755,
        "node_id": "I_kwDOG1WDQc5hUJA7",
        "number": 278,
        "title": "cannot import name 'load_dataset' from 'datasets'",
        "user": {
            "login": "jaybutera",
            "id": 2101658,
            "node_id": "MDQ6VXNlcjIxMDE2NTg=",
            "avatar_url": "https://avatars.githubusercontent.com/u/2101658?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/jaybutera",
            "html_url": "https://github.com/jaybutera",
            "followers_url": "https://api.github.com/users/jaybutera/followers",
            "following_url": "https://api.github.com/users/jaybutera/following{/other_user}",
            "gists_url": "https://api.github.com/users/jaybutera/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/jaybutera/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/jaybutera/subscriptions",
            "organizations_url": "https://api.github.com/users/jaybutera/orgs",
            "repos_url": "https://api.github.com/users/jaybutera/repos",
            "events_url": "https://api.github.com/users/jaybutera/events{/privacy}",
            "received_events_url": "https://api.github.com/users/jaybutera/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-20T18:40:52Z",
        "updated_at": "2023-03-21T16:43:51Z",
        "closed_at": "2023-03-20T19:18:37Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "load_dataset is imported in the file the readme recommends running\r\n```python\r\npython artifacts/download_dataset.py SHP --path ./datasets --number_of_samples 200\r\n```\r\n\r\nhttps://github.com/nebuly-ai/nebullvm/blob/52a4e1b26dbca5f6ce7f4afd0d6142a38465e7f6/apps/accelerate/chatllama/artifacts/download_dataset.py#L8\r\nBut no function is defined in the datasets module",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/278/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/278/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/277",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/277/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/277/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/277/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/277",
        "id": 1632592290,
        "node_id": "PR_kwDOG1WDQc5MefCl",
        "number": 277,
        "title": "Update Stable diffusion to TensorRT 8.6.0",
        "user": {
            "login": "valeriosofi",
            "id": 28647171,
            "node_id": "MDQ6VXNlcjI4NjQ3MTcx",
            "avatar_url": "https://avatars.githubusercontent.com/u/28647171?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/valeriosofi",
            "html_url": "https://github.com/valeriosofi",
            "followers_url": "https://api.github.com/users/valeriosofi/followers",
            "following_url": "https://api.github.com/users/valeriosofi/following{/other_user}",
            "gists_url": "https://api.github.com/users/valeriosofi/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/valeriosofi/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/valeriosofi/subscriptions",
            "organizations_url": "https://api.github.com/users/valeriosofi/orgs",
            "repos_url": "https://api.github.com/users/valeriosofi/repos",
            "events_url": "https://api.github.com/users/valeriosofi/events{/privacy}",
            "received_events_url": "https://api.github.com/users/valeriosofi/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-20T17:49:42Z",
        "updated_at": "2023-03-21T09:01:38Z",
        "closed_at": "2023-03-21T09:01:38Z",
        "author_association": "COLLABORATOR",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/277",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/277",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/277.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/277.patch",
            "merged_at": "2023-03-21T09:01:38Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/277/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/277/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/276",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/276/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/276/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/276/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/276",
        "id": 1631651605,
        "node_id": "I_kwDOG1WDQc5hQQcV",
        "number": 276,
        "title": "[Chatllama] Community Integration: Making nebullvm/ChatLLaMA cheaper, faster, and more efficient",
        "user": {
            "login": "binmakeswell",
            "id": 61670638,
            "node_id": "MDQ6VXNlcjYxNjcwNjM4",
            "avatar_url": "https://avatars.githubusercontent.com/u/61670638?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/binmakeswell",
            "html_url": "https://github.com/binmakeswell",
            "followers_url": "https://api.github.com/users/binmakeswell/followers",
            "following_url": "https://api.github.com/users/binmakeswell/following{/other_user}",
            "gists_url": "https://api.github.com/users/binmakeswell/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/binmakeswell/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/binmakeswell/subscriptions",
            "organizations_url": "https://api.github.com/users/binmakeswell/orgs",
            "repos_url": "https://api.github.com/users/binmakeswell/repos",
            "events_url": "https://api.github.com/users/binmakeswell/events{/privacy}",
            "received_events_url": "https://api.github.com/users/binmakeswell/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-20T08:49:30Z",
        "updated_at": "2023-03-20T10:41:42Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Thank you for your outstanding contribution to nebullvm and ChatLLaMA!\r\n\r\nAIGC, e.g., ChatGPT, has recently risen to be one of the hottest topics in AI. We are happy to share a fantastic solution where the costs of training/inference nebullvm/ChatLLaMA can be cheaper a lot!\r\n\r\n[Colossal-AI](https://github.com/hpcaitech/ColossalAI) provides an optimized open source low-cost solution that replicates ChatGPT training process. Compared to the PyTorch, a single-machine training process can be 7.73 times faster, and a mini demo training process requires only 1.62GB of GPU memory. More details can be found on the [blog](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt).\r\n\r\nOpen-source code\uff1ahttps://github.com/hpcaitech/ColossalAI#chatgpt\r\n\r\nWe would appreciate it if we could build the integration with you to benefit the community, and we are willing to provide help you need in this cooperation for free, making efforts towards the era of large AI models from the starting point of replicating ChatGPT!\r\n\r\nThank you very much.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/276/reactions",
            "total_count": 1,
            "+1": 1,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/276/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/275",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/275/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/275/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/275/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/275",
        "id": 1631609023,
        "node_id": "I_kwDOG1WDQc5hQGC_",
        "number": 275,
        "title": "[Chatllama] train chatllama REWARD model using deepspeed ,got:RuntimeError: Found dtype Float but expected Half",
        "user": {
            "login": "balcklive",
            "id": 57667856,
            "node_id": "MDQ6VXNlcjU3NjY3ODU2",
            "avatar_url": "https://avatars.githubusercontent.com/u/57667856?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/balcklive",
            "html_url": "https://github.com/balcklive",
            "followers_url": "https://api.github.com/users/balcklive/followers",
            "following_url": "https://api.github.com/users/balcklive/following{/other_user}",
            "gists_url": "https://api.github.com/users/balcklive/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/balcklive/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/balcklive/subscriptions",
            "organizations_url": "https://api.github.com/users/balcklive/orgs",
            "repos_url": "https://api.github.com/users/balcklive/repos",
            "events_url": "https://api.github.com/users/balcklive/events{/privacy}",
            "received_events_url": "https://api.github.com/users/balcklive/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-03-20T08:16:36Z",
        "updated_at": "2023-04-03T14:43:22Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": " {\r\n    \"train_batch_size\": 1,\r\n    \"gradient_accumulation_steps\": 1,\r\n    \"optimizer\": {\r\n        \"type\": \"Adam\",\r\n        \"params\": {\r\n            \"lr\": 0.00015\r\n        }\r\n    },\r\n    \"fp16\": {\r\n        \"enabled\": true,\r\n        \"auto_cast\": false,\r\n        \"loss_scale\": 0,\r\n        \"initial_scale_power\": 16,\r\n        \"loss_scale_window\": 1000,\r\n        \"hysteresis\": 2,\r\n        \"min_loss_scale\": 1\r\n    },\r\n    \"zero_optimization\": {\r\n        \"stage\": 2,\r\n        \"offload_optimizer\": {\r\n            \"device\": \"cpu\"\r\n        },\r\n        \"contiguous_gradients\": true,\r\n        \"overlap_comm\": true\r\n    },\r\n    \"num_gpus\": 1\r\n}\r\nUsing /home/ubuntu/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\r\nNo modifications detected for re-loaded extension module utils, skipping build step...\r\nLoading extension module utils...\r\nTime to load utils op: 0.00028967857360839844 seconds\r\nStart Training the Reward Model\r\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\r\nTraceback (most recent call last):\r\n  File \"artifacts/main.py\", line 54, in <module>\r\n    reward_trainer.train()\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/chatllama/rlhf/reward.py\", line 379, in train\r\n    self.model_engine.backward(loss)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/deepspeed/utils/nvtx.py\", line 11, in wrapped_fn\r\n    ret_val = func(*args, **kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/deepspeed/runtime/engine.py\", line 1964, in backward\r\n    self.optimizer.backward(loss, retain_graph=retain_graph)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2028, in backward\r\n    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 54, in backward\r\n    scaled_loss.backward(retain_graph=retain_graph)\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/_tensor.py\", line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 200, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\nRuntimeError: Found dtype Float but expected Half\r\n\r\nIs it a GPU memory not enough problem? any help would be appreciated.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/275/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/275/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/274",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/274/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/274/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/274/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/274",
        "id": 1630875722,
        "node_id": "I_kwDOG1WDQc5hNTBK",
        "number": 274,
        "title": "[Chatllama] actor.py: the load function  is correct?",
        "user": {
            "login": "bucm-tcm-tool",
            "id": 61950366,
            "node_id": "MDQ6VXNlcjYxOTUwMzY2",
            "avatar_url": "https://avatars.githubusercontent.com/u/61950366?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bucm-tcm-tool",
            "html_url": "https://github.com/bucm-tcm-tool",
            "followers_url": "https://api.github.com/users/bucm-tcm-tool/followers",
            "following_url": "https://api.github.com/users/bucm-tcm-tool/following{/other_user}",
            "gists_url": "https://api.github.com/users/bucm-tcm-tool/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/bucm-tcm-tool/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/bucm-tcm-tool/subscriptions",
            "organizations_url": "https://api.github.com/users/bucm-tcm-tool/orgs",
            "repos_url": "https://api.github.com/users/bucm-tcm-tool/repos",
            "events_url": "https://api.github.com/users/bucm-tcm-tool/events{/privacy}",
            "received_events_url": "https://api.github.com/users/bucm-tcm-tool/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-19T10:28:58Z",
        "updated_at": "2023-03-20T09:09:21Z",
        "closed_at": "2023-03-20T09:09:20Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "**Here are some possible errors:**\r\n\r\n- **The code in the rlhf/actor.py, line 117:** \r\n\r\n`      self.head.load_state_dict(model_dict[\"head\"])`\r\nWe have never defined \"self.head\", so should we delete this line ?\r\n\r\n- **The code in the rlhf/actor.py, line 133-136:** \r\n\r\n`      torch.save(\r\n           {\"model\": self.model.state_dict(), \"head\": self.head.state_dict()},\r\n            path,\r\n        )`\r\n\r\nshould we delete \"head\": self.head.state_dict() ?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/274/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/274/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/273",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/273/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/273/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/273/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/273",
        "id": 1629031872,
        "node_id": "I_kwDOG1WDQc5hGQ3A",
        "number": 273,
        "title": "[Speedster] `optimize_model` function transfers the original model from the GPU to the CPU",
        "user": {
            "login": "mfumanelli",
            "id": 53374883,
            "node_id": "MDQ6VXNlcjUzMzc0ODgz",
            "avatar_url": "https://avatars.githubusercontent.com/u/53374883?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mfumanelli",
            "html_url": "https://github.com/mfumanelli",
            "followers_url": "https://api.github.com/users/mfumanelli/followers",
            "following_url": "https://api.github.com/users/mfumanelli/following{/other_user}",
            "gists_url": "https://api.github.com/users/mfumanelli/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mfumanelli/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mfumanelli/subscriptions",
            "organizations_url": "https://api.github.com/users/mfumanelli/orgs",
            "repos_url": "https://api.github.com/users/mfumanelli/repos",
            "events_url": "https://api.github.com/users/mfumanelli/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mfumanelli/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 3825828844,
                "node_id": "LA_kwDOG1WDQc7kCYPs",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/good%20first%20issue",
                "name": "good first issue",
                "color": "7057ff",
                "default": true,
                "description": "Good for newcomers"
            },
            {
                "id": 4981513383,
                "node_id": "LA_kwDOG1WDQc8AAAABKOvcpw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/speedster",
                "name": "speedster",
                "color": "bfdadc",
                "default": false,
                "description": "Issue related to the Speedster App"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 6,
        "created_at": "2023-03-17T10:02:47Z",
        "updated_at": "2023-10-08T18:36:46Z",
        "closed_at": null,
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "body": "### Problem\r\nThe `optimize_model` function transfers the original model from the GPU to the CPU:\r\n\r\n![image](https://user-images.githubusercontent.com/53374883/225872593-d49b1c3a-4b53-434b-a437-cc9e19390e5f.png)\r\n\r\n### Desired Behavior\r\nMaintaining the model on the initial device.\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/273/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/273/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/272",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/272/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/272/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/272/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/272",
        "id": 1628795207,
        "node_id": "I_kwDOG1WDQc5hFXFH",
        "number": 272,
        "title": "[Speedster] speedster usage",
        "user": {
            "login": "vinnitu",
            "id": 432168,
            "node_id": "MDQ6VXNlcjQzMjE2OA==",
            "avatar_url": "https://avatars.githubusercontent.com/u/432168?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/vinnitu",
            "html_url": "https://github.com/vinnitu",
            "followers_url": "https://api.github.com/users/vinnitu/followers",
            "following_url": "https://api.github.com/users/vinnitu/following{/other_user}",
            "gists_url": "https://api.github.com/users/vinnitu/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/vinnitu/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/vinnitu/subscriptions",
            "organizations_url": "https://api.github.com/users/vinnitu/orgs",
            "repos_url": "https://api.github.com/users/vinnitu/repos",
            "events_url": "https://api.github.com/users/vinnitu/events{/privacy}",
            "received_events_url": "https://api.github.com/users/vinnitu/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [
            {
                "id": 4981513383,
                "node_id": "LA_kwDOG1WDQc8AAAABKOvcpw",
                "url": "https://api.github.com/repos/nebuly-ai/nebuly/labels/speedster",
                "name": "speedster",
                "color": "bfdadc",
                "default": false,
                "description": "Issue related to the Speedster App"
            }
        ],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-03-17T06:50:18Z",
        "updated_at": "2023-03-28T14:07:12Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi!\r\n\r\nIs it properly formed code usage optimized model?\r\n\r\n```python\r\nfrom transformers import AlbertTokenizer\r\nfrom speedster import load_model\r\n\r\ntokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v1\")\r\n\r\ntext = \"This is an example text for the huggingface model.\"\r\ninput_sample = tokenizer(text, return_tensors=\"pt\")\r\n\r\noptimized_model = load_model(\"model_save_path\")\r\noutput = optimized_model(**input_sample)\r\n\r\nresult = tokenizer.batch_decode(output)\r\n\r\nprint(result)\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/272/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/272/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/271",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/271/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/271/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/271/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/271",
        "id": 1628643953,
        "node_id": "PR_kwDOG1WDQc5MRYlX",
        "number": 271,
        "title": "fix model parallel support for llama",
        "user": {
            "login": "zhzou2020",
            "id": 7436697,
            "node_id": "MDQ6VXNlcjc0MzY2OTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7436697?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zhzou2020",
            "html_url": "https://github.com/zhzou2020",
            "followers_url": "https://api.github.com/users/zhzou2020/followers",
            "following_url": "https://api.github.com/users/zhzou2020/following{/other_user}",
            "gists_url": "https://api.github.com/users/zhzou2020/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zhzou2020/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zhzou2020/subscriptions",
            "organizations_url": "https://api.github.com/users/zhzou2020/orgs",
            "repos_url": "https://api.github.com/users/zhzou2020/repos",
            "events_url": "https://api.github.com/users/zhzou2020/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zhzou2020/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-17T03:52:18Z",
        "updated_at": "2023-03-17T08:36:27Z",
        "closed_at": "2023-03-17T08:36:27Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/271",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/271",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/271.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/271.patch",
            "merged_at": "2023-03-17T08:36:27Z"
        },
        "body": "[#263](https://github.com/nebuly-ai/nebullvm/issues/263) fix the issue of scores and kv_mask mismatch when using LLaMa 13B model.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/271/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/271/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/270",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/270/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/270/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/270/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/270",
        "id": 1627404311,
        "node_id": "I_kwDOG1WDQc5hADgX",
        "number": 270,
        "title": "[Chatllama] 3B to 13B \u2192 4x Nvidia A100 (80Gb)?",
        "user": {
            "login": "CS123n",
            "id": 102355527,
            "node_id": "U_kgDOBhnSRw",
            "avatar_url": "https://avatars.githubusercontent.com/u/102355527?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/CS123n",
            "html_url": "https://github.com/CS123n",
            "followers_url": "https://api.github.com/users/CS123n/followers",
            "following_url": "https://api.github.com/users/CS123n/following{/other_user}",
            "gists_url": "https://api.github.com/users/CS123n/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/CS123n/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/CS123n/subscriptions",
            "organizations_url": "https://api.github.com/users/CS123n/orgs",
            "repos_url": "https://api.github.com/users/CS123n/repos",
            "events_url": "https://api.github.com/users/CS123n/events{/privacy}",
            "received_events_url": "https://api.github.com/users/CS123n/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-16T12:45:37Z",
        "updated_at": "2023-03-20T08:34:51Z",
        "closed_at": "2023-03-20T08:34:51Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "Hi, if the batch size is 1, do I still need 4 A100 to start the training process? What is the minimal memory requirements?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/270/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/270/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/269",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/269/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/269/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/269/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/pull/269",
        "id": 1627261722,
        "node_id": "PR_kwDOG1WDQc5MMsTo",
        "number": 269,
        "title": "[Speedster] Fix typos and update benchmarks",
        "user": {
            "login": "mfumanelli",
            "id": 53374883,
            "node_id": "MDQ6VXNlcjUzMzc0ODgz",
            "avatar_url": "https://avatars.githubusercontent.com/u/53374883?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/mfumanelli",
            "html_url": "https://github.com/mfumanelli",
            "followers_url": "https://api.github.com/users/mfumanelli/followers",
            "following_url": "https://api.github.com/users/mfumanelli/following{/other_user}",
            "gists_url": "https://api.github.com/users/mfumanelli/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/mfumanelli/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/mfumanelli/subscriptions",
            "organizations_url": "https://api.github.com/users/mfumanelli/orgs",
            "repos_url": "https://api.github.com/users/mfumanelli/repos",
            "events_url": "https://api.github.com/users/mfumanelli/events{/privacy}",
            "received_events_url": "https://api.github.com/users/mfumanelli/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-16T11:20:00Z",
        "updated_at": "2023-03-21T09:02:48Z",
        "closed_at": "2023-03-21T09:02:48Z",
        "author_association": "MEMBER",
        "active_lock_reason": null,
        "draft": false,
        "pull_request": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/pulls/269",
            "html_url": "https://github.com/nebuly-ai/nebuly/pull/269",
            "diff_url": "https://github.com/nebuly-ai/nebuly/pull/269.diff",
            "patch_url": "https://github.com/nebuly-ai/nebuly/pull/269.patch",
            "merged_at": "2023-03-21T09:02:47Z"
        },
        "body": null,
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/269/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/269/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/268",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/268/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/268/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/268/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/268",
        "id": 1626979809,
        "node_id": "I_kwDOG1WDQc5g-b3h",
        "number": 268,
        "title": "[Chatllama] Is the training correct?",
        "user": {
            "login": "menandro",
            "id": 14872148,
            "node_id": "MDQ6VXNlcjE0ODcyMTQ4",
            "avatar_url": "https://avatars.githubusercontent.com/u/14872148?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/menandro",
            "html_url": "https://github.com/menandro",
            "followers_url": "https://api.github.com/users/menandro/followers",
            "following_url": "https://api.github.com/users/menandro/following{/other_user}",
            "gists_url": "https://api.github.com/users/menandro/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/menandro/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/menandro/subscriptions",
            "organizations_url": "https://api.github.com/users/menandro/orgs",
            "repos_url": "https://api.github.com/users/menandro/repos",
            "events_url": "https://api.github.com/users/menandro/events{/privacy}",
            "received_events_url": "https://api.github.com/users/menandro/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-03-16T08:47:17Z",
        "updated_at": "2023-04-03T13:49:02Z",
        "closed_at": "2023-04-03T13:49:02Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "llama_model.generate returns (llama_model.py, line 545):\r\n`generated_tokens.append(next_token)`\r\nand saved to `sequences` called by (actor.py, line 210).\r\n\r\nAbove generates `max_length` tokens which is set in (actor.py, line 208):\r\n`max_length = states.shape[1] + max_completion`\r\nAbove code takes the length of the input tokens (states) and max_completion. But, line 216 we have:\r\n`actions = sequences[:, states.shape[1] :]  # noqa E203`\r\nwhich means `actions` is taken as the last `max_collection` tokens of sequences. \r\n\r\nShouldn't actions be the entire `generated_tokens` and `sequences` should be `states + \" \" + generated_tokens`?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/268/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/268/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/267",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/267/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/267/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/267/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/267",
        "id": 1626821961,
        "node_id": "I_kwDOG1WDQc5g91VJ",
        "number": 267,
        "title": "[Chatllama] ImportError: cannot import name 'ConfigActor' from 'config'",
        "user": {
            "login": "judyhappy",
            "id": 3390962,
            "node_id": "MDQ6VXNlcjMzOTA5NjI=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3390962?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/judyhappy",
            "html_url": "https://github.com/judyhappy",
            "followers_url": "https://api.github.com/users/judyhappy/followers",
            "following_url": "https://api.github.com/users/judyhappy/following{/other_user}",
            "gists_url": "https://api.github.com/users/judyhappy/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/judyhappy/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/judyhappy/subscriptions",
            "organizations_url": "https://api.github.com/users/judyhappy/orgs",
            "repos_url": "https://api.github.com/users/judyhappy/repos",
            "events_url": "https://api.github.com/users/judyhappy/events{/privacy}",
            "received_events_url": "https://api.github.com/users/judyhappy/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-03-16T06:43:51Z",
        "updated_at": "2023-03-16T09:30:04Z",
        "closed_at": "2023-03-16T09:30:04Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": " python3 ./artifacts/main.py ./artifacts/config/config.yaml --type REWARD\r\n\r\nTraceback (most recent call last):\r\n  File \"./artifacts/main.py\", line 3, in <module>\r\n    from chatllama.rlhf.actor import ActorTrainer\r\n  File \"/usr2/judyhappy/.local/lib/python3.8/site-packages/chatllama/rlhf/actor.py\", line 9, in <module>\r\n    from config import ConfigActor\r\nImportError: cannot import name 'ConfigActor' from 'config'  (/usr2/judyhappy/.local/lib/python3.8/site-packages/config/__init__.py)\r\n\r\npip list:\r\nconfig                     0.5.1\r\nchatllama                0.0.4\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/267/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/267/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/266",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/266/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/266/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/266/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/266",
        "id": 1625195431,
        "node_id": "I_kwDOG1WDQc5g3oOn",
        "number": 266,
        "title": "[chatllama]training batch_size=2 would crash",
        "user": {
            "login": "BeyonderXX",
            "id": 42961051,
            "node_id": "MDQ6VXNlcjQyOTYxMDUx",
            "avatar_url": "https://avatars.githubusercontent.com/u/42961051?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/BeyonderXX",
            "html_url": "https://github.com/BeyonderXX",
            "followers_url": "https://api.github.com/users/BeyonderXX/followers",
            "following_url": "https://api.github.com/users/BeyonderXX/following{/other_user}",
            "gists_url": "https://api.github.com/users/BeyonderXX/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/BeyonderXX/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/BeyonderXX/subscriptions",
            "organizations_url": "https://api.github.com/users/BeyonderXX/orgs",
            "repos_url": "https://api.github.com/users/BeyonderXX/repos",
            "events_url": "https://api.github.com/users/BeyonderXX/events{/privacy}",
            "received_events_url": "https://api.github.com/users/BeyonderXX/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-03-15T10:08:58Z",
        "updated_at": "2023-04-03T14:42:23Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "[chatllama] Inference is OK. But I try to train ACTOR model with default deepspeed architecture in LLaMA 7B model.\r\nHowever, when my batch size is 1, the code is OK. It would crash with more than 2 batch size. \r\nIts Error report:\r\n`/root/InstructUIE/run_llama/nebullvm/apps/accelerate/chatllama/chatllama/llama_model.py:29 \u2502\r\n\u2502 3 in forward                                                                               \u2502\r\n\u2502                                                                                            \u2502\r\n\u2502   290 \u2502   \u2502                                                                                \u2502\r\n\u2502   291 \u2502   \u2502   bsz, seqlen, _ = x.shape                                                     \u2502\r\n\u2502   292 \u2502   \u2502   print(x.shape)                                                               \u2502\r\n\u2502 \u2771 293 \u2502   \u2502   xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)                              \u2502\r\n\u2502   294 \u2502   \u2502                                                                                \u2502\r\n\u2502   295 \u2502   \u2502   xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)                 \u2502\r\n\u2502   296 \u2502   \u2502   xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)                 \u2502\r\n\u2502                                                                                            \u2502\r\n\u2502 /opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:1194 in _call_impl       \u2502\r\n\u2502                                                                                            \u2502\r\n\u2502   1191 \u2502   \u2502   # this function, and just call forward.                                     \u2502\r\n\u2502   1192 \u2502   \u2502   if not (self._backward_hooks or self._forward_hooks or self._forward_pre_ho \u2502\r\n\u2502   1193 \u2502   \u2502   \u2502   \u2502   or _global_forward_hooks or _global_forward_pre_hooks):             \u2502\r\n\u2502 \u2771 1194 \u2502   \u2502   \u2502   return forward_call(*input, **kwargs)                                   \u2502\r\n\u2502   1195 \u2502   \u2502   # Do not call functions when jit is used                                    \u2502\r\n\u2502   1196 \u2502   \u2502   full_backward_hooks, non_full_backward_hooks = [], []                       \u2502\r\n\u2502   1197 \u2502   \u2502   if self._backward_hooks or _global_backward_hooks:                          \u2502\r\n\u2502                                                                                            \u2502\r\n\u2502 /opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py:114 in forward           \u2502\r\n\u2502                                                                                            \u2502\r\n\u2502   111 \u2502   \u2502   \u2502   init.uniform_(self.bias, -bound, bound)                                  \u2502\r\n\u2502   112 \u2502                                                                                    \u2502\r\n\u2502   113 \u2502   def forward(self, input: Tensor) -> Tensor:                                      \u2502\r\n\u2502 \u2771 114 \u2502   \u2502   return F.linear(input, self.weight, self.bias)                               \u2502\r\n\u2502   115 \u2502                                                                                    \u2502\r\n\u2502   116 \u2502   def extra_repr(self) -> str:                                                     \u2502\r\n\u2502   117 \u2502   \u2502   return 'in_features={}, out_features={}, bias={}'.format(                    \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nRuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx( handle,\r\nopa, opb, m, n, k, &falpha, a, CUDA_R_16BF, lda, b, CUDA_R_16BF, ldb, &fbeta, c, CUDA_R_16BF,\r\nldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.`\r\nI checked the tensor shape and linear layer, its OK, but why? \r\nHere is my deepspeed config file\r\n`{\r\n    \"train_micro_batch_size_per_gpu\": 2,\r\n    \"gradient_accumulation_steps\": 64,\r\n    \"bfloat16\": {\r\n        \"enabled\": true\r\n    },\r\n    \"optimizer\": {\r\n        \"type\": \"AdamW\",\r\n        \"params\": {\r\n            \"lr\": 0.0001,\r\n            \"betas\": [0.9, 0.999],\r\n            \"eps\": 1e-8,\r\n            \"weight_decay\": 0.1\r\n        }\r\n    },\r\n    \"zero_optimization\": {\r\n        \"stage\": 2,\r\n        \"offload_optimizer\": {\r\n            \"device\": \"cpu\",\r\n            \"pin_memory\": true\r\n        },\r\n        \"offload_param\": {\r\n            \"device\": \"cpu\",\r\n            \"pin_memory\": true\r\n        },\r\n        \"allgather_partitions\": true,\r\n        \"allgather_bucket_size\": 2e8,\r\n        \"overlap_comm\": true,\r\n        \"reduce_scatter\": true,\r\n        \"reduce_bucket_size\": 2e8,\r\n        \"contiguous_gradients\": true\r\n    }\r\n}\r\n`\r\n\r\nMy GPU are A100PCIe *8, here is my environment:\r\n`\r\n(base) root@61e731354b65:~# nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2022 NVIDIA Corporation\r\nBuilt on Tue_May__3_18:49:52_PDT_2022\r\nCuda compilation tools, release 11.7, V11.7.64\r\nBuild cuda_11.7.r11.7/compiler.31294372_0\r\n\r\n\r\nPyTorch version: 1.13.1+cu117\r\nIs debug build: False\r\nCUDA used to build PyTorch: 11.7\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.7.13 (default, Mar 29 2022, 02:18:16)  [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-debian-buster-sid\r\nIs CUDA available: True\r\nCUDA runtime version: 11.7.64\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\nGPU 2: NVIDIA A100 80GB PCIe\r\nGPU 3: NVIDIA A100 80GB PCIe\r\nGPU 4: NVIDIA A100 80GB PCIe\r\nGPU 5: NVIDIA A100 80GB PCIe\r\nGPU 6: NVIDIA A100 80GB PCIe\r\nGPU 7: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 510.54\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              72\r\nOn-line CPU(s) list: 0-71\r\nThread(s) per core:  2\r\nCore(s) per socket:  18\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz\r\nStepping:            4\r\nCPU MHz:             1199.890\r\nCPU max MHz:         3700.0000\r\nCPU min MHz:         1200.0000\r\nBogoMIPS:            6000.00\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            25344K\r\nNUMA node0 CPU(s):   0-17,36-53\r\nNUMA node1 CPU(s):   18-35,54-71\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke md_clear spec_ctrl intel_stibp flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.21.5\r\n[pip3] torch==1.13.1+cu117\r\n[pip3] torchaudio==0.13.1+cu117\r\n[pip3] torchelastic==0.2.0\r\n[pip3] torchtext==0.13.0\r\n[pip3] torchvision==0.14.1+cu117\r\n[conda] blas                      1.0                         mkl\r\n[conda] cudatoolkit               11.3.1               ha36c431_9    nvidia\r\n[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch\r\n[conda] mkl                       2021.4.0           h06a4308_640\r\n[conda] mkl-service               2.4.0            py37h7f8727e_0\r\n[conda] mkl_fft                   1.3.1            py37hd3c417c_0\r\n[conda] mkl_random                1.2.2            py37h51133e4_0\r\n[conda] numpy                     1.21.5           py37he7a7128_2\r\n[conda] numpy-base                1.21.5           py37hf524024_2\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torch                     1.13.1+cu117             pypi_0    pypi\r\n[conda] torchaudio                0.13.1+cu117             pypi_0    pypi\r\n[conda] torchelastic              0.2.0                    pypi_0    pypi\r\n[conda] torchtext                 0.13.0                     py37    pytorch\r\n[conda] torchvision               0.14.1+cu117             pypi_0    pypi`\r\nWhat happened?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/266/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/266/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/265",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/265/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/265/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/265/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/265",
        "id": 1625092384,
        "node_id": "I_kwDOG1WDQc5g3PEg",
        "number": 265,
        "title": "[Chatllama] Generate Data Error on tutorial ",
        "user": {
            "login": "yicheng-2019",
            "id": 58028882,
            "node_id": "MDQ6VXNlcjU4MDI4ODgy",
            "avatar_url": "https://avatars.githubusercontent.com/u/58028882?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/yicheng-2019",
            "html_url": "https://github.com/yicheng-2019",
            "followers_url": "https://api.github.com/users/yicheng-2019/followers",
            "following_url": "https://api.github.com/users/yicheng-2019/following{/other_user}",
            "gists_url": "https://api.github.com/users/yicheng-2019/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/yicheng-2019/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/yicheng-2019/subscriptions",
            "organizations_url": "https://api.github.com/users/yicheng-2019/orgs",
            "repos_url": "https://api.github.com/users/yicheng-2019/repos",
            "events_url": "https://api.github.com/users/yicheng-2019/events{/privacy}",
            "received_events_url": "https://api.github.com/users/yicheng-2019/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-15T09:21:22Z",
        "updated_at": "2023-03-31T07:21:54Z",
        "closed_at": "2023-03-31T07:21:54Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I run the GetStart but the dataset has the error\r\n\r\n(llama) qiuyc@qiuyc-3090:~/code/Chatllama/chatllama-nebullvm$ python artifacts/main.py artifacts/config/config.yaml --type ALL\r\nCurrent device used :cuda\r\nNo previous model found at ./models/reward for model gpt2-large.pt\r\nLoading dataset from ./datasets/reward_training_data.json\r\nLoaded 200 samples\r\nStart Training the Reward Model\r\nLooking for checkpoints...\r\nFound checkpoint for epoch 1, step 10...\r\nLoading ...\r\nEpoch: 1/32, Iteration: 11/25, Training Loss: 3.4106693267822266\r\nprediction [4.451936  3.87707   5.655977  4.2465377 2.3933697 5.1995487 4.347823\r\n 5.8636036] target [4.  3.  2.  3.  4.  4.5 4.  3. ]\r\nEpoch: 1/32, Iteration: 12/25, Training Loss: 2.217592477798462\r\nprediction [2.7735171 5.110946  5.260797  5.696887  4.2918286 2.9091988 5.3532596\r\n 4.6064796] target [3. 3. 4. 4. 3. 4. 3. 4.]\r\nEpoch: 1/32, Iteration: 13/25, Training Loss: 3.16831636428833\r\nprediction [2.6523368 5.0899324 5.2925634 4.878763  4.6527834 5.071644  4.953468\r\n 4.7791476] target [4. 2. 3. 4. 3. 4. 4. 3.]\r\nEpoch: 1/32, Iteration: 14/25, Training Loss: 1.3716436624526978\r\nprediction [4.5529017 4.795724  5.041059  4.568875  2.6253047 5.424052  4.8917665\r\n 3.5276096] target [4.  4.5 4.  3.  3.  3.  4.  4. ]\r\nEpoch: 1/32, Iteration: 15/25, Training Loss: 1.31028413772583\r\nprediction [5.3360367 4.746885  3.1630383 4.666844  4.6595182 2.9819224 4.9737635\r\n 4.466095 ] target [4. 4. 2. 3. 3. 4. 5. 4.]\r\nTraceback (most recent call last):\r\n  File \"artifacts/main.py\", line 57, in <module>\r\n    reward_trainer.train()\r\n  File \"/home/qiuyc/code/Chatllama/chatllama-nebullvm/chatllama/rlhf/reward.py\", line 404, in train\r\n    for i, inputs in enumerate(self.train_dataloader):\r\n  File \"/home/qiuyc/anaconda3/envs/llama/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 652, in __next__\r\n    data = self._next_data()\r\n  File \"/home/qiuyc/anaconda3/envs/llama/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 692, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"/home/qiuyc/anaconda3/envs/llama/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/qiuyc/anaconda3/envs/llama/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/home/qiuyc/code/Chatllama/chatllama-nebullvm/chatllama/rlhf/reward.py\", line 239, in __getitem__\r\n    score = float(self.data[idx][\"score\"])\r\nTypeError: float() argument must be a string or a number, not 'NoneType'\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/265/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/265/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/264",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/264/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/264/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/264/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/264",
        "id": 1623660409,
        "node_id": "I_kwDOG1WDQc5gxxd5",
        "number": 264,
        "title": "[OpenAlphaTensor] mis-ordered dimensions in call to PositionEncoding?",
        "user": {
            "login": "kurtosis",
            "id": 13108483,
            "node_id": "MDQ6VXNlcjEzMTA4NDgz",
            "avatar_url": "https://avatars.githubusercontent.com/u/13108483?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/kurtosis",
            "html_url": "https://github.com/kurtosis",
            "followers_url": "https://api.github.com/users/kurtosis/followers",
            "following_url": "https://api.github.com/users/kurtosis/following{/other_user}",
            "gists_url": "https://api.github.com/users/kurtosis/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/kurtosis/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/kurtosis/subscriptions",
            "organizations_url": "https://api.github.com/users/kurtosis/orgs",
            "repos_url": "https://api.github.com/users/kurtosis/repos",
            "events_url": "https://api.github.com/users/kurtosis/events{/privacy}",
            "received_events_url": "https://api.github.com/users/kurtosis/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 0,
        "created_at": "2023-03-14T14:58:20Z",
        "updated_at": "2023-03-20T10:43:00Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "`self.embedding(a)` has dimensions of `(sample size, # of steps, embedding_dim)`\r\nFrom the docstring for PositionEncoding it seems like the first two dimensions should be swapped, i.e. `seq_len, batch_size`.\r\nhttps://github.com/nebuly-ai/nebullvm/blob/b49ad1cdb0ba3fccf623e33c903bf7224120aaba/apps/accelerate/open_alpha_tensor/open_alpha_tensor/core/modules/heads.py#L87",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/264/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/264/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/263",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/263/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/263/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/263/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/263",
        "id": 1623574884,
        "node_id": "I_kwDOG1WDQc5gxclk",
        "number": 263,
        "title": "[Chatllama] Supervised Finetune on LLaMA-13B",
        "user": {
            "login": "zhzou2020",
            "id": 7436697,
            "node_id": "MDQ6VXNlcjc0MzY2OTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/7436697?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/zhzou2020",
            "html_url": "https://github.com/zhzou2020",
            "followers_url": "https://api.github.com/users/zhzou2020/followers",
            "following_url": "https://api.github.com/users/zhzou2020/following{/other_user}",
            "gists_url": "https://api.github.com/users/zhzou2020/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/zhzou2020/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/zhzou2020/subscriptions",
            "organizations_url": "https://api.github.com/users/zhzou2020/orgs",
            "repos_url": "https://api.github.com/users/zhzou2020/repos",
            "events_url": "https://api.github.com/users/zhzou2020/events{/privacy}",
            "received_events_url": "https://api.github.com/users/zhzou2020/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 5,
        "created_at": "2023-03-14T14:15:09Z",
        "updated_at": "2023-03-17T08:38:25Z",
        "closed_at": "2023-03-17T08:38:24Z",
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I tried to load LLaMA-13B model and train it using the following command:\r\n`torchrun --nproc_per_node 2 artifacts/main.py artifacts/config/config.yaml --type ACTOR`\r\n\r\nconfig:\r\n```\r\nactor_config:\r\n  model: \"llama-13B\"\r\n  model_path: \"/root/InstructUIE/run_llama/llama/13B\"\r\n  checkpoint_folder: \"/root/InstructUIE/run_llama/llama/13B/checkpoints\"\r\n  tokenizer_folder: \"/root/InstructUIE/run_llama/llama/tokenizer.model\"\r\n  train_dataset_path: \"./datasets/actor_training_data.json\"\r\n  validation_dataset_path: null\r\n  froze_embeddings: True\r\n  use_fairscale: True\r\n  max_sequence_length: 1024\r\n  max_tokens: 512\r\n  temperature: 0.8\r\n  batch_size: 1\r\n  iteration_per_print: 1\r\n  lr: 0.00001\r\n  epochs: 3\r\n  deepspeed_enable: False\r\n  deepspeed_config_path: \"/root/InstructUIE/ds_configs/stage2_llama.config\"\r\n```\r\n\r\nAnd I get the following error:\r\n\r\n```\r\n        size mismatch for layers.38.attention.wq.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n        size mismatch for layers.38.attention.wk.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n        size mismatch for layers.38.attention.wv.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n        size mismatch for layers.38.attention.wo.weight: copying a param with shape torch.Size([5120, 2560]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n        size mismatch for layers.38.feed_forward.w1.weight: copying a param with shape torch.Size([6912, 5120]) from checkpoint, the shape in current model is torch.Size([13824, 5120]).\r\n        size mismatch for layers.38.feed_forward.w2.weight: copying a param with shape torch.Size([5120, 6912]) from checkpoint, the shape in current model is torch.Size([5120, 13824]).\r\n        size mismatch for layers.38.feed_forward.w3.weight: copying a param with shape torch.Size([6912, 5120]) from checkpoint, the shape in current model is torch.Size([13824, 5120]).\r\n        size mismatch for layers.39.attention.wq.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n        size mismatch for layers.39.attention.wk.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n        size mismatch for layers.39.attention.wv.weight: copying a param with shape torch.Size([2560, 5120]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n        size mismatch for layers.39.attention.wo.weight: copying a param with shape torch.Size([5120, 2560]) from checkpoint, the shape in current model is torch.Size([5120, 5120]).\r\n        size mismatch for layers.39.feed_forward.w1.weight: copying a param with shape torch.Size([6912, 5120]) from checkpoint, the shape in current model is torch.Size([13824, 5120]).\r\n        size mismatch for layers.39.feed_forward.w2.weight: copying a param with shape torch.Size([5120, 6912]) from checkpoint, the shape in current model is torch.Size([5120, 13824]).\r\n        size mismatch for layers.39.feed_forward.w3.weight: copying a param with shape torch.Size([6912, 5120]) from checkpoint, the shape in current model is torch.Size([13824, 5120]).\r\n        size mismatch for output.weight: copying a param with shape torch.Size([16000, 5120]) from checkpoint, the shape in current model is torch.Size([32000, 5120]).\r\n```",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/263/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/263/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/262",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/262/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/262/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/262/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/262",
        "id": 1623052738,
        "node_id": "I_kwDOG1WDQc5gvdHC",
        "number": 262,
        "title": "[Chatllama] RLHF Training - RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method",
        "user": {
            "login": "shrinath-suresh",
            "id": 63862647,
            "node_id": "MDQ6VXNlcjYzODYyNjQ3",
            "avatar_url": "https://avatars.githubusercontent.com/u/63862647?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/shrinath-suresh",
            "html_url": "https://github.com/shrinath-suresh",
            "followers_url": "https://api.github.com/users/shrinath-suresh/followers",
            "following_url": "https://api.github.com/users/shrinath-suresh/following{/other_user}",
            "gists_url": "https://api.github.com/users/shrinath-suresh/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/shrinath-suresh/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/shrinath-suresh/subscriptions",
            "organizations_url": "https://api.github.com/users/shrinath-suresh/orgs",
            "repos_url": "https://api.github.com/users/shrinath-suresh/repos",
            "events_url": "https://api.github.com/users/shrinath-suresh/events{/privacy}",
            "received_events_url": "https://api.github.com/users/shrinath-suresh/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "closed",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 7,
        "created_at": "2023-03-14T09:17:35Z",
        "updated_at": "2023-04-06T04:51:55Z",
        "closed_at": "2023-04-06T04:51:55Z",
        "author_association": "CONTRIBUTOR",
        "active_lock_reason": null,
        "body": "While training RL model with the following command\r\n\r\n```\r\npython artifacts/main.py artifacts/config/config.yaml --type RL\r\n```\r\nwe get the torch dataset error. To fix the error, we changed the following lines in [trainer.py:427](https://github.com/nebuly-ai/nebullvm/blob/d16dfd8312218540fbdc633222e4655480040778/apps/accelerate/chatllama/chatllama/rlhf/trainer.py#L427)\r\n\r\n```\r\n        # from\r\n        #dataloader = DataLoader(\r\n            #ExperienceDataset(memories, device), batch_size=batch_size\r\n        #)\r\n       # to \r\n       dataset = ExperienceDataset(memories, device)\r\n```\r\nand in [trainer.py:455](https://github.com/nebuly-ai/nebullvm/blob/d16dfd8312218540fbdc633222e4655480040778/apps/accelerate/chatllama/chatllama/rlhf/trainer.py#L455)\r\n\r\n```\r\n#from\r\n#training_data=dataloader\r\n#to\r\ntraining_data=dataset\r\n```\r\nIssue Reference - #229 \r\n\r\nPost this fix, we are facing the following error\r\n\r\n```\r\nUsing /home/ubuntu/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\r\nNo modifications detected for re-loaded extension module utils, skipping build step...\r\nLoading extension module utils...\r\nTime to load utils op: 0.00028014183044433594 seconds\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/artifacts/main.py\", line 48, in <module>\r\n    rlhf_trainer.train()\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/chatllama/rlhf/trainer.py\", line 788, in train\r\n    self.learn(memories)\r\n  File \"<@beartype(chatllama.rlhf.trainer.RLTrainer.learn) at 0x7f279c996820>\", line 33, in learn\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/chatllama/rlhf/trainer.py\", line 492, in learn\r\n    for i, (\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/dataloader.py\", line 125, in __next__\r\n    return next(self.data)\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/dataloader.py\", line 158, in <genexpr>\r\n    self.data = (x for x in self.dataloader)\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 628, in __next__\r\n    data = self._next_data()\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1333, in _next_data\r\n    return self._process_data(data)\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1359, in _process_data\r\n    data.reraise()\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_utils.py\", line 543, in reraise\r\n    raise exception\r\nRuntimeError: Caught RuntimeError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/chatllama/rlhf/trainer.py\", line 207, in __getitem__\r\n    self.data[idx].states.to(self.device),\r\n  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 217, in _lazy_init\r\n    raise RuntimeError(\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n```\r\n\r\nAttaching the log, dataset, config for reference\r\n\r\nTraining log - \r\n[rl_train.log](https://github.com/nebuly-ai/nebullvm/files/10966534/rl_train.log)\r\nConfigurations - \r\n[test.zip](https://github.com/nebuly-ai/nebullvm/files/10966551/test.zip)\r\nDataset - \r\n[rlhf_training_data.zip](https://github.com/nebuly-ai/nebullvm/files/10966565/rlhf_training_data.zip)\r\n\r\nTraining Environment:\r\nNvidia - A10 - 24 GB - g5.4xlarge - AWS instance\r\nPackages are installed from the [README instructions](https://github.com/nebuly-ai/nebullvm/blob/main/apps/accelerate/chatllama/README.md#quick-install)\r\n\r\n",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/262/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/262/timeline",
        "performed_via_github_app": null,
        "state_reason": "completed"
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/261",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/261/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/261/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/261/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/261",
        "id": 1620870425,
        "node_id": "I_kwDOG1WDQc5gnIUZ",
        "number": 261,
        "title": "[Chatllama] training 20b model hardware requirements",
        "user": {
            "login": "ehartford",
            "id": 1117701,
            "node_id": "MDQ6VXNlcjExMTc3MDE=",
            "avatar_url": "https://avatars.githubusercontent.com/u/1117701?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/ehartford",
            "html_url": "https://github.com/ehartford",
            "followers_url": "https://api.github.com/users/ehartford/followers",
            "following_url": "https://api.github.com/users/ehartford/following{/other_user}",
            "gists_url": "https://api.github.com/users/ehartford/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/ehartford/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/ehartford/subscriptions",
            "organizations_url": "https://api.github.com/users/ehartford/orgs",
            "repos_url": "https://api.github.com/users/ehartford/repos",
            "events_url": "https://api.github.com/users/ehartford/events{/privacy}",
            "received_events_url": "https://api.github.com/users/ehartford/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-13T07:01:48Z",
        "updated_at": "2023-03-14T09:47:48Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I saw the hardware requirement for training chat-llama\r\n\r\n13B to 20B \u2192 8x Nvidia A100 (80Gb)\r\n\r\nbut check this article from HF where they show how to do it with a single 4090\r\n\r\nhttps://huggingface.co/blog/trl-peft\r\n\r\nCan this method be used with chat-llama?  (ie using 8-bit, and trainable adapters)",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/261/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/261/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/260",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/260/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/260/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/260/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/260",
        "id": 1620718196,
        "node_id": "I_kwDOG1WDQc5gmjJ0",
        "number": 260,
        "title": "[Chatllama] Should I add a <end_of_text> at the end of sentence?",
        "user": {
            "login": "bino282",
            "id": 17800187,
            "node_id": "MDQ6VXNlcjE3ODAwMTg3",
            "avatar_url": "https://avatars.githubusercontent.com/u/17800187?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/bino282",
            "html_url": "https://github.com/bino282",
            "followers_url": "https://api.github.com/users/bino282/followers",
            "following_url": "https://api.github.com/users/bino282/following{/other_user}",
            "gists_url": "https://api.github.com/users/bino282/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/bino282/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/bino282/subscriptions",
            "organizations_url": "https://api.github.com/users/bino282/orgs",
            "repos_url": "https://api.github.com/users/bino282/repos",
            "events_url": "https://api.github.com/users/bino282/events{/privacy}",
            "received_events_url": "https://api.github.com/users/bino282/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 3,
        "created_at": "2023-03-13T04:08:17Z",
        "updated_at": "2023-04-03T14:41:17Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "When I train a actor model with bloom-560M. I realize that , the model generates  the text repeated at the end. The model always generates enough words predefined by max_length but it;s not stop early. Should I add a <end_of_text> at the end of sentence when training?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/260/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/260/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/259",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/259/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/259/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/259/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/259",
        "id": 1620420129,
        "node_id": "I_kwDOG1WDQc5glaYh",
        "number": 259,
        "title": "[Chatllama] GPT2 model is missing error on tutorial",
        "user": {
            "login": "TakafumiYano",
            "id": 3312617,
            "node_id": "MDQ6VXNlcjMzMTI2MTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3312617?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TakafumiYano",
            "html_url": "https://github.com/TakafumiYano",
            "followers_url": "https://api.github.com/users/TakafumiYano/followers",
            "following_url": "https://api.github.com/users/TakafumiYano/following{/other_user}",
            "gists_url": "https://api.github.com/users/TakafumiYano/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TakafumiYano/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TakafumiYano/subscriptions",
            "organizations_url": "https://api.github.com/users/TakafumiYano/orgs",
            "repos_url": "https://api.github.com/users/TakafumiYano/repos",
            "events_url": "https://api.github.com/users/TakafumiYano/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TakafumiYano/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-12T16:10:22Z",
        "updated_at": "2023-03-14T09:48:14Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "HI all\r\n\r\nI am a beginner and small question for this field.\r\nI followed the readme and tried to run the study as per the default cofig.yaml. However, I got an error that the GPT2 model does not exist in the model folder. Should I just place the GPT2 weight file here or do I need to place a file like GPT2 model (model.py) together? Looking at this github repository, only the model file (llama_model.py) for LLaMA exists, so I feel like I need to put the model.py for GPT as well. If possible, I would like to know where can I get the pretrained GPT2 model compatible with this chatLLaMA. Thankyou.\r\n\r\n\r\n**The error message is following:**\r\n\r\nWarning, Impossible to load the model:\r\n./models/gpt2-large.pt\r\nNo previous checkpoint found.\r\nWarning, Impossible to load the model:\r\n./models/gpt2-large.pt\r\nNo previous checkpoint found.\r\nStart RL Training\r\nEpisode: 1 of 100, Timestep: 1 of 32\r\nTraceback (most recent call last):",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/259/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/259/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/258",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/258/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/258/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/258/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/258",
        "id": 1620371920,
        "node_id": "I_kwDOG1WDQc5glOnQ",
        "number": 258,
        "title": "[Nebullvm] Can't run GPU support from docker without AVX support in CPU",
        "user": {
            "login": "convertthinking",
            "id": 125678133,
            "node_id": "U_kgDOB32yNQ",
            "avatar_url": "https://avatars.githubusercontent.com/u/125678133?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/convertthinking",
            "html_url": "https://github.com/convertthinking",
            "followers_url": "https://api.github.com/users/convertthinking/followers",
            "following_url": "https://api.github.com/users/convertthinking/following{/other_user}",
            "gists_url": "https://api.github.com/users/convertthinking/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/convertthinking/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/convertthinking/subscriptions",
            "organizations_url": "https://api.github.com/users/convertthinking/orgs",
            "repos_url": "https://api.github.com/users/convertthinking/repos",
            "events_url": "https://api.github.com/users/convertthinking/events{/privacy}",
            "received_events_url": "https://api.github.com/users/convertthinking/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 1,
        "created_at": "2023-03-12T13:46:03Z",
        "updated_at": "2023-03-20T10:42:24Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "I'm trying to run nebulvm with GPU support from docker, but got an error:\r\n\r\n> docker run --rm --gpus all -ti -p 8888:8888 -v $PWD:/nebullvm nebulydocker/nebullvm:latest\r\n> \r\n> =====================\r\n> == NVIDIA TensorRT ==\r\n> =====================\r\n> \r\n> NVIDIA Release 22.12 (build 49968236)\r\n> NVIDIA TensorRT Version 8.5.1\r\n> Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\r\n> \r\n> Container image Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\r\n> \r\n> https://developer.nvidia.com/tensorrt\r\n> \r\n> Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n> \r\n> This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n> By pulling and using the container, you accept the terms and conditions of this license:\r\n> https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n> \r\n> To install Python sample dependencies, run /opt/tensorrt/python/python_setup.sh\r\n> \r\n> To install the open-source samples corresponding to this TensorRT release version\r\n> run /opt/tensorrt/install_opensource.sh.  To build the open source parsers,\r\n> plugins, and samples for current top-of-tree on master or a different branch,\r\n> run /opt/tensorrt/install_opensource.sh -b <branch>\r\n> See https://github.com/NVIDIA/TensorRT for more information.\r\n> \r\n> ERROR: This container was built for CPUs supporting at least the AVX instruction set, but\r\n>        the CPU detected was Intel(R) Xeon(R) CPU           E5645  @ 2.40GHz, which does not report\r\n>        support for AVX.  An Illegal Instrution exception at runtime is likely to result.\r\n>        See https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX .\r\n> \r\n\r\nCPUs with SSE without AVX are still handy and all around, why cannot it be checked and use SSE if AVX instructions are not available?",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/258/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/258/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/257",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/257/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/257/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/257/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/257",
        "id": 1620354327,
        "node_id": "I_kwDOG1WDQc5glKUX",
        "number": 257,
        "title": "[Chatllama] Training Japanese language for finetuning or training from scratch",
        "user": {
            "login": "TakafumiYano",
            "id": 3312617,
            "node_id": "MDQ6VXNlcjMzMTI2MTc=",
            "avatar_url": "https://avatars.githubusercontent.com/u/3312617?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/TakafumiYano",
            "html_url": "https://github.com/TakafumiYano",
            "followers_url": "https://api.github.com/users/TakafumiYano/followers",
            "following_url": "https://api.github.com/users/TakafumiYano/following{/other_user}",
            "gists_url": "https://api.github.com/users/TakafumiYano/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/TakafumiYano/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/TakafumiYano/subscriptions",
            "organizations_url": "https://api.github.com/users/TakafumiYano/orgs",
            "repos_url": "https://api.github.com/users/TakafumiYano/repos",
            "events_url": "https://api.github.com/users/TakafumiYano/events{/privacy}",
            "received_events_url": "https://api.github.com/users/TakafumiYano/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 2,
        "created_at": "2023-03-12T12:52:16Z",
        "updated_at": "2023-03-14T09:48:58Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "To the chatLLaMA team, \r\nThank you very much for this nice project.\r\nI looked at the model file and saw that the comment of compatiblity with training, so I thought it would be possible to train with a training dataset that we have prepared ourselves.\r\nIf possible, can you give us some tips on how to train additional datasets?\r\nWe would like to experiment and report back with documentation.\r\n\r\nThanks.",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/257/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/257/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    },
    {
        "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/256",
        "repository_url": "https://api.github.com/repos/nebuly-ai/nebuly",
        "labels_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/256/labels{/name}",
        "comments_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/256/comments",
        "events_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/256/events",
        "html_url": "https://github.com/nebuly-ai/nebuly/issues/256",
        "id": 1620281534,
        "node_id": "I_kwDOG1WDQc5gk4i-",
        "number": 256,
        "title": "[Chatllama] Errors when training actor model based on LLaMA-7B",
        "user": {
            "login": "young-chao",
            "id": 34190033,
            "node_id": "MDQ6VXNlcjM0MTkwMDMz",
            "avatar_url": "https://avatars.githubusercontent.com/u/34190033?v=4",
            "gravatar_id": "",
            "url": "https://api.github.com/users/young-chao",
            "html_url": "https://github.com/young-chao",
            "followers_url": "https://api.github.com/users/young-chao/followers",
            "following_url": "https://api.github.com/users/young-chao/following{/other_user}",
            "gists_url": "https://api.github.com/users/young-chao/gists{/gist_id}",
            "starred_url": "https://api.github.com/users/young-chao/starred{/owner}{/repo}",
            "subscriptions_url": "https://api.github.com/users/young-chao/subscriptions",
            "organizations_url": "https://api.github.com/users/young-chao/orgs",
            "repos_url": "https://api.github.com/users/young-chao/repos",
            "events_url": "https://api.github.com/users/young-chao/events{/privacy}",
            "received_events_url": "https://api.github.com/users/young-chao/received_events",
            "type": "User",
            "site_admin": false
        },
        "labels": [],
        "state": "open",
        "locked": false,
        "assignee": null,
        "assignees": [],
        "milestone": null,
        "comments": 4,
        "created_at": "2023-03-12T08:39:17Z",
        "updated_at": "2023-03-14T09:46:46Z",
        "closed_at": null,
        "author_association": "NONE",
        "active_lock_reason": null,
        "body": "root@b787722dc2e1:/workspace/workfile/Projects/chatllama# python artifacts/main.py artifacts/config/config.yaml --type ACTOR\r\nCurrent device used :cuda\r\nlocal_rank: -1 world_size: -1\r\nTraceback (most recent call last):\r\n  File \"/workspace/workfile/Projects/chatllama/artifacts/main.py\", line 50, in <module>\r\n    actor_trainer = ActorTrainer(config.actor)\r\n  File \"/usr/local/lib/python3.9/site-packages/chatllama/rlhf/actor.py\", line 292, in __init__\r\n    self.model = ActorModel(config)\r\n  File \"/usr/local/lib/python3.9/site-packages/chatllama/rlhf/actor.py\", line 52, in __init__\r\n    local_rank, world_size = setup_model_parallel()\r\n  File \"/usr/local/lib/python3.9/site-packages/chatllama/llama_model.py\", line 551, in setup_model_parallel\r\n    torch.distributed.init_process_group(\"nccl\")\r\n  File \"/usr/local/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 754, in init_process_group\r\n    store, rank, world_size = next(rendezvous_iterator)\r\n  File \"/usr/local/lib/python3.9/site-packages/torch/distributed/rendezvous.py\", line 236, in _env_rendezvous_handler\r\n    rank = int(_get_env_or_raise(\"RANK\"))\r\n  File \"/usr/local/lib/python3.9/site-packages/torch/distributed/rendezvous.py\", line 221, in _get_env_or_raise\r\n    raise _env_error(env_var)\r\nValueError: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
        "reactions": {
            "url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/256/reactions",
            "total_count": 0,
            "+1": 0,
            "-1": 0,
            "laugh": 0,
            "hooray": 0,
            "confused": 0,
            "heart": 0,
            "rocket": 0,
            "eyes": 0
        },
        "timeline_url": "https://api.github.com/repos/nebuly-ai/nebuly/issues/256/timeline",
        "performed_via_github_app": null,
        "state_reason": null
    }
]